# Normal environment hyperparameters

# env/general params
max_timesteps: 2000          # per episode (MAX_TIMESTEPS)
max_episodes: 300            # NOTE - may affect annealing steps
target_score: 300            # stop training when average score over r_list > target_score
len_r_list: 100              # length of reward list to average over for target score
init_rand_steps: 10000       # number of steps with random actions before training

# Agent hyperparams
n_mini_critics: 5            # each mini-critic is a single mlp, which combine to make one mega-critic
n_quantiles: 25              # quantiles per mini critic
top_quantiles_to_drop_per_net: auto  # per mini critic (auto based on n_quantiles)
actor_hidden_dims: [512, 512]
mini_critic_hidden_dims: [256, 256]  # * n_mini_critics
batch_size: 64
discount: 0.99               # gamma
tau: 0.005
actor_lr: 3.71e-4            # empirically chosen
critic_lr: 3.8e-4            # empirically chosen
alpha_lr: 3.24e-4            # empirically chosen

# ERE buffer (see paper for their choices)
use_per: true                # use PER sampling as well
buffer_size: 100000          # smaller size improves learning early on but is outperformed later on
eta0: 0.996                  # 0.994 - 0.999 is good (according to paper)
annealing_steps: auto        # auto-set to max estimated steps in training (600k is good for normal)
cmin: 5000                   # min number of samples to sample from
recency_scale: 1             # scale factor for recency