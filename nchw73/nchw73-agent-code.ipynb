{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "# **Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSUGAJsTLOV",
    "outputId": "875609e2-e3eb-4b29-c03c-1989820b1ea7"
   },
   "outputs": [],
   "source": [
    "# https://github.com/robert-lieck/rldurham/tree/main - rldurham source\n",
    "\n",
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham\n",
    "\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "AFrqd24JTLOW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np # TODO consider switching out for torch where poss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.distributions as D\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "\n",
    "import rldurham as rld\n",
    "\n",
    "# from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "# **RL agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIMESTEPS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERs\n",
    "# for https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def train_on_environment(actor, env, class_to_take_gradient_step,\n",
    " replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length):\n",
    "    '''\n",
    "    abstracted training loop function for both real & dreamer environments to use\n",
    "\n",
    "    the real environment will always have MAX_TIMESTEPS as the max_timesteps value?\n",
    "    while the dreamer enviroment might have less timesteps?\n",
    "    '''\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "    input_buffer = torch.empty((0, 54)) # save start sequence for the dreamer model TODO - where does 54 come from?\n",
    "\n",
    "    for t in range(max_timesteps): \n",
    "        total_steps += 1\n",
    "        # select action and step the env\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "\n",
    "        # handle different return formats\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, info = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            print(\"step length is not 5\")\n",
    "            next_state, reward, done, info = step\n",
    "\n",
    "        ep_timesteps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        if max_timesteps == MAX_TIMESTEPS: # i.e. \n",
    "            # reward design NOTE justify!\n",
    "            if reward == -100.0:\n",
    "                reward = -10.0\n",
    "            else:\n",
    "                reward *= 2\n",
    "            replay_buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "        else:\n",
    "            if t == sequence_length:\n",
    "                for row in input_buffer.cpu().numpy():\n",
    "                    replay_buffer.add(row[:24], row[24:28], row[28:52], row[52], row[53])\n",
    "                    # TODO - why does this ^ use slices? Where do these numbers come from?\n",
    "            elif t > sequence_length:\n",
    "                replay_buffer.add(state, action, next_state, reward, done)\n",
    "            # NOTE - don't store if simulation ends without reaching the sequence length\n",
    "        \n",
    "        if t < sequence_length: # store in input buffer TODO what does this do? why's it needed?\n",
    "            input_buffer = torch.cat([\n",
    "                input_buffer,\n",
    "                torch.tensor(\n",
    "                    np.concatenate((state, action, next_state, np.array([reward]), np.array([done])),\n",
    "                    axis=0)\n",
    "                    ).unsqueeze(0)\n",
    "                ], axis=0)        \n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "        if total_steps >= batch_size: # do 1 training update using 1 batch from buffer\n",
    "            # train the agent using experiences from the real environment\n",
    "            class_to_take_gradient_step.take_gradient_step(replay_buffer, t, batch_size)\n",
    "    \n",
    "        if done: # break if ep finished\n",
    "            break\n",
    "\n",
    "    if sequence_length > 0: # TODO - this means...? which env are we in?\n",
    "        return ep_timesteps, ep_reward, input_buffer, info\n",
    "    return ep_timesteps, ep_reward, None, info\n",
    "\n",
    "\n",
    "# test loop for agent on environment\n",
    "def simulate_on_environment(actor, env, max_timesteps, state):\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "    for t in range(max_timesteps):\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, _ = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, _ = step\n",
    "        ep_timesteps += 1\n",
    "    \n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "    \n",
    "        if done or t == max_timesteps - 1:\n",
    "            break\n",
    "    return ep_timesteps, ep_reward\n",
    "\n",
    "# generate value array from replay buffer\n",
    "def retrieve_from_replay_buffer(replay_buffer, ptr):\n",
    "    '''\n",
    "    '''\n",
    "    return np.concatenate((\n",
    "                    replay_buffer.state[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.action[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.reward[ptr:replay_buffer.ptr],\n",
    "                    1. - replay_buffer.not_done[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.next_state[ptr:replay_buffer.ptr],                \n",
    "                ), \n",
    "                axis = 1 # along the columns (so each row is a memory)\n",
    "            )\n",
    "\n",
    "# function to create sequences with a given window size and step size\n",
    "def create_sequences(memories, window_size, step_size):\n",
    "    '''\n",
    "    Function to create sequences of memories from the replay buffer.\n",
    "\n",
    "    Each sequence is of length window_size and each spaced apart by step_size\n",
    "    '''\n",
    "    n_memories = memories.shape[0] # just the number of time steps currently in the buffer\n",
    "\n",
    "    # calc number of seq (of len window_size) we can create from mems available  \n",
    "    n_sequences = math.floor((n_memories - window_size) / step_size) + 1 # +1 because indices start at 0\n",
    "\n",
    "    sequences = np.zeros((n_sequences, window_size, memories.shape[1]))\n",
    "    for i in range(n_sequences):\n",
    "        start_idx = i * step_size # idx to start seq from\n",
    "        sequences[i, :] = memories[start_idx:start_idx + window_size, :] # grab seq of memories window_size long from start_idx\n",
    "    return sequences\n",
    "\n",
    "def gen_test_train_seq(replay_buffer, train_set, test_set, train_split, window_size, step_size, ptr):\n",
    "    '''\n",
    "    Function to split train and test data\n",
    "    '''\n",
    "    memories = retrieve_from_replay_buffer(replay_buffer, ptr)\n",
    "    try:\n",
    "        memory_sequences = create_sequences(memories, window_size, step_size)\n",
    "        n_sequences = memory_sequences.shape[0]\n",
    "    except: # TODO How might this go wrong? not enough new data to create a sequence?\n",
    "        return train_set, test_set\n",
    "\n",
    "    # shuffle the sequences & split\n",
    "    indices = np.arange(n_sequences)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(train_split * n_sequences) # get split point \n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "\n",
    "    if train_set is None: # if this is first train/test set, create the sets\n",
    "        return memory_sequences[train_indices, :], memory_sequences[test_indices, :]\n",
    "    # else just add to existing sets\n",
    "    return np.concatenate((train_set, memory_sequences[train_indices, :]), axis=0), np.concatenate((test_set, memory_sequences[test_indices, :]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dreamer_eps(dreamer_avg_loss, score_threshold):\n",
    "    '''\n",
    "    Get number of dreamed episdoes the agent should run on the dreamer model.\n",
    "\n",
    "    Only use dreamer for trainibg if it's sufficiently accurate model of env.\n",
    "\n",
    "    Note - score_threshold is between 0 and 1.\n",
    "    '''\n",
    "    max_dreamer_it = 10 # a perfect dreamer has this many eps\n",
    "\n",
    "    if dreamer_avg_loss >= score_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        norm_score = dreamer_avg_loss/score_threshold # normalise score relative to threshold\n",
    "        inv_score = 1 - norm_score # invert so that closer to 1 ==> dreamer score much better than threshold\n",
    "        sq_score = inv_score**2 # square so that num iter increases quadratically as accuracy improves\n",
    "        return int(max_dreamer_it * sq_score) # scale so that max iterations is 10 (when dreamer very accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "9kTeUHYL7gU6"
   },
   "outputs": [],
   "source": [
    "class EREReplayBuffer(object):\n",
    "    '''\n",
    "    implementation - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    An ERE buffer is implemented to improve sample efficiency, \n",
    "        for which the paper can be found here: https://arxiv.org/abs/1906.04009\n",
    "\n",
    "    Rationale:\n",
    "    - actions from early in training are likely v different to optimal, so want to prioritise recent ones so we're not constantly learning from crap ones\n",
    "    - as time goes on, they get more similar so annealing eta allows uniform sampling later on\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, T, max_size=int(1e6), eta=0.996, cmin=5000):\n",
    "        self.max_size, self.ptr, self.size, self.rollover = max_size, 0, 0, False\n",
    "        self.eta0 = eta # 0.996 - 0.999 is good (according to paper)\n",
    "        self.cmin = cmin\n",
    "        self.c_list = []\n",
    "        self.index = []\n",
    "        self.T = T\n",
    "\n",
    "        self.reward = np.empty((max_size, 1))\n",
    "        self.state = np.empty((max_size, state_dim))\n",
    "        self.action = np.empty((max_size, action_dim))\n",
    "        self.not_done = np.empty((max_size, 1))\n",
    "        self.next_state = np.empty((max_size, state_dim))\n",
    "        \n",
    "    def sample(self, batch_size, t):\n",
    "        # update eta value for current timestep\n",
    "        eta = self.eta_anneal(t)\n",
    "\n",
    "        index = np.array([self._get_index(eta, k, batch_size) for k in range(batch_size)])\n",
    "\n",
    "        r = torch.tensor(self.reward[index], dtype = torch.float)\n",
    "        s = torch.tensor(self.state[index], dtype = torch.float)\n",
    "        ns = torch.tensor(self.next_state[index], dtype = torch.float)\n",
    "        a = torch.tensor(self.action[index], dtype = torch.float)\n",
    "        nd = torch.tensor(self.not_done[index], dtype = torch.float)\n",
    "        \n",
    "        return s, a, ns, r, nd\n",
    "    \n",
    "    def _get_index(self, eta, k, batch_size):\n",
    "        c_calc = self.size * eta ** (k * 1000 / batch_size)\n",
    "        ck = c_calc if c_calc > self.cmin else self.size\n",
    "\n",
    "        if not self.rollover: # if we're not overwriting yet, ...\n",
    "            return np.random.randint(self.size - ck, self.size)\n",
    "        \n",
    "        return np.random.randint(self.ptr + self.size - ck, self.ptr + self.size) % self.size\n",
    "\n",
    "    def eta_anneal(self, t):\n",
    "        # eta anneals over time --> 1, which reduces emphasis on recent experiences over time\n",
    "        return self.eta0 + (1 - self.eta0) * t / self.T\n",
    " \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        # Add experience to replay buffer \n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.ptr %= self.max_size\n",
    "\n",
    "        # increase size counter until full, then start overwriting (rollover)\n",
    "        if self.max_size > self.size + 1:\n",
    "          self.size += 1\n",
    "        else:\n",
    "          self.size = self.max_size\n",
    "          self.rollover = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dreamer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class DreamerAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, seq_len, num_layers, num_heads, dropout_prob, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.input_dim = state_dim + action_dim + 2\n",
    "        self.target_dim = self.state_dim + self.action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.input_fc = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.target_fc = nn.Linear(self.target_dim, hidden_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            num_heads, \n",
    "            dropout=dropout_prob,\n",
    "            activation=F.gelu,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_next_state = nn.Linear(hidden_dim + self.target_dim, state_dim)\n",
    "        self.output_reward = nn.Linear(hidden_dim + self.target_dim, 1)\n",
    "        self.output_done = nn.Linear(hidden_dim + self.target_dim, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=3e-4)\n",
    "        \n",
    "    # separate out the ground truth variables and compare against predictions\n",
    "    def loss_fn(self, output_next_state, output_reward, output_done, ground_truth):\n",
    "        reward, done, next_state = torch.split(ground_truth, [1, 1, self.state_dim], dim=-1)\n",
    "        loss = self.mse_loss(output_next_state[:, -1], next_state)\n",
    "        loss += self.mse_loss(output_reward[:, -1], reward)\n",
    "        loss += self.ce_loss(output_done[:, -1], done)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # separate the input and target tensors\n",
    "        target = input_tensor[:, -1, :self.target_dim].unsqueeze(1)\n",
    "        encoded_target = self.target_fc(target)\n",
    "        encoded_input = self.input_fc(input_tensor[:, :-1, :self.input_dim])\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target], axis=2))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target], axis=2))\n",
    "        # output_done = self.output_done(torch.cat([encoded_output, target], axis=2)) # don't need sigmoid because nn.CrossEntropy already does it?\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target], axis=2)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "    \n",
    "    def predict(self, input_tensor, target_tensor):\n",
    "        # separate the input and target tensors\n",
    "        encoded_target = self.target_fc(target_tensor)\n",
    "        encoded_input = self.input_fc(input_tensor)\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target_tensor], axis=1)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "\n",
    "    # transformer training loop\n",
    "    # sequences shape: (batch, sequence, features)\n",
    "    def train_dreamer(self, sequences, epochs, batch_size=256):\n",
    "        print(\"Training Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float)\n",
    "        \n",
    "        train_dataset = TensorDataset(inputs)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(train_dataloader):\n",
    "                input_batch = input_batch[0]\n",
    "                self.optimizer.zero_grad()\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, running_loss / len(train_dataloader)))\n",
    "\n",
    "    # transformer testing loop\n",
    "    def test_dreamer(self, sequences, batch_size=64):\n",
    "        print(\"Testing Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float)\n",
    "        \n",
    "        test_dataset = TensorDataset(inputs)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(test_dataloader):\n",
    "                input_batch = input_batch[0]\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                running_loss += loss.item()\n",
    "            print('Test Loss: {:.4f}'.format(running_loss / len(test_dataloader)))\n",
    "        return running_loss / len(test_dataloader)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.actions = torch.cat([self.actions, torch.tensor(np.array([action]))], axis=0)\n",
    "        input_sequence = torch.cat([self.states[:-1], self.actions[:-1], self.rewards, self.dones], axis=1).to(torch.float32)\n",
    "        target = torch.cat([self.states[-1], self.actions[-1]], axis=0).unsqueeze(0).to(torch.float32)\n",
    "        with torch.no_grad():\n",
    "            next_state, reward, done = self.predict(input_sequence, target)\n",
    "            done = (done >= 0.6).float() # bias towards not done to avoid false positives\n",
    "\n",
    "            self.states = torch.cat([self.states, next_state], axis=0)\n",
    "            self.rewards = torch.cat([self.rewards, reward], axis=0)\n",
    "            self.dones = torch.cat([self.dones, done], axis=0)\n",
    "            \n",
    "            if self.states.shape[0] > self.seq_len:\n",
    "                self.states = self.states[1:]\n",
    "                self.rewards = self.rewards[1:]\n",
    "                self.dones = self.dones[1:]\n",
    "            if self.actions.shape[0] > self.seq_len - 1:\n",
    "                self.actions = self.actions[1:]\n",
    "        \n",
    "        return next_state.squeeze(0).cpu().numpy(), reward.cpu().item(), done.cpu().item(), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actor-critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS\n",
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def quantile_huber_loss(quantiles, samples, sum_over_quantiles = False):\n",
    "    '''\n",
    "    Huber loss is less sensitive to outliers than MSE\n",
    "    '''\n",
    "    #return huber loss - uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise\n",
    "    delta = samples[:, np.newaxis, np.newaxis, :] - quantiles[:, :, :, np.newaxis]  \n",
    "    abs_delta = torch.abs(delta)\n",
    "    huber_loss = torch.where(abs_delta > 1, abs_delta - 0.5, delta ** 2 * 0.5)\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "    cumulative_prob = (torch.arange(n_quantiles, device=quantiles.device, dtype=torch.float) + 0.5) / n_quantiles\n",
    "    cumulative_prob_shaped = cumulative_prob.view(1, 1, -1, 1)\n",
    "    loss = (torch.abs(cumulative_prob_shaped - (delta < 0).float()) * huber_loss)\n",
    "\n",
    "    # Summing over the quantile dimension \n",
    "    if sum_over_quantiles:\n",
    "        loss = loss.sum(dim=-2).mean()\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# MLP for actor that implements D2RL architecture\n",
    "class ActorMLP(nn.Module):\n",
    "  def __init__(self,input_size,hidden_sizes,output_size):\n",
    "    super().__init__()\n",
    "    self.list_of_layers = []\n",
    "    input_size_ = input_size\n",
    "    num_inputs = 24 \n",
    "    input_dim = hidden_sizes[0] + num_inputs\n",
    "    for i, next_size in enumerate(hidden_sizes):\n",
    "      if i == 0:\n",
    "        lay = nn.Linear(input_size_, next_size)\n",
    "      else:\n",
    "        lay = nn.Linear(input_dim, next_size)\n",
    "      self.add_module(f'layer{i}', lay)\n",
    "      self.list_of_layers.append(lay)\n",
    "      input_size_ = next_size\n",
    "        \n",
    "    self.last_layer_mean_linear = nn.Linear(input_dim, output_size)\n",
    "    self.last_layer_log_std_linear = nn.Linear(input_dim, output_size)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "\n",
    "    for layer in self.list_of_layers:\n",
    "      intermediate = layer(curr)\n",
    "      curr = F.gelu(intermediate)\n",
    "      curr = torch.cat([curr, input_], dim=1)\n",
    "\n",
    "    mean_linear = self.last_layer_mean_linear(curr)\n",
    "    log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "    return mean_linear, log_std_linear\n",
    "\n",
    "# MLP for critic that implements D2RL architecture \n",
    "class CriticMLP(nn.Module):\n",
    "  def __init__(self,input_size,hidden_sizes,output_size):\n",
    "    super().__init__()\n",
    "    input_size_ = input_size\n",
    "    input_dim = 28 + hidden_sizes[0] \n",
    "    self.list_of_layers = []\n",
    "    for i, next_size in enumerate(hidden_sizes):\n",
    "      if i == 0:\n",
    "        lay = nn.Linear(input_size_, next_size)\n",
    "      else: \n",
    "        lay = nn.Linear(input_dim, next_size)\n",
    "      self.add_module(f'layer{i}', lay)\n",
    "      self.list_of_layers.append(lay)\n",
    "    self.last_layer = nn.Linear(input_dim, output_size)\n",
    "  \n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for lay in self.list_of_layers:\n",
    "      curr_ = F.gelu(lay(curr))\n",
    "      curr = torch.cat([curr_, input_], dim = 1)\n",
    "    output = self.last_layer(curr)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh dist for actor to sample actions from \n",
    "\n",
    "# FROM ORIGINAL CODE https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# class TanhNormal(D.Distribution):\n",
    "#   def __init__(self, normal_mean, normal_std):\n",
    "#     super().__init__()\n",
    "#     self.normal_mean = normal_mean\n",
    "#     self.normal_std = normal_std\n",
    "#     self.normal = D.Normal(normal_mean, normal_std)\n",
    "#     self.stand_normal = D.Normal(torch.zeros_like(self.normal_mean), torch.ones_like(self.normal_std))        \n",
    "      \n",
    "#   def logsigmoid(tensor):\n",
    "#     denominator = 1 + torch.exp(-tensor)\n",
    "#     return torch.log(1/ denominator)\n",
    "\n",
    "#   def log_probability(self, pre_tanh):\n",
    "#     final = (self.normal.log_prob(pre_tanh)) - (2 * np.log(2) + F.logsigmoid(2 * pre_tanh) + F.logsigmoid(-2 * pre_tanh))\n",
    "#     return final\n",
    "\n",
    "#   def random_sample(self):\n",
    "#     pretanh = self.normal_mean + self.normal_std * self.stand_normal.sample()\n",
    "#     return torch.tanh(pretanh), pretanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class Gradient_Step(object):\n",
    "  '''\n",
    "  see https://github.com/pairlab/d2rl/blob/main/sac/sac.py\n",
    "  '''\n",
    "  def __init__(\n",
    "    self,\n",
    "    *,\n",
    "    actor,\n",
    "    critic,\n",
    "    critic_target,\n",
    "    discount,\n",
    "    tau,\n",
    "    top_quantiles_to_drop,\n",
    "    target_entropy,\n",
    "    quantiles_total\n",
    "  ):\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.critic_target = critic_target\n",
    "\n",
    "    self.log_alpha = torch.zeros((1,), requires_grad=True) # log alpha is learned\n",
    "    self.quantiles_total = quantiles_total\n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "    \n",
    "    self.alpha_optimizer = optim.Adam([self.log_alpha], lr=3e-4)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "    self.discount, self.tau = discount, tau\n",
    "    self.top_quantiles_to_drop, self.target_entropy = top_quantiles_to_drop, target_entropy\n",
    "\n",
    "  def take_gradient_step(self, replay_buffer, t, batch_size=256):\n",
    "    # Sample batch from replay buffer\n",
    "    state, action, next_state, reward, not_done = replay_buffer.sample(batch_size, t)\n",
    "    alpha = torch.exp(self.log_alpha) # entropy temperature coefficient\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # Action by the current actor for the sampled state\n",
    "      new_next_action, next_log_pi = self.actor(next_state) # make a new action on old memory\n",
    "\n",
    "      # Compute and cut quantiles at the next state\n",
    "      next_z = self.critic_target(next_state, new_next_action)  \n",
    "      \n",
    "      # Sort and drop top k quantiles to control overestimation (TQC)\n",
    "      sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "      sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop] # estimated truncated Q-val dist for next state\n",
    "\n",
    "      # td error + entropy term\n",
    "      target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "    \n",
    "    # Get current quantile estimates using action from the replay buffer\n",
    "    cur_z = self.critic(state, action)\n",
    "    critic_loss = quantile_huber_loss(cur_z, target)\n",
    "\n",
    "    new_action, log_pi = self.actor(state)\n",
    "    # detach the variable from the graph so we don't change it with other losses\n",
    "    alpha_loss = -self.log_alpha * (log_pi + self.target_entropy).detach().mean()\n",
    "\n",
    "    # Optimise critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # Soft update target networks\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Compute actor loss\n",
    "    actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "    \n",
    "    # Optimise the actor\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # Optimise the entropy coefficient\n",
    "    self.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    self.alpha_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = ActorMLP(state_dim, [512, 512], action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.mlp(obs)\n",
    "        log_std = log_std.clamp(-20, 2) # clamp for stability\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        base_N_dist = D.Normal(mean, std) # base normal dist\n",
    "        tanh_transform = TanhTransform(cache_size=1) # transform to get the tanh dist\n",
    "\n",
    "        log_prob = None\n",
    "        if self.training: # i.e. agent.train()\n",
    "            transformed_dist = D.TransformedDistribution(base_N_dist, tanh_transform) # transformed distribution\n",
    "            action = transformed_dist.rsample() # samples from base dist & applies transform\n",
    "            log_prob = transformed_dist.log_prob(action) # log prob of action after transform\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True) # sum over action dim\n",
    "        else: # evaluation mode\n",
    "            action = torch.tanh(mean)\n",
    "\n",
    "        # FROM ORIGINAL CODE using custom tanh dist\n",
    "        # if self.training:\n",
    "        #     tanh_dist = TanhNormal(mean, std) # use custom tanh dist\n",
    "        #     action, pre_tanh = tanh_dist.random_sample()\n",
    "        #     log_prob = tanh_dist.log_probability(pre_tanh)\n",
    "        #     log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        # else:\n",
    "        #     action = torch.tanh(mean)\n",
    "            \n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs)[np.newaxis, :]\n",
    "        action, log_prob = self.forward(obs)\n",
    "        return np.array(action[0].cpu().detach())\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets):\n",
    "        super().__init__()\n",
    "        self.mlp_list = []\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        for i in range(n_nets):\n",
    "            net = CriticMLP(state_dim + action_dim, [256, 256], n_quantiles)\n",
    "            self.add_module(f'net{i}', net)\n",
    "            self.mlp_list.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        quantiles = torch.stack(tuple(net(torch.cat((state, action), dim=1)) for net in self.mlp_list), dim=1)\n",
    "        return quantiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H466HICfpDB1"
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# params = {\n",
    "#   'update_every' : 50,  # Training frequency\n",
    "#   'save_interval' : int(1e5),  # Model saving interval, in steps\n",
    "#   'eval_interval' : int(2e3),  # Model evaluating interval, in steps\n",
    "#   'delay_freq' : 1,  # Delayed frequency for Actor and Target Net\n",
    "#   'gamma' : 0.99,  # Discounted Factor\n",
    "#   'net_width' : 256,  # Hidden net width\n",
    "#   'a_lr' : 1e-4,  # Learning rate of actor\n",
    "#   'c_lr' : 1e-4,  # Learning rate of critic\n",
    "#   'batch_size' : 256,  # Batch size for training\n",
    "#   'explore_noise' : 0.15,  # Exploring noise when interacting\n",
    "#   'explore_noise_decay' : 0.998,  # Decay rate of explore noise\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# agent hyperparameters\n",
    "n_quantiles = 25\n",
    "top_quantiles_to_drop_per_net = 2\n",
    "n_nets = 5\n",
    "batch_size = 256\n",
    "discount = 0.98\n",
    "tau = 0.005\n",
    "\n",
    "# dreamer hyperparameters\n",
    "batch_size_dreamer = 512\n",
    "hidden_dim = 256\n",
    "num_layers = 16\n",
    "num_heads = 4\n",
    "dropout_prob = 0.1\n",
    "window_size = 40               # transformer context window size\n",
    "step_size = 1                  # how many timesteps to skip between each context window\n",
    "train_split = 0.8              # train/validation split\n",
    "score_threshold = 0.8          # use dreamer if loss < score_threshold\n",
    "dreamer_train_epochs = 15      # how many epochs to train the dreamer model for\n",
    "dreamer_train_frequency = 10   # how often to train the dreamer model\n",
    "episode_threshold = 50         # how many episodes to run before training the dreamer model\n",
    "max_size = int(5e4)            # maximum size of the training set for the dreamer model\n",
    "\n",
    "save_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theo's hyperparams\n",
    "\n",
    "## DO NOT CHANGE ##\n",
    "seed = 42\n",
    "max_timesteps = 2000 # num timesteps per episode\n",
    "\n",
    "## Adjust as needed ##\n",
    "max_episodes = 1000\n",
    "target_score = 300 # stop training when average score over 100 episodes is > target_score\n",
    "max_total_steps = max_episodes * max_timesteps\n",
    "warm_up_steps = 10 * max_timesteps\n",
    "\n",
    "# recording/logging\n",
    "plot_interval = 10 # plot every Nth episode\n",
    "is_recording = True\n",
    "video_interval = 25 # record every Nth episode\n",
    "ep_start_rec = 100 # start recording on this episode\n",
    "\n",
    "\n",
    "\n",
    "hardcore = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1Xrcek4hxDXl",
    "outputId": "6161f148-a164-4e22-d97a-f9aa53e7b8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SETUP\n",
    "\n",
    "# make env\n",
    "if hardcore == False:\n",
    "  env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "else:\n",
    "  env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"nchw73-agent-hardcore-video\" if hardcore else \"nchw73-agent-video\",  # prefix for videos\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "rld.check_device() # training on CPU recommended\n",
    "\n",
    "env.video = False # switch video recording off (only switch on every x episodes as this is slow)\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, params['action_dim'], params['state_dim'] = rld.env_info(env, print_out=True) # note: state_dim = obs_dim\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=seed)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWjENKkU_My"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, state, info = rld.seed_everything(seed, env)\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "replay_buffer = EREReplayBuffer(params['state_dim'], params['action_dim'], max_timesteps)\n",
    "\n",
    "actor = Actor(params['state_dim'], params['action_dim'])\n",
    "\n",
    "critic = Critic(params['state_dim'], params['action_dim'], n_quantiles, n_nets)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "dreamer = DreamerAgent(params['state_dim'], params['action_dim'], hidden_dim,\n",
    "    window_size, num_layers, num_heads, dropout_prob)\n",
    "\n",
    "class_to_take_gradient_step = Gradient_Step(\n",
    "    actor=actor, critic=critic, critic_target=critic_target,\n",
    "    top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_nets, # total number of quantiles to drop\n",
    "    discount=discount, tau=tau,\n",
    "    target_entropy = -np.prod(env.action_space.shape).item(), # \n",
    "    quantiles_total = n_quantiles * n_nets\n",
    "    )\n",
    "\n",
    "actor.train()\n",
    "\n",
    "ep_timesteps = 0\n",
    "total_steps = 0\n",
    "ep_reward = 0\n",
    "memory_ptr = 0 # marks boundary between new and old data in the replay buffer\n",
    "train_set, test_set = None, None\n",
    "reward_list = []\n",
    "reward_avg_list = []\n",
    "# plot_data = []\n",
    "ep_timesteps_dreamer = ep_reward_dreamer = dreamer_eps = 0\n",
    "current_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 1 | Ep tsteps: 2000 | Reward: -67.353 | Dreamer: False\n",
      "Ep: 2 | Ep tsteps: 2000 | Reward: -69.715 | Dreamer: False\n",
      "Ep: 3 | Ep tsteps: 103 | Reward: -104.061 | Dreamer: False\n",
      "Ep: 4 | Ep tsteps: 2000 | Reward: -70.995 | Dreamer: False\n",
      "Ep: 5 | Ep tsteps: 58 | Reward: -100.139 | Dreamer: False\n",
      "Ep: 6 | Ep tsteps: 2000 | Reward: -61.235 | Dreamer: False\n",
      "Ep: 7 | Ep tsteps: 2000 | Reward: -60.125 | Dreamer: False\n",
      "Ep: 8 | Ep tsteps: 2000 | Reward: -55.060 | Dreamer: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[264], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# sample real env\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m ep_timesteps, ep_reward, input_buffer, info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_take_gradient_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_timesteps\n\u001b[1;32m     17\u001b[0m tracker\u001b[38;5;241m.\u001b[39mtrack(info) \u001b[38;5;66;03m# track statistics for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[249], line 61\u001b[0m, in \u001b[0;36mtrain_on_environment\u001b[0;34m(actor, env, class_to_take_gradient_step, replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length)\u001b[0m\n\u001b[1;32m     57\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size: \u001b[38;5;66;03m# do 1 training update using 1 batch from buffer\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# train the agent using experiences from the real environment\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mclass_to_take_gradient_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;66;03m# break if ep finished\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[257], line 53\u001b[0m, in \u001b[0;36mGradient_Step.take_gradient_step\u001b[0;34m(self, replay_buffer, t, batch_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Get current quantile estimates using action from the replay buffer\u001b[39;00m\n\u001b[1;32m     52\u001b[0m cur_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(state, action)\n\u001b[0;32m---> 53\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m \u001b[43mquantile_huber_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m new_action, log_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# detach the variable from the graph so we don't change it with other losses\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[255], line 15\u001b[0m, in \u001b[0;36mquantile_huber_loss\u001b[0;34m(quantiles, samples, sum_over_quantiles)\u001b[0m\n\u001b[1;32m     13\u001b[0m cumulative_prob \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39marange(n_quantiles, device\u001b[38;5;241m=\u001b[39mquantiles\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m/\u001b[39m n_quantiles\n\u001b[1;32m     14\u001b[0m cumulative_prob_shaped \u001b[38;5;241m=\u001b[39m cumulative_prob\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mabs(cumulative_prob_shaped \u001b[38;5;241m-\u001b[39m (\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m)\u001b[38;5;241m.\u001b[39mfloat()) \u001b[38;5;241m*\u001b[39m huber_loss)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Summing over the quantile dimension \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sum_over_quantiles:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for episode in range(1, max_episodes+1): # index from 1\n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    if is_recording and episode >= ep_start_rec:\n",
    "        env.info = episode % video_interval == 0   # track every x episodes (usually tracking every episode is fine)\n",
    "        env.video = episode % video_interval == 0  # record videos every x episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # sample real env\n",
    "    ep_timesteps, ep_reward, input_buffer, info = train_on_environment(\n",
    "        actor, env, class_to_take_gradient_step, replay_buffer, max_timesteps, state,\n",
    "        batch_size, total_steps, window_size)\n",
    "\n",
    "    total_steps += ep_timesteps\n",
    "    tracker.track(info) # track statistics for plotting\n",
    "\n",
    "    # update train/test sets for dreamer (add new eps and trim to window size)\n",
    "    train_set, test_set = gen_test_train_seq(\n",
    "        replay_buffer, train_set, test_set, train_split, window_size, step_size, memory_ptr)\n",
    "\n",
    "    # train dreamer after ep_thresh and if the input buffer has enough data to fill context window\n",
    "    if episode >= episode_threshold and input_buffer.shape[0] == window_size:\n",
    "        # train and assess the dreamer every train_frequency\n",
    "        if episode % dreamer_train_frequency == 0:\n",
    "            dreamer.train_dreamer(train_set, dreamer_train_epochs, batch_size_dreamer)\n",
    "\n",
    "        # truncate the training set to control train time performance\n",
    "        if test_set.shape[0] > max_size:\n",
    "            train_set = train_set[-max_size:]\n",
    "\n",
    "        print('Size of sequences: ', train_set.shape[0], test_set.shape[0])\n",
    "        \n",
    "        # evaluate the dreamer's performance & decide num of training steps for dreamer\n",
    "        dreamer_avg_loss = dreamer.test_dreamer(test_set, batch_size_dreamer)\n",
    "        dreamer_eps = get_dreamer_eps(dreamer_avg_loss, score_threshold)\n",
    "\n",
    "        # train on dreamer if its accurate enough\n",
    "        if dreamer_eps > 0:\n",
    "            print(f'Dreamer active for {dreamer_eps} iterations')\n",
    "            ep_timesteps_dreamer = ep_reward_dreamer = 0\n",
    "            for dep in range(dreamer_eps): # (dep = dreamer episode)\n",
    "                print(f'Dreamer ep: {dep+1}')\n",
    "\n",
    "                # initialise dreamer states with the input sequence\n",
    "                dreamer.states = input_buffer[:, :params['state_dim']]\n",
    "                dreamer.actions = input_buffer[:-1, params['state_dim']:params['state_dim']+params['action_dim']]\n",
    "                dreamer.rewards = input_buffer[:-1, -2-params['state_dim']].unsqueeze(1)\n",
    "                dreamer.dones = input_buffer[:-1, -1-params['state_dim']].unsqueeze(1)\n",
    "\n",
    "            # sample from dreamer environment (ignore input buffer and info)\n",
    "            _td, _rd, _, _ = train_on_environment(\n",
    "                actor, dreamer, class_to_take_gradient_step, replay_buffer,\n",
    "                math.ceil(total_steps/episode) - 1, # max_timesteps \n",
    "                dreamer.states[-1].cpu().numpy(),\n",
    "                batch_size, total_steps, window_size)\n",
    "\n",
    "            ep_timesteps_dreamer += _td\n",
    "            ep_reward_dreamer += _rd\n",
    "\n",
    "    memory_ptr = replay_buffer.ptr # update the memory pointer to the current position in the buffer\n",
    "\n",
    "    print(f\"Ep: {episode} | Ep tsteps: {ep_timesteps} | Reward: {ep_reward:.3f} | Dreamer: {dreamer_eps > 0}\")\n",
    "    if dreamer_eps > 0:\n",
    "        print(f\"\\t Dreamer Eps: {dreamer_eps} | Dreamer Avg Timesteps: {ep_timesteps_dreamer/dreamer_eps} | Dreamer Avg Reward: {ep_reward_dreamer/dreamer_eps}\")\n",
    "\n",
    "    reward_list.append(ep_reward)\n",
    "    reward_avg_list.append(ep_reward)\n",
    "    ep_reward = 0\n",
    "\n",
    "    # plot tracked statistics (on real env)\n",
    "    if episode % plot_interval == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "    # break condition - stop if we consistently meet target score\n",
    "    if len(reward_avg_list) >= 100:\n",
    "        print(f'Current progress: {np.array(reward_avg_list).mean()}/{target_score}')\n",
    "        if np.array(reward_avg_list).mean() >= target_score: # quit when we've got (basically) optimal performance\n",
    "            print('Completed environment!')\n",
    "            break\n",
    "        reward_avg_list = reward_avg_list[-99:] # discard oldest on list (keep most recent 99)\n",
    "\n",
    "if save_model:\n",
    "    torch.save(actor.state_dict(), './model.pth')\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file (for coursework)\n",
    "if hardcore:\n",
    "    filename = \"nchw73-agent-hardcore-log.txt\"\n",
    "else:\n",
    "    filename = \"nchw73-agent-log.txt\"\n",
    "env.write_log(folder=\"logs\", file=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
