{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "# **Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyOJa8oZPjGq"
   },
   "source": [
    "https://arena-chapter2-rl.streamlit.app/[2.2]_Q-Learning_and_DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSUGAJsTLOV",
    "outputId": "875609e2-e3eb-4b29-c03c-1989820b1ea7"
   },
   "outputs": [],
   "source": [
    "# https://github.com/robert-lieck/rldurham/tree/main - rldurham repo\n",
    "\n",
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "AFrqd24JTLOW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np # consider switching out for torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributions import Distribution, Normal\n",
    "import rldurham as rld\n",
    "\n",
    "# from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "# **RL agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIMESTEPS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "8YWZaW-YoT2j"
   },
   "outputs": [],
   "source": [
    "# HELPERS\n",
    "# for https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# def evaluate_policy(env, agent, turns=3):\n",
    "#     total_scores = 0\n",
    "#     for i in range(turns):\n",
    "#         s, _ = env.reset()\n",
    "#         for t in range(2000): # 2000 = max_timesteps (TODO - diff from Jinghao's imp)\n",
    "#             # Take deterministic actions at test time\n",
    "#             a = agent.select_action(s, deterministic=True)\n",
    "#             s_next, r, dw, tr, _ = env.step(a)\n",
    "#             done = (dw or tr)\n",
    "\n",
    "#             total_scores += r\n",
    "#             s = s_next\n",
    "\n",
    "#             if done:\n",
    "#                 break\n",
    "\n",
    "#     return int(total_scores / turns)\n",
    "\n",
    "# def adapt_reward(r):\n",
    "#     \"\"\"Reward engineering for better training.\"\"\"\n",
    "#     # For BipedalWalker (both versions)\n",
    "#     if r <= -100:\n",
    "#         r = -1\n",
    "#     return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERs\n",
    "# for https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# abstracted training loop function for both environments to use.\n",
    "# the real environment will always have MAX_TIMESTEPS as the max_timesteps value\n",
    "# while the dreamer enviroment might have less timesteps\n",
    "def train_on_environment(actor, env, class_to_take_gradient_step,\n",
    " replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length):\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "    input_buffer = torch.empty((0, 54)) # save start sequence for the dreamer model TODO - where does 54 come from?\n",
    "\n",
    "    for t in range(max_timesteps): \n",
    "        total_steps += 1\n",
    "        # select action and step the env\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "\n",
    "        # handle different return formats\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, info = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            print(\"step length is not 5\")\n",
    "            next_state, reward, done, info = step\n",
    "\n",
    "        ep_timesteps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        # change reward of real environment\n",
    "        if max_timesteps == MAX_TIMESTEPS: # i.e. if real environment? TODO - can dreamers possibly have max_timesteps?\n",
    "            # reward design NOTE justify!\n",
    "            if reward == -100.0:\n",
    "                reward = -10.0\n",
    "            else:\n",
    "                reward *= 2\n",
    "            replay_buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "        else: # must be dreamer env\n",
    "            if t == sequence_length:\n",
    "                for row in input_buffer.cpu().numpy():\n",
    "                    replay_buffer.add(row[:24], row[24:28], row[28:52], row[52], row[53])\n",
    "                    # TODO - why does this ^ use slices? Where do these numbers come from?\n",
    "            elif t > sequence_length:\n",
    "                replay_buffer.add(state, action, next_state, reward, done)\n",
    "            # NOTE - don't store if simulation ends without reaching the sequence length\n",
    "        \n",
    "        if t < sequence_length: # store in input buffer TODO what does this do? why's it needed?\n",
    "            input_buffer = torch.cat([\n",
    "                input_buffer,\n",
    "                torch.tensor(\n",
    "                    np.concatenate((state, action, next_state, np.array([reward]), np.array([done])),\n",
    "                    axis=0)\n",
    "                    ).unsqueeze(0)\n",
    "                ], axis=0)        \n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "        if total_steps >= batch_size: # do 1 training update using 1 batch from buffer\n",
    "            # train the agent using experiences from the real environment\n",
    "            class_to_take_gradient_step.take_gradient_step(replay_buffer, t, batch_size)\n",
    "    \n",
    "        if done: # break if ep finished\n",
    "            break\n",
    "\n",
    "    if sequence_length > 0: # TODO - this means...? which env are we in?\n",
    "        return ep_timesteps, ep_reward, input_buffer, info\n",
    "    return ep_timesteps, ep_reward, None, info\n",
    "\n",
    "\n",
    "# test loop for agent on environment\n",
    "def simulate_on_environment(actor, env, max_timesteps, state):\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "    for t in range(max_timesteps):\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, _ = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, _ = step\n",
    "        ep_timesteps += 1\n",
    "    \n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "    \n",
    "        if done or t == max_timesteps - 1:\n",
    "            break\n",
    "    return ep_timesteps, ep_reward\n",
    "\n",
    "# generate value array from replay buffer\n",
    "def retrieve_from_replay_buffer(replay_buffer, ptr):\n",
    "    '''\n",
    "    '''\n",
    "    return np.concatenate((\n",
    "                    replay_buffer.state[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.action[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.reward[ptr:replay_buffer.ptr],\n",
    "                    1. - replay_buffer.not_done[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.next_state[ptr:replay_buffer.ptr],                \n",
    "                ), \n",
    "                axis = 1 # along the columns (so each row is a memory)\n",
    "            )\n",
    "\n",
    "# function to create sequences with a given window size and step size\n",
    "def create_sequences(memories, window_size, step_size):\n",
    "    '''\n",
    "    Function to create sequences of memories from the replay buffer.\n",
    "\n",
    "    Each sequence is of length window_size and each spaced apart by step_size\n",
    "    '''\n",
    "    n_memories = memories.shape[0] # just the number of time steps currently in the buffer\n",
    "\n",
    "    # calc number of seq (of len window_size) we can create from mems available  \n",
    "    n_sequences = math.floor((n_memories - window_size) / step_size) + 1 # +1 because indices start at 0\n",
    "\n",
    "    sequences = np.zeros((n_sequences, window_size, memories.shape[1]))\n",
    "    for i in range(n_sequences):\n",
    "        start_idx = i * step_size # idx to start seq from\n",
    "        sequences[i, :] = memories[start_idx:start_idx + window_size, :] # grab seq of memories window_size long from start_idx\n",
    "    return sequences\n",
    "\n",
    "def gen_test_train_seq(replay_buffer, train_set, test_set, train_split, window_size, step_size, ptr):\n",
    "    '''\n",
    "    Function to split train and test data\n",
    "    '''\n",
    "    memories = retrieve_from_replay_buffer(replay_buffer, ptr)\n",
    "    try:\n",
    "        memory_sequences = create_sequences(memories, window_size, step_size)\n",
    "        n_sequences = memory_sequences.shape[0]\n",
    "    except: # TODO How might this go wrong? not enough new data to create a sequence?\n",
    "        return train_set, test_set\n",
    "\n",
    "    # shuffle the sequences & split\n",
    "    indices = np.arange(n_sequences)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(train_split * n_sequences) # get split point \n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "\n",
    "    if train_set is None: # if this is first train/test set, create the sets\n",
    "        return memory_sequences[train_indices, :], memory_sequences[test_indices, :]\n",
    "    # else just add to existing sets\n",
    "    return np.concatenate((train_set, memory_sequences[train_indices, :]), axis=0), np.concatenate((test_set, memory_sequences[test_indices, :]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dreamer_eps(dreamer_avg_loss, score_threshold):\n",
    "    '''\n",
    "    Get number of dreamed episdoes the agent should run on the dreamer model.\n",
    "\n",
    "    Only use dreamer for trainibg if it's sufficiently accurate model of env.\n",
    "\n",
    "    Note - score_threshold is between 0 and 1.\n",
    "    '''\n",
    "    max_dreamer_it = 10 # a perfect dreamer has this many eps\n",
    "\n",
    "    if dreamer_avg_loss >= score_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        norm_score = dreamer_avg_loss/score_threshold # normalise score relative to threshold\n",
    "        inv_score = 1 - norm_score # invert so that closer to 1 ==> dreamer score much better than threshold\n",
    "        sq_score = inv_score**2 # square so that num iter increases quadratically as accuracy improves\n",
    "        return int(max_dreamer_it * sq_score) # scale so that max iterations is 10 (when dreamer very accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "9kTeUHYL7gU6"
   },
   "outputs": [],
   "source": [
    "class EREReplayBuffer(object):\n",
    "    '''\n",
    "    implementation - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    An ERE buffer is implemented to improve sample efficiency, \n",
    "        for which the paper can be found here: https://arxiv.org/abs/1906.04009\n",
    "\n",
    "    Rationale:\n",
    "    - actions from early in training are likely v different to optimal, so want to prioritise recent ones so we're not constantly learning from crap ones\n",
    "    - as time goes on, they get more similar so annealing eta allows uniform sampling later on\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, T, max_size=int(1e6), eta=0.996, cmin=5000):\n",
    "        self.max_size, self.ptr, self.size, self.rollover = max_size, 0, 0, False\n",
    "        self.eta0 = eta # 0.996 - 0.999 is good (according to paper)\n",
    "        self.cmin = cmin\n",
    "        self.c_list = []\n",
    "        self.index = []\n",
    "        self.T = T\n",
    "\n",
    "        self.reward = np.empty((max_size, 1))\n",
    "        self.state = np.empty((max_size, state_dim))\n",
    "        self.action = np.empty((max_size, action_dim))\n",
    "        self.not_done = np.empty((max_size, 1))\n",
    "        self.next_state = np.empty((max_size, state_dim))\n",
    "        \n",
    "    def sample(self, batch_size, t):\n",
    "        # update eta value for current timestep\n",
    "        eta = self.eta_anneal(t)\n",
    "\n",
    "        index = np.array([self._get_index(eta, k, batch_size) for k in range(batch_size)])\n",
    "\n",
    "        r = torch.tensor(self.reward[index], dtype = torch.float)\n",
    "        s = torch.tensor(self.state[index], dtype = torch.float)\n",
    "        ns = torch.tensor(self.next_state[index], dtype = torch.float)\n",
    "        a = torch.tensor(self.action[index], dtype = torch.float)\n",
    "        nd = torch.tensor(self.not_done[index], dtype = torch.float)\n",
    "        \n",
    "        return s, a, ns, r, nd\n",
    "    \n",
    "    def _get_index(self, eta, k, batch_size):\n",
    "        c_calc = self.size * eta ** (k * 1000 / batch_size)\n",
    "        ck = c_calc if c_calc > self.cmin else self.size\n",
    "\n",
    "        if not self.rollover: # if we're not overwriting yet, ...\n",
    "            return np.random.randint(self.size - ck, self.size)\n",
    "        \n",
    "        return np.random.randint(self.ptr + self.size - ck, self.ptr + self.size) % self.size\n",
    "\n",
    "    def eta_anneal(self, t):\n",
    "        # eta anneals over time --> 1, which reduces emphasis on recent experiences over time\n",
    "        return self.eta0 + (1 - self.eta0) * t / self.T\n",
    " \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        # Add experience to replay buffer \n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.ptr %= self.max_size\n",
    "\n",
    "        # increase size counter until full, then start overwriting (rollover)\n",
    "        if self.max_size > self.size + 1:\n",
    "          self.size += 1\n",
    "        else:\n",
    "          self.size = self.max_size\n",
    "          self.rollover = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "i35_XJdr9LoC"
   },
   "outputs": [],
   "source": [
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# actor and critic\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, net_width, maxaction):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, net_width)\n",
    "        self.l2 = nn.Linear(net_width, net_width)\n",
    "        self.l3 = nn.Linear(net_width, action_dim)\n",
    "        self.maxaction = maxaction\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = torch.tanh(self.l1(state))\n",
    "        a = torch.tanh(self.l2(a))\n",
    "        a = torch.tanh(self.l3(a)) * self.maxaction\n",
    "        return a\n",
    "\n",
    "class Double_Q_Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, net_width):\n",
    "        super(Double_Q_Critic, self).__init__()\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, net_width)\n",
    "        self.l2 = nn.Linear(net_width, net_width)\n",
    "        self.l3 = nn.Linear(net_width, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, net_width)\n",
    "        self.l5 = nn.Linear(net_width, net_width)\n",
    "        self.l6 = nn.Linear(net_width, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "s04vKGU3dNuF"
   },
   "outputs": [],
   "source": [
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# class TD3_agent():\n",
    "#     def __init__(self, **kwargs):\n",
    "#         # Init hyperparameters for agent, just like \"self.gamma = opt.gamma, self.lambd = opt.lambd, ...\"\n",
    "#         self.__dict__.update(kwargs)\n",
    "#         self.policy_noise = 0.2 * self.max_action\n",
    "#         self.noise_clip = 0.5 * self.max_action\n",
    "#         self.tau = 0.005\n",
    "#         self.delay_counter = 0\n",
    "\n",
    "#         # Actor network and its target\n",
    "#         self.actor = Actor(self.state_dim, self.action_dim, self.net_width, self.max_action)\n",
    "#         self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.a_lr)\n",
    "#         self.actor_target = copy.deepcopy(self.actor)\n",
    "\n",
    "#         # Double Q-Critic network and its target\n",
    "#         self.q_critic = Double_Q_Critic(self.state_dim, self.action_dim, self.net_width)\n",
    "#         self.q_critic_optimizer = torch.optim.Adam(self.q_critic.parameters(), lr=self.c_lr)\n",
    "#         self.q_critic_target = copy.deepcopy(self.q_critic)\n",
    "\n",
    "#         self.replay_buffer = ReplayBuffer(self.state_dim, self.action_dim, max_size=int(1e6))\n",
    "\n",
    "#     def select_action(self, state, deterministic):\n",
    "#         with torch.no_grad():\n",
    "#             state = torch.FloatTensor(state[np.newaxis, :]) # from [x,x,...,x] to [[x,x,...,x]]\n",
    "#             a = self.actor(state).cpu().numpy()[0] # from [[x,x,...,x]] to [x,x,...,x]\n",
    "#             if deterministic:\n",
    "#                 return a\n",
    "#             else:\n",
    "#                 noise = np.random.normal(0, self.max_action * self.explore_noise, size=self.action_dim)\n",
    "#                 return (a + noise).clip(-self.max_action, self.max_action)\n",
    "\n",
    "#     def train(self):\n",
    "#         self.delay_counter += 1\n",
    "#         with torch.no_grad():\n",
    "#             s, a, r, s_next, dw = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "#             # Compute target Q using target actor and target critic networks\n",
    "#             target_a_noise = (torch.randn_like(a) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "#             '''↓↓↓ Target Policy Smoothing Regularization ↓↓↓'''\n",
    "#             smoothed_target_a = (self.actor_target(s_next) + target_a_noise).clamp(-self.max_action, self.max_action)\n",
    "#             target_Q1, target_Q2 = self.q_critic_target(s_next, smoothed_target_a)\n",
    "#             '''↓↓↓ Clipped Double Q-learning ↓↓↓'''\n",
    "#             target_Q = torch.min(target_Q1, target_Q2)\n",
    "#             target_Q = r + (~dw) * self.gamma * target_Q # dw: die or win\n",
    "\n",
    "#         current_Q1, current_Q2 = self.q_critic(s, a) # Get current Q estimates\n",
    "\n",
    "#         # Compute critic loss, and optimize the q_critic\n",
    "#         q_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "#         self.q_critic_optimizer.zero_grad()\n",
    "#         q_loss.backward()\n",
    "#         self.q_critic_optimizer.step()\n",
    "\n",
    "#         # Delayed policy updates\n",
    "#         '''↓↓↓ Clipped Double Q-learning ↓↓↓'''\n",
    "#         if self.delay_counter > self.delay_freq:\n",
    "#             # Update the Actor\n",
    "#             a_loss = -self.q_critic.Q1(s, self.actor(s)).mean()\n",
    "#             self.actor_optimizer.zero_grad()\n",
    "#             a_loss.backward()\n",
    "#             self.actor_optimizer.step()\n",
    "\n",
    "#             # Update the frozen target models\n",
    "#             with torch.no_grad():\n",
    "#                 for param, target_param in zip(self.q_critic.parameters(), self.q_critic_target.parameters()):\n",
    "#                     target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "#                 for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "#                     target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "#             self.delay_counter = 0\n",
    "\n",
    "#     def save(self, EnvName, timestep):\n",
    "#         torch.save(self.actor.state_dict(), f\"./model/{EnvName}_actor{timestep}.pth\")\n",
    "#         torch.save(self.q_critic.state_dict(), f\"./model/{EnvName}_q_critic{timestep}.pth\")\n",
    "\n",
    "#     def load(self, EnvName, timestep):\n",
    "#         self.actor.load_state_dict(torch.load(f\"./model/{EnvName}_actor{timestep}.pth\"))\n",
    "#         self.q_critic.load_state_dict(torch.load(f\"./model/{EnvName}_q_critic{timestep}.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dreamer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class DreamerAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, seq_len, num_layers, num_heads, dropout_prob, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.input_dim = state_dim + action_dim + 2\n",
    "        self.target_dim = self.state_dim + self.action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.input_fc = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.target_fc = nn.Linear(self.target_dim, hidden_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            num_heads, \n",
    "            dropout=dropout_prob,\n",
    "            activation=F.gelu,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_next_state = nn.Linear(hidden_dim + self.target_dim, state_dim)\n",
    "        self.output_reward = nn.Linear(hidden_dim + self.target_dim, 1)\n",
    "        self.output_done = nn.Linear(hidden_dim + self.target_dim, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=3e-4)\n",
    "        \n",
    "    # separate out the ground truth variables and compare against predictions\n",
    "    def loss_fn(self, output_next_state, output_reward, output_done, ground_truth):\n",
    "        reward, done, next_state = torch.split(ground_truth, [1, 1, self.state_dim], dim=-1)\n",
    "        loss = self.mse_loss(output_next_state[:, -1], next_state)\n",
    "        loss += self.mse_loss(output_reward[:, -1], reward)\n",
    "        loss += self.ce_loss(output_done[:, -1], done)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # separate the input and target tensors\n",
    "        target = input_tensor[:, -1, :self.target_dim].unsqueeze(1)\n",
    "        encoded_target = self.target_fc(target)\n",
    "        encoded_input = self.input_fc(input_tensor[:, :-1, :self.input_dim])\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target], axis=2))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target], axis=2))\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target], axis=2)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "    \n",
    "    def predict(self, input_tensor, target_tensor):\n",
    "        # separate the input and target tensors\n",
    "        encoded_target = self.target_fc(target_tensor)\n",
    "        encoded_input = self.input_fc(input_tensor)\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target_tensor], axis=1)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "\n",
    "    # transformer training loop\n",
    "    # sequences shape: (batch, sequence, features)\n",
    "    def train_dreamer(self, sequences, epochs, batch_size=256):\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float)\n",
    "        \n",
    "        train_dataset = TensorDataset(inputs)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(train_dataloader):\n",
    "                input_batch = input_batch[0]\n",
    "                self.optimizer.zero_grad()\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, running_loss / len(train_dataloader)))\n",
    "\n",
    "    # transformer testing loop\n",
    "    def test_dreamer(self, sequences, batch_size=64):\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float)\n",
    "        \n",
    "        test_dataset = TensorDataset(inputs)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(test_dataloader):\n",
    "                input_batch = input_batch[0]\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                running_loss += loss.item()\n",
    "            print('Test Loss: {:.4f}'.format(running_loss / len(test_dataloader)))\n",
    "        return running_loss / len(test_dataloader)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.actions = torch.cat([self.actions, torch.tensor(np.array([action]))], axis=0)\n",
    "        input_sequence = torch.cat([self.states[:-1], self.actions[:-1], self.rewards, self.dones], axis=1).to(torch.float32)\n",
    "        target = torch.cat([self.states[-1], self.actions[-1]], axis=0).unsqueeze(0).to(torch.float32)\n",
    "        with torch.no_grad():\n",
    "            next_state, reward, done = self.predict(input_sequence, target)\n",
    "            done = (done >= 0.6).float() # bias towards not done to avoid false positives\n",
    "\n",
    "            self.states = torch.cat([self.states, next_state], axis=0)\n",
    "            self.rewards = torch.cat([self.rewards, reward], axis=0)\n",
    "            self.dones = torch.cat([self.dones, done], axis=0)\n",
    "            \n",
    "            if self.states.shape[0] > self.seq_len:\n",
    "                self.states = self.states[1:]\n",
    "                self.rewards = self.rewards[1:]\n",
    "                self.dones = self.dones[1:]\n",
    "            if self.actions.shape[0] > self.seq_len - 1:\n",
    "                self.actions = self.actions[1:]\n",
    "        \n",
    "        return next_state.squeeze(0).cpu().numpy(), reward.cpu().item(), done.cpu().item(), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "def quantile_huber_loss(quantiles, samples, sum_over_quantiles = False):\n",
    "    #return huber loss - uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise\n",
    "    delta = samples[:, np.newaxis, np.newaxis, :] - quantiles[:, :, :, np.newaxis]  \n",
    "    abs_delta = torch.abs(delta)\n",
    "    huber_loss = torch.where(abs_delta > 1, abs_delta - 0.5, delta ** 2 * 0.5)\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "    cumulative_prob = (torch.arange(n_quantiles, device=quantiles.device, dtype=torch.float) + 0.5) / n_quantiles\n",
    "    cumulative_prob_shaped = cumulative_prob.view(1, 1, -1, 1)\n",
    "    loss = (torch.abs(cumulative_prob_shaped - (delta < 0).float()) * huber_loss)\n",
    "\n",
    "    # Summing over the quantile dimension \n",
    "    if sum_over_quantiles:\n",
    "        loss = loss.sum(dim=-2).mean()\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actor-critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# MLP for critic that implements D2RL architecture \n",
    "class CriticMLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_sizes,output_size):\n",
    "        super().__init__()\n",
    "        input_size_ = input_size\n",
    "        input_dim = 28 + hidden_sizes[0] \n",
    "        self.list_of_layers = []\n",
    "        for i, next_size in enumerate(hidden_sizes):\n",
    "            if i == 0:\n",
    "              lay = nn.Linear(input_size_, next_size)\n",
    "            else: \n",
    "              lay = nn.Linear(input_dim, next_size)\n",
    "            self.add_module(f'layer{i}', lay)\n",
    "            self.list_of_layers.append(lay)\n",
    "        self.last_layer = nn.Linear(input_dim, output_size)\n",
    "    \n",
    "\n",
    "    def forward(self, input_):\n",
    "        curr = input_\n",
    "        for lay in self.list_of_layers:\n",
    "            curr_ = F.gelu(lay(curr))\n",
    "            curr = torch.cat([curr_, input_], dim = 1)\n",
    "        output = self.last_layer(curr)\n",
    "        return output\n",
    "\n",
    "\n",
    "# MLP for actor that implements D2RL architecture\n",
    "class ActorMLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_sizes,output_size):\n",
    "        super().__init__()\n",
    "        self.list_of_layers = []\n",
    "        input_size_ = input_size\n",
    "        num_inputs = 24 \n",
    "        input_dim = hidden_sizes[0] + num_inputs\n",
    "        for i, next_size in enumerate(hidden_sizes):\n",
    "            if i == 0:\n",
    "              lay = nn.Linear(input_size_, next_size)\n",
    "            else:\n",
    "              lay = nn.Linear(input_dim, next_size)\n",
    "            self.add_module(f'layer{i}', lay)\n",
    "            self.list_of_layers.append(lay)\n",
    "            input_size_ = next_size\n",
    "            \n",
    "        self.last_layer_mean_linear = nn.Linear(input_dim, output_size)\n",
    "        self.last_layer_log_std_linear = nn.Linear(input_dim, output_size)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        curr = input_\n",
    "\n",
    "        for layer in self.list_of_layers:\n",
    "            intermediate = layer(curr)\n",
    "            curr = F.gelu(intermediate)\n",
    "\n",
    "            curr = torch.cat([curr, input_], dim=1)\n",
    "\n",
    "        mean_linear = self.last_layer_mean_linear(curr)\n",
    "        log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "        return mean_linear, log_std_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class TanhNormal(Distribution):\n",
    "    def __init__(self, normal_mean, normal_std):\n",
    "        super().__init__()\n",
    "        self.normal_mean = normal_mean\n",
    "        self.normal_std = normal_std\n",
    "        self.normal = Normal(normal_mean, normal_std)\n",
    "        self.stand_normal = Normal(torch.zeros_like(self.normal_mean), torch.ones_like(self.normal_std))        \n",
    "        \n",
    "    def logsigmoid(tensor):\n",
    "      denominator = 1 + torch.exp(-tensor)\n",
    "      return torch.log(1/ denominator)\n",
    "\n",
    "    def log_probability(self, pre_tanh):\n",
    "        final = (self.normal.log_prob(pre_tanh)) - (2 * np.log(2) + F.logsigmoid(2 * pre_tanh) + F.logsigmoid(-2 * pre_tanh))\n",
    "        return final\n",
    "\n",
    "    def random_sample(self):\n",
    "        pretanh = self.normal_mean + self.normal_std * self.stand_normal.sample()\n",
    "        return torch.tanh(pretanh), pretanh\n",
    "  \n",
    "\n",
    "class Gradient_Step(object):\n",
    "  def __init__(\n",
    "    self,\n",
    "    *,\n",
    "    actor,\n",
    "    critic,\n",
    "    critic_target,\n",
    "    discount,\n",
    "    tau,\n",
    "    top_quantiles_to_drop,\n",
    "    target_entropy,\n",
    "    quantiles_total\n",
    "  ):\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.critic_target = critic_target\n",
    "    self.log_alpha = torch.zeros((1,), requires_grad=True)\n",
    "    self.quantiles_total = quantiles_total\n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "    \n",
    "    self.alpha_optimizer = optim.Adam([self.log_alpha], lr=3e-4)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "    self.discount, self.tau, self.top_quantiles_to_drop, self.target_entropy  = discount, tau, top_quantiles_to_drop, target_entropy\n",
    "\n",
    "\n",
    "  def take_gradient_step(self, replay_buffer, t, batch_size=256):\n",
    "    # Sample replay buffer\n",
    "    state, action, next_state, reward, not_done = replay_buffer.sample(batch_size, t)\n",
    "    alpha = torch.exp(self.log_alpha) #entropy temperature coefficient\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # Action by the current actor for the sampled state\n",
    "      new_next_action, next_log_pi = self.actor(next_state)\n",
    "\n",
    "      # Compute and cut quantiles at the next state\n",
    "      next_z = self.critic_target(next_state, new_next_action)  \n",
    "      \n",
    "      # Sort and drop top k quantiles to control overestimation.\n",
    "      sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "      sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop]\n",
    "\n",
    "      # td error + entropy term\n",
    "      target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "    \n",
    "    # Get current Quantile estimates using action from the replay buffer\n",
    "    cur_z = self.critic(state, action)\n",
    "    critic_loss = quantile_huber_loss(cur_z, target)\n",
    "\n",
    "\n",
    "    new_action, log_pi = self.actor(state)\n",
    "    # detach the variable from the graph so we don't change it with other losses\n",
    "    alpha_loss = -self.log_alpha * (log_pi + self.target_entropy).detach().mean()\n",
    "\n",
    "    # Optimise critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # Update target networks\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Compute actor loss\n",
    "    actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "    \n",
    "    # Optimise the actor\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # Optimise the entropy coefficient\n",
    "    self.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    self.alpha_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = ActorMLP(state_dim, [512, 512], action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.mlp(obs)\n",
    "        log_std = log_std.clamp(-20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        log_prob = None\n",
    "        if self.training == False: \n",
    "            action = torch.tanh(mean)\n",
    "        elif self.training == True:\n",
    "            tanh_dist = TanhNormal(mean, std)\n",
    "            action, pre_tanh = tanh_dist.random_sample()\n",
    "            log_prob = tanh_dist.log_probability(pre_tanh)\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True)      \n",
    "        else:  \n",
    "            print('Something wrong with training mode')\n",
    "            \n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs)[np.newaxis, :]\n",
    "        action, log_prob = self.forward(obs)\n",
    "        return np.array(action[0].cpu().detach())\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets):\n",
    "        super().__init__()\n",
    "        self.list_of_mlp = []\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        for i in range(n_nets):\n",
    "            net = CriticMLP(state_dim + action_dim, [256, 256], n_quantiles)\n",
    "            self.add_module(f'net{i}', net)\n",
    "            self.list_of_mlp.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        quantiles = torch.stack(tuple(net(torch.cat((state, action), dim=1)) for net in self.list_of_mlp), dim=1)\n",
    "        return quantiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "H466HICfpDB1"
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "\n",
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "params = {\n",
    "  'Loadmodel': False,  # Load pretrained model or Not\n",
    "  'ModelIdex': 30,  # Which model to load\n",
    "  'update_every' : 50,  # Training frequency\n",
    "  'save_interval' : int(1e5),  # Model saving interval, in steps\n",
    "  'eval_interval' : int(2e3),  # Model evaluating interval, in steps\n",
    "  'delay_freq' : 1,  # Delayed frequency for Actor and Target Net\n",
    "  'gamma' : 0.99,  # Discounted Factor\n",
    "  'net_width' : 256,  # Hidden net width\n",
    "  'a_lr' : 1e-4,  # Learning rate of actor\n",
    "  'c_lr' : 1e-4,  # Learning rate of critic\n",
    "  'batch_size' : 256,  # Batch size for training\n",
    "  'explore_noise' : 0.15,  # Exploring noise when interacting\n",
    "  'explore_noise_decay' : 0.998,  # Decay rate of explore noise\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "plot_interval = 10 # update the plot every N episodes\n",
    " # videos can take a very long time to render so only do it every N episodes\n",
    "\n",
    "# agent hyperparameters\n",
    "n_quantiles = 25\n",
    "top_quantiles_to_drop_per_net = 2\n",
    "n_nets = 5\n",
    "batch_size = 256\n",
    "discount = 0.98\n",
    "tau = 0.005\n",
    "\n",
    "# dreamer hyperparameters\n",
    "batch_size_dreamer = 512\n",
    "hidden_dim = 256\n",
    "num_layers = 16\n",
    "num_heads = 4\n",
    "dropout_prob = 0.1\n",
    "window_size = 40               # transformer context window size\n",
    "step_size = 1                  # how many timesteps to skip between each context window\n",
    "train_split = 0.80             # train/validation split\n",
    "score_threshold = 0.8          # use dreamer if loss < score_threshold\n",
    "dreamer_train_epochs = 15      # how many epochs to train the dreamer model for\n",
    "dreamer_train_frequency = 10   # how often to train the dreamer model\n",
    "episode_threshold = 50         # how many episodes to run before training the dreamer model\n",
    "max_size = int(5e4)            # maximum size of the training set for the dreamer model\n",
    "\n",
    "save_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theo's hyperparams\n",
    "\n",
    "## DO NOT CHANGE ##\n",
    "seed = 42\n",
    "max_timesteps = 2000 # num timesteps per episode\n",
    "\n",
    "## Adjust as needed ##\n",
    "max_episodes = 1000\n",
    "target_score = 300 # stop training when average score over 100 episodes is > target_score\n",
    "max_total_steps = max_episodes * max_timesteps\n",
    "warm_up_steps = 10 * max_timesteps\n",
    "\n",
    "# recording\n",
    "is_recording = True\n",
    "video_every = 25 # record every Nth episode\n",
    "ep_start_rec = 100 # start recording on this episode\n",
    "\n",
    "\n",
    "hardcore = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1Xrcek4hxDXl",
    "outputId": "6161f148-a164-4e22-d97a-f9aa53e7b8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SETUP\n",
    "\n",
    "# make env\n",
    "if hardcore == False:\n",
    "  env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "else:\n",
    "  env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"nchw73-agent-hardcore-video\" if hardcore else \"nchw73-agent-video\",  # prefix for videos\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, params['action_dim'], params['state_dim'] = rld.env_info(env, print_out=True) # note: state_dim = obs_dim\n",
    "params['max_action'] = float(env.action_space.high[0]) # Jinghao: this is used to scale the agent's actions to ensure they stay in action space\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=seed)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWjENKkU_My"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, state, info = rld.seed_everything(seed, env)\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "env.video = False # switch video recording off (only switch on every x episodes as this is slow)\n",
    "\n",
    "replay_buffer = EREReplayBuffer(params['state_dim'], params['action_dim'], max_timesteps)\n",
    "\n",
    "actor = Actor(params['state_dim'], params['action_dim'])\n",
    "\n",
    "critic = Critic(params['state_dim'], params['action_dim'], n_quantiles, n_nets)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "dreamer = DreamerAgent(params['state_dim'], params['action_dim'], hidden_dim, window_size, num_layers, num_heads, dropout_prob)\n",
    "\n",
    "class_to_take_gradient_step = Gradient_Step(\n",
    "    actor=actor, critic=critic, critic_target=critic_target,\n",
    "    top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_nets, # total number of quantiles to drop\n",
    "    discount=discount, tau=tau,\n",
    "    target_entropy=-np.prod(env.action_space.shape).item(), \n",
    "    quantiles_total = n_quantiles * n_nets)\n",
    "\n",
    "actor.train()\n",
    "\n",
    "ep_timesteps = 0\n",
    "total_steps = 0\n",
    "ep_reward = 0\n",
    "memory_ptr = 0 # marks boundary between new and old data in the replay buffer\n",
    "train_set, test_set = None, None\n",
    "reward_list = []\n",
    "reward_avg_list = []\n",
    "# plot_data = []\n",
    "ep_timesteps_dreamer = ep_reward_dreamer = dreamer_eps = 0\n",
    "current_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 10 | Episode T: 50 | Reward: -111.711 | Dreamer: False\n",
      "Ep: 11 | Episode T: 53 | Reward: -112.237 | Dreamer: False\n",
      "Ep: 12 | Episode T: 62 | Reward: -112.441 | Dreamer: False\n",
      "Ep: 13 | Episode T: 2000 | Reward: -45.276 | Dreamer: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[233], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# sample real env\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m ep_timesteps, ep_reward, input_buffer, info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_take_gradient_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_timesteps\n\u001b[1;32m     17\u001b[0m tracker\u001b[38;5;241m.\u001b[39mtrack(info) \u001b[38;5;66;03m# track statistics for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[218], line 61\u001b[0m, in \u001b[0;36mtrain_on_environment\u001b[0;34m(actor, env, class_to_take_gradient_step, replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length)\u001b[0m\n\u001b[1;32m     57\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size: \u001b[38;5;66;03m# do 1 training update using 1 batch from buffer\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# train the agent using experiences from the real environment\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mclass_to_take_gradient_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;66;03m# break if ep finished\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[226], line 70\u001b[0m, in \u001b[0;36mGradient_Step.take_gradient_step\u001b[0;34m(self, replay_buffer, t, batch_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Get current Quantile estimates using action from the replay buffer\u001b[39;00m\n\u001b[1;32m     69\u001b[0m cur_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(state, action)\n\u001b[0;32m---> 70\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m \u001b[43mquantile_huber_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m new_action, log_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# detach the variable from the graph so we don't change it with other losses\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[224], line 7\u001b[0m, in \u001b[0;36mquantile_huber_loss\u001b[0;34m(quantiles, samples, sum_over_quantiles)\u001b[0m\n\u001b[1;32m      5\u001b[0m delta \u001b[38;5;241m=\u001b[39m samples[:, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis, :] \u001b[38;5;241m-\u001b[39m quantiles[:, :, :, np\u001b[38;5;241m.\u001b[39mnewaxis]  \n\u001b[1;32m      6\u001b[0m abs_delta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(delta)\n\u001b[0;32m----> 7\u001b[0m huber_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabs_delta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabs_delta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m n_quantiles \u001b[38;5;241m=\u001b[39m quantiles\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      9\u001b[0m cumulative_prob \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39marange(n_quantiles, device\u001b[38;5;241m=\u001b[39mquantiles\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m/\u001b[39m n_quantiles\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training procedure\n",
    "for episode in range(1, max_episodes+1): # index from 1\n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    if is_recording and episode >= ep_start_rec:\n",
    "        env.info = episode % video_every == 0   # track every x episodes (usually tracking every episode is fine)\n",
    "        env.video = episode % video_every == 0  # record videos every x episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # sample real env\n",
    "    ep_timesteps, ep_reward, input_buffer, info = train_on_environment(\n",
    "        actor, env, class_to_take_gradient_step, replay_buffer, max_timesteps, state,\n",
    "        batch_size, total_steps, window_size)\n",
    "\n",
    "    total_steps += ep_timesteps\n",
    "    tracker.track(info) # track statistics for plotting\n",
    "\n",
    "    # update train/test sets for dreamer (add new eps and trim to window size)\n",
    "    train_set, test_set = gen_test_train_seq(\n",
    "        replay_buffer, train_set, test_set, train_split, window_size, step_size, memory_ptr)\n",
    "\n",
    "    # train dreamer after ep_thresh and if the input buffer has enough data to fill context window\n",
    "    if episode >= episode_threshold and input_buffer.shape[0] == window_size:\n",
    "        # train and assess the dreamer every train_frequency\n",
    "        if episode % dreamer_train_frequency == 0:\n",
    "            dreamer.train_dreamer(train_set, dreamer_train_epochs, batch_size_dreamer)\n",
    "\n",
    "        # truncate the training set to control train time performance\n",
    "        if test_set.shape[0] > max_size:\n",
    "            train_set = train_set[-max_size:]\n",
    "\n",
    "        print('Size of sequences: ', train_set.shape[0], test_set.shape[0])\n",
    "        \n",
    "        # evaluate the dreamer's performance & decide num of training steps for dreamer\n",
    "        dreamer_avg_loss = dreamer.test_dreamer(test_set, batch_size_dreamer)\n",
    "        dreamer_eps = get_dreamer_eps(dreamer_avg_loss, score_threshold)\n",
    "\n",
    "        # train on dreamer if its accurate enough\n",
    "        if dreamer_eps > 0:\n",
    "            print(f'Dreamer active for {dreamer_eps} iterations')\n",
    "            ep_timesteps_dreamer = ep_reward_dreamer = 0\n",
    "            for dep in range(dreamer_eps): # (dep = dreamer episode)\n",
    "                print(f'Dreamer ep: {dep+1}')\n",
    "\n",
    "                # initialise dreamer states with the input sequence\n",
    "                dreamer.states = input_buffer[:, :params['state_dim']]\n",
    "                dreamer.actions = input_buffer[:-1, params['state_dim']:params['state_dim']+params['action_dim']]\n",
    "                dreamer.rewards = input_buffer[:-1, -2-params['state_dim']].unsqueeze(1)\n",
    "                dreamer.dones = input_buffer[:-1, -1-params['state_dim']].unsqueeze(1)\n",
    "\n",
    "            # sample from dreamer environment (ignore input buffer and info)\n",
    "            _td, _rd, _, _ = train_on_environment(\n",
    "                actor, dreamer, class_to_take_gradient_step, replay_buffer,\n",
    "                math.ceil(total_steps/episode) - 1, dreamer.states[-1].cpu().numpy(),\n",
    "                batch_size, total_steps, window_size)\n",
    "\n",
    "            ep_timesteps_dreamer += _td\n",
    "            ep_reward_dreamer += _rd\n",
    "\n",
    "    memory_ptr = replay_buffer.ptr # update the memory pointer to the current position in the buffer\n",
    "\n",
    "    print(f\"Ep: {episode} | Ep tsteps: {ep_timesteps} | Reward: {ep_reward:.3f} | Dreamer: {dreamer_eps > 0}\")\n",
    "    if dreamer_eps > 0:\n",
    "        print(f\"\\t Dreamer Eps: {dreamer_eps} | Dreamer Avg Timesteps: {ep_timesteps_dreamer/dreamer_eps} | Dreamer Avg Reward: {ep_reward_dreamer/dreamer_eps}\")\n",
    "\n",
    "    reward_list.append(ep_reward)\n",
    "    reward_avg_list.append(ep_reward)\n",
    "    ep_reward = 0\n",
    "\n",
    "    # plot tracked statistics (on real env)\n",
    "    if episode % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "    # break condition - stop if we consistently meet target score\n",
    "    if len(reward_avg_list) == 100:\n",
    "        print(f'Current progress: {np.array(reward_avg_list).mean()}/{target_score}')\n",
    "        if np.array(reward_avg_list).mean() >= target_score: # quit when we've got (basically) optimal performance\n",
    "            print('Completed environment!')\n",
    "            break\n",
    "        reward_avg_list = reward_avg_list[-99:] # discard oldest on list\n",
    "\n",
    "if save_model:\n",
    "    torch.save(actor.state_dict(), './model.pth')\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file (for coursework)\n",
    "if hardcore:\n",
    "    filename = \"nchw73-agent-hardcore-log.txt\"\n",
    "else:\n",
    "    filename = \"nchw73-agent-log.txt\"\n",
    "env.write_log(folder=\"logs\", file=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
