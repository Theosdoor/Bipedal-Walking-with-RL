{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "# **Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSUGAJsTLOV",
    "outputId": "875609e2-e3eb-4b29-c03c-1989820b1ea7"
   },
   "outputs": [],
   "source": [
    "# https://github.com/robert-lieck/rldurham/tree/main - rldurham source\n",
    "\n",
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham\n",
    "\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFrqd24JTLOW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np # TODO consider switching out for torch where poss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.distributions as D\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "\n",
    "import rldurham as rld\n",
    "\n",
    "# from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "# **RL agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "MAX_TIMESTEPS = 2000 # [DONT CHANGE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERs\n",
    "# for https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def train_on_environment(actor, env, grad_step_class,\n",
    " replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length):\n",
    "    '''\n",
    "    abstracted training loop function for both real & dreamer environments to use\n",
    "\n",
    "    the real environment will always have MAX_TIMESTEPS as the max_timesteps value?\n",
    "    while the dreamer enviroment might have less timesteps?\n",
    "    '''\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "    input_buffer = torch.empty((0, 54)) # save start sequence for the dreamer model TODO - where does 54 come from?\n",
    "\n",
    "    for t in range(max_timesteps): \n",
    "        total_steps += 1\n",
    "        # select action and step the env\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "\n",
    "        # handle different return formats\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, info = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, info = step\n",
    "\n",
    "        ep_timesteps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        if max_timesteps == MAX_TIMESTEPS: # i.e. \n",
    "            # reward design NOTE justify!\n",
    "            if reward == -100.0:\n",
    "                reward = -10.0\n",
    "            else:\n",
    "                reward *= 2\n",
    "            replay_buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "        else:\n",
    "            if t == sequence_length:\n",
    "                for row in input_buffer.cpu().numpy():\n",
    "                    replay_buffer.add(row[:24], row[24:28], row[28:52], row[52], row[53])\n",
    "                    # TODO - why does this ^ use slices? Where do these numbers come from?\n",
    "            elif t > sequence_length:\n",
    "                replay_buffer.add(state, action, next_state, reward, done)\n",
    "            # NOTE - don't store if simulation ends without reaching the sequence length\n",
    "        \n",
    "        if t < sequence_length: # store in input buffer TODO what does this do? why's it needed?\n",
    "            input_buffer = torch.cat([\n",
    "                input_buffer,\n",
    "                torch.tensor(\n",
    "                    np.concatenate((state, action, next_state, np.array([reward]), np.array([done])),\n",
    "                    axis=0)\n",
    "                    ).unsqueeze(0)\n",
    "                ], axis=0)        \n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "        if total_steps >= batch_size: # do 1 training update using 1 batch from buffer\n",
    "            # train the agent using experiences from the real environment\n",
    "            grad_step_class.take_gradient_step(replay_buffer, total_steps, batch_size)\n",
    "    \n",
    "        if done: # break if ep finished\n",
    "            break\n",
    "\n",
    "    if sequence_length > 0: # TODO - this means...? which env are we in?\n",
    "        return ep_timesteps, ep_reward, input_buffer, info\n",
    "    return ep_timesteps, ep_reward, None, info\n",
    "\n",
    "\n",
    "# test loop for agent on environment\n",
    "def simulate_on_environment(actor, env, max_timesteps, state):\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "    for t in range(max_timesteps):\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, _ = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, _ = step\n",
    "        ep_timesteps += 1\n",
    "    \n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "    \n",
    "        if done or t == max_timesteps - 1:\n",
    "            break\n",
    "    return ep_timesteps, ep_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# generate value array from replay buffer\n",
    "def retrieve_from_replay_buffer(replay_buffer, ptr):\n",
    "    '''\n",
    "    '''\n",
    "    return np.concatenate((\n",
    "                    replay_buffer.state[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.action[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.reward[ptr:replay_buffer.ptr],\n",
    "                    1. - replay_buffer.not_done[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.next_state[ptr:replay_buffer.ptr],                \n",
    "                ), \n",
    "                axis = 1 # along the columns (so each row is a memory)\n",
    "            )\n",
    "\n",
    "# function to create sequences with a given window size and step size\n",
    "def create_sequences(memories, window_size, step_size):\n",
    "    '''\n",
    "    Function to create sequences of memories from the replay buffer.\n",
    "\n",
    "    Each sequence is of length window_size and each spaced apart by step_size\n",
    "    '''\n",
    "    n_memories = memories.shape[0] # just the number of time steps currently in the buffer\n",
    "\n",
    "    # calc number of seq (of len window_size) we can create from mems available  \n",
    "    n_sequences = math.floor((n_memories - window_size) / step_size) + 1 # +1 because indices start at 0\n",
    "\n",
    "    sequences = np.zeros((n_sequences, window_size, memories.shape[1]))\n",
    "    for i in range(n_sequences):\n",
    "        start_idx = i * step_size # idx to start seq from\n",
    "        sequences[i, :] = memories[start_idx:start_idx + window_size, :] # grab seq of memories window_size long from start_idx\n",
    "    return sequences\n",
    "\n",
    "def gen_test_train_seq(replay_buffer, train_set, test_set, train_split, window_size, step_size, ptr):\n",
    "    '''\n",
    "    Function to split train and test data\n",
    "    '''\n",
    "    memories = retrieve_from_replay_buffer(replay_buffer, ptr)\n",
    "    try:\n",
    "        memory_sequences = create_sequences(memories, window_size, step_size)\n",
    "        n_sequences = memory_sequences.shape[0]\n",
    "    except: # TODO How might this go wrong? not enough new data to create a sequence?\n",
    "        return train_set, test_set\n",
    "\n",
    "    # shuffle the sequences & split\n",
    "    indices = np.arange(n_sequences)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(train_split * n_sequences) # get split point \n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "\n",
    "    if train_set is None: # if this is first train/test set, create the sets\n",
    "        return memory_sequences[train_indices, :], memory_sequences[test_indices, :]\n",
    "    # else just add to existing sets\n",
    "    return np.concatenate((train_set, memory_sequences[train_indices, :]), axis=0), np.concatenate((test_set, memory_sequences[test_indices, :]), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dreamer_eps(dreamer_avg_loss, score_threshold):\n",
    "    '''\n",
    "    Get number of dreamed episdoes the agent should run on the dreamer model.\n",
    "\n",
    "    Only use dreamer for training if it's sufficiently accurate model of env.\n",
    "\n",
    "    Note - score_threshold is between 0 and 1.\n",
    "\n",
    "    Disclaimer - calculation from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    '''\n",
    "    max_dreamer_it = 10 # a perfect dreamer has this many eps\n",
    "\n",
    "    if dreamer_avg_loss >= score_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        norm_score = dreamer_avg_loss/score_threshold # normalise score relative to threshold\n",
    "        inv_score = 1 - norm_score # invert so that closer to 1 ==> dreamer score much better than threshold\n",
    "        sq_score = inv_score**2 # square so that num iter increases quadratically as accuracy improves\n",
    "        return int(max_dreamer_it * sq_score) # scale so that max iterations is 10 (when dreamer very accurate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "id": "9kTeUHYL7gU6"
   },
   "outputs": [],
   "source": [
    "class EREReplayBuffer(object):\n",
    "    '''\n",
    "    implementation - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    An ERE buffer is implemented to improve sample efficiency, \n",
    "        for which the paper can be found here: https://arxiv.org/abs/1906.04009\n",
    "\n",
    "    Rationale:\n",
    "    - actions from early in training are likely v different to optimal, so want to prioritise recent ones so we're not constantly learning from crap ones\n",
    "    - as time goes on, they get more similar so annealing eta allows uniform sampling later on\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, T, max_size, eta, cmin):\n",
    "        self.max_size, self.ptr, self.size, self.rollover = max_size, 0, 0, False\n",
    "        self.eta0 = eta\n",
    "        self.cmin = cmin\n",
    "        self.c_list = []\n",
    "        self.index = []\n",
    "        self.T = T\n",
    "\n",
    "        self.reward = np.empty((max_size, 1))\n",
    "        self.state = np.empty((max_size, state_dim))\n",
    "        self.action = np.empty((max_size, action_dim))\n",
    "        self.not_done = np.empty((max_size, 1))\n",
    "        self.next_state = np.empty((max_size, state_dim))\n",
    "        \n",
    "    def sample(self, batch_size, t):\n",
    "        # update eta value for current timestep\n",
    "        eta = self.eta_anneal(t)\n",
    "\n",
    "        index = np.array([self._get_index(eta, k, batch_size) for k in range(batch_size)])\n",
    "\n",
    "        r = torch.tensor(self.reward[index], dtype = torch.float)\n",
    "        s = torch.tensor(self.state[index], dtype = torch.float)\n",
    "        ns = torch.tensor(self.next_state[index], dtype = torch.float)\n",
    "        a = torch.tensor(self.action[index], dtype = torch.float)\n",
    "        nd = torch.tensor(self.not_done[index], dtype = torch.float)\n",
    "        \n",
    "        return s, a, ns, r, nd\n",
    "    \n",
    "    def _get_index(self, eta, k, batch_size):\n",
    "        c_calc = self.size * eta ** (k * 1000 / batch_size)\n",
    "        ck = c_calc if c_calc > self.cmin else self.size\n",
    "\n",
    "        if not self.rollover: # if we're not overwriting yet, ...\n",
    "            return np.random.randint(self.size - ck, self.size)\n",
    "        \n",
    "        return np.random.randint(self.ptr + self.size - ck, self.ptr + self.size) % self.size\n",
    "\n",
    "    def eta_anneal(self, t):\n",
    "        # eta anneals over time --> 1, which reduces emphasis on recent experiences over time\n",
    "        return min(1.0, self.eta0 + (1 - self.eta0) * t / self.T)\n",
    " \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        # Add experience to replay buffer \n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.ptr %= self.max_size\n",
    "\n",
    "        # increase size counter until full, then start overwriting (rollover)\n",
    "        if self.max_size > self.size + 1:\n",
    "          self.size += 1\n",
    "        else:\n",
    "          self.size = self.max_size\n",
    "          self.rollover = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dreamer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "# key ideas and concepts for the auto-regressive transformer design stem from this paper: https://arxiv.org/abs/2202.09481\n",
    "\n",
    "class DreamerAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, seq_len, num_layers, num_heads, dropout_prob, lr):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.input_dim = state_dim + action_dim + 2\n",
    "        self.target_dim = self.state_dim + self.action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.input_fc = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.target_fc = nn.Linear(self.target_dim, hidden_dim)\n",
    "        self.transformer = nn.Transformer( # uses transformer model!\n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            num_heads, \n",
    "            dropout=dropout_prob,\n",
    "            activation=F.gelu,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_next_state = nn.Linear(hidden_dim + self.target_dim, state_dim)\n",
    "        self.output_reward = nn.Linear(hidden_dim + self.target_dim, 1)\n",
    "        self.output_done = nn.Linear(hidden_dim + self.target_dim, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=3e-4)\n",
    "        \n",
    "    # separate out the ground truth variables and compare against predictions\n",
    "    def loss_fn(self, output_next_state, output_reward, output_done, ground_truth):\n",
    "        reward, done, next_state = torch.split(ground_truth, [1, 1, self.state_dim], dim=-1)\n",
    "        loss = self.mse_loss(output_next_state[:, -1], next_state)\n",
    "        loss += self.mse_loss(output_reward[:, -1], reward)\n",
    "        loss += self.ce_loss(output_done[:, -1], done)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # separate the input and target tensors\n",
    "        target = input_tensor[:, -1, :self.target_dim].unsqueeze(1)\n",
    "        encoded_target = self.target_fc(target)\n",
    "        encoded_input = self.input_fc(input_tensor[:, :-1, :self.input_dim])\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target], axis=2))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target], axis=2))\n",
    "        # output_done = self.output_done(torch.cat([encoded_output, target], axis=2)) # don't need sigmoid because nn.CrossEntropy already does it?\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target], axis=2)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "    \n",
    "    def predict(self, input_tensor, target_tensor):\n",
    "        # separate the input and target tensors\n",
    "        encoded_target = self.target_fc(target_tensor)\n",
    "        encoded_input = self.input_fc(input_tensor)\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target_tensor], axis=1)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "\n",
    "    # transformer training loop\n",
    "    # sequences shape: (batch, sequence, features)\n",
    "    def train_dreamer(self, sequences, epochs, batch_size=256):\n",
    "        print(\"Training Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float)\n",
    "        \n",
    "        train_dataset = TensorDataset(inputs)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(train_dataloader):\n",
    "                input_batch = input_batch[0]\n",
    "                self.optimizer.zero_grad()\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, running_loss / len(train_dataloader)))\n",
    "\n",
    "    # transformer testing loop\n",
    "    def test_dreamer(self, sequences, batch_size=64):\n",
    "        print(\"Testing Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float)\n",
    "        \n",
    "        test_dataset = TensorDataset(inputs)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(test_dataloader):\n",
    "                input_batch = input_batch[0]\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                running_loss += loss.item()\n",
    "            print('Test Loss: {:.4f}'.format(running_loss / len(test_dataloader)))\n",
    "        return running_loss / len(test_dataloader)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.actions = torch.cat([self.actions, torch.tensor(np.array([action]))], axis=0)\n",
    "        input_sequence = torch.cat([self.states[:-1], self.actions[:-1], self.rewards, self.dones], axis=1).to(torch.float32)\n",
    "        target = torch.cat([self.states[-1], self.actions[-1]], axis=0).unsqueeze(0).to(torch.float32)\n",
    "        with torch.no_grad():\n",
    "            next_state, reward, done = self.predict(input_sequence, target)\n",
    "            done = (done >= 0.6).float() # bias towards not done to avoid false positives\n",
    "\n",
    "            self.states = torch.cat([self.states, next_state], axis=0)\n",
    "            self.rewards = torch.cat([self.rewards, reward], axis=0)\n",
    "            self.dones = torch.cat([self.dones, done], axis=0)\n",
    "            \n",
    "            if self.states.shape[0] > self.seq_len:\n",
    "                self.states = self.states[1:]\n",
    "                self.rewards = self.rewards[1:]\n",
    "                self.dones = self.dones[1:]\n",
    "            if self.actions.shape[0] > self.seq_len - 1:\n",
    "                self.actions = self.actions[1:]\n",
    "        \n",
    "        return next_state.squeeze(0).cpu().numpy(), reward.cpu().item(), done.cpu().item(), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actor-critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS\n",
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def quantile_huber_loss(quantiles, samples, sum_over_quantiles = False):\n",
    "    '''\n",
    "    From TQC (see paper p3)\n",
    "    Specific implementation: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/utils.py#L8\n",
    "\n",
    "    Huber loss is less sensitive to outliers than MSE\n",
    "\n",
    "    samples: (batch_size, 1, n_target_quantiles) -> (batch_size, 1, 1, n_target_quantiles)\n",
    "    quantiles: (batch_size, n_critics, n_quantiles) -> (batch_size, n_critics, n_quantiles, 1)\n",
    "    pairwise_delta: (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    '''\n",
    "    # uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise\n",
    "    delta = samples[:, np.newaxis, np.newaxis, :] - quantiles[:, :, :, np.newaxis]  \n",
    "    abs_delta = torch.abs(delta)\n",
    "    huber_loss = torch.where(abs_delta > 1, abs_delta - 0.5, delta ** 2 * 0.5)\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "\n",
    "    # cumulative probabilities to calc quantiles\n",
    "    cum_prob = (torch.arange(n_quantiles, device=quantiles.device, dtype=torch.float) + 0.5) / n_quantiles\n",
    "    cum_prob = cum_prob.view(1, 1, -1, 1) # quantiles has shape (batch_size, n_critics, n_quantiles), so make cum_prob broadcastable to (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    \n",
    "    loss = (torch.abs(cum_prob - (delta < 0).float()) * huber_loss)\n",
    "\n",
    "    # Summing over the quantile dimension \n",
    "    if sum_over_quantiles:\n",
    "        loss = loss.sum(dim=-2).mean()\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# MLP for actor that implements D2RL architecture\n",
    "class ActorMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim, output size = action dim\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "    # input_dim_ = input_dim\n",
    "    # num_inputs = 24 # input_dim\n",
    "    # input_dim = hidden_dims[0] + num_inputs\n",
    "\n",
    "    current_dim = input_dim # first layer has input dim = state dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim # prev layers output + original input size\n",
    "      # if i == 0:\n",
    "      #   lay = nn.Linear(input_dim_, next_size)\n",
    "      # else:\n",
    "      #   lay = nn.Linear(input_dim, next_size)\n",
    "      # self.add_module(f'layer{i}', lay)\n",
    "      # self.layer_list.append(lay)\n",
    "      # input_dim_ = next_size\n",
    "        \n",
    "    # Final layer input dim is last hidden layer output + original input size\n",
    "    self.last_layer_mean_linear = nn.Linear(current_dim, output_dim)\n",
    "    self.last_layer_log_std_linear = nn.Linear(current_dim, output_dim)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim=1) # cat with output layer\n",
    "\n",
    "    mean_linear = self.last_layer_mean_linear(curr)\n",
    "    log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "    return mean_linear, log_std_linear\n",
    "\n",
    "# MLP for critic that implements D2RL architecture \n",
    "class CriticMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim + action dim, output size = n_quantiles\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    current_dim = input_dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim\n",
    "\n",
    "    self.last_layer = nn.Linear(current_dim, output_dim)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim = 1)\n",
    "      \n",
    "    output = self.last_layer(curr)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh dist for actor to sample actions from \n",
    "\n",
    "# FROM ORIGINAL CODE https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# class TanhNormal(D.Distribution):\n",
    "#   def __init__(self, normal_mean, normal_std):\n",
    "#     super().__init__()\n",
    "#     self.normal_mean = normal_mean\n",
    "#     self.normal_std = normal_std\n",
    "#     self.normal = D.Normal(normal_mean, normal_std)\n",
    "#     self.stand_normal = D.Normal(torch.zeros_like(self.normal_mean), torch.ones_like(self.normal_std))        \n",
    "      \n",
    "#   def logsigmoid(tensor):\n",
    "#     denominator = 1 + torch.exp(-tensor)\n",
    "#     return torch.log(1/ denominator)\n",
    "\n",
    "#   def log_probability(self, pre_tanh):\n",
    "#     final = (self.normal.log_prob(pre_tanh)) - (2 * np.log(2) + F.logsigmoid(2 * pre_tanh) + F.logsigmoid(-2 * pre_tanh))\n",
    "#     return final\n",
    "\n",
    "#   def random_sample(self):\n",
    "#     pretanh = self.normal_mean + self.normal_std * self.stand_normal.sample()\n",
    "#     return torch.tanh(pretanh), pretanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class GradientStep(object):\n",
    "  '''\n",
    "  see (D2RL) https://github.com/pairlab/d2rl/blob/main/sac/sac.py\n",
    "  and (TQC) https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/tqc/tqc.py\n",
    "  '''\n",
    "  def __init__(self,*,\n",
    "    actor, critic, critic_target, discount, tau,\n",
    "    actor_lr, critic_lr, alpha_lr,\n",
    "    n_quantiles, n_mini_critics, top_quantiles_to_drop_per_net, target_entropy,\n",
    "    ):\n",
    "\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.critic_target = critic_target\n",
    "\n",
    "    self.log_alpha = torch.zeros((1,), requires_grad=True) # log alpha is learned\n",
    "    self.quantiles_total = n_quantiles * n_mini_critics\n",
    "    \n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "    self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    self.discount, self.tau = discount, tau\n",
    "    self.top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_mini_critics # total number of quantiles to drop\n",
    "    self.target_entropy = target_entropy\n",
    "\n",
    "  def take_gradient_step(self, replay_buffer, total_steps, batch_size=256):\n",
    "    # Sample batch from replay buffer\n",
    "    state, action, next_state, reward, not_done = replay_buffer.sample(batch_size, total_steps)\n",
    "    alpha = torch.exp(self.log_alpha) # entropy temperature coefficient\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # sample new action from actor on next state\n",
    "      new_next_action, next_log_pi = self.actor(next_state)\n",
    "\n",
    "      # Compute and cut quantiles at the next state\n",
    "      next_z = self.critic_target(next_state, new_next_action)  \n",
    "      \n",
    "      # Sort and drop top k quantiles to control overestimation (TQC)\n",
    "      sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "      sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop] # estimated truncated Q-val dist for next state\n",
    "\n",
    "      # td error + entropy term\n",
    "      target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "    \n",
    "    # Get current quantile estimates using action from the replay buffer\n",
    "    cur_z = self.critic(state, action)\n",
    "    critic_loss = quantile_huber_loss(cur_z, target)\n",
    "\n",
    "    new_action, log_pi = self.actor(state)\n",
    "    # detach the variable from the graph so we don't change it with other losses\n",
    "    alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean() # as in D2RL implementation for auto entropy tuning\n",
    "\n",
    "    # Optimise critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # Soft update target networks\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Compute actor (π) loss\n",
    "    # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]\n",
    "    actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "    # ^ mean(2) is over quantiles, mean(1) is over critic ensemble\n",
    "    \n",
    "    # Optimise the actor\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # Optimise the entropy coefficient\n",
    "    self.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    self.alpha_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[512, 512]):\n",
    "        super().__init__()\n",
    "        self.mlp = ActorMLP(state_dim, hidden_dims, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.mlp(obs)\n",
    "        log_std = log_std.clamp(-20, 2) # clamp for stability\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        base_N_dist = D.Normal(mean, std) # base normal dist\n",
    "        tanh_transform = TanhTransform(cache_size=1) # transform to get the tanh dist\n",
    "\n",
    "        log_prob = None\n",
    "        if self.training: # i.e. agent.train()\n",
    "            transformed_dist = D.TransformedDistribution(base_N_dist, tanh_transform) # transformed distribution\n",
    "            action = transformed_dist.rsample() # samples from base dist & applies transform\n",
    "            log_prob = transformed_dist.log_prob(action) # log prob of action after transform\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True) # sum over action dim\n",
    "        else: # evaluation mode\n",
    "            action = torch.tanh(mean)\n",
    "\n",
    "        # FROM ORIGINAL CODE using custom tanh dist\n",
    "        # if self.training:\n",
    "        #     tanh_dist = TanhNormal(mean, std) # use custom tanh dist\n",
    "        #     action, pre_tanh = tanh_dist.random_sample()\n",
    "        #     log_prob = tanh_dist.log_probability(pre_tanh)\n",
    "        #     log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        # else:\n",
    "        #     action = torch.tanh(mean)\n",
    "            \n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs)[np.newaxis, :]\n",
    "        action, log_prob = self.forward(obs)\n",
    "        return np.array(action[0].cpu().detach())\n",
    "\n",
    "\n",
    "class Critic(nn.Module): # really a mega-critic from lots of mini-critics\n",
    "    '''\n",
    "    Ensemble of critics for TQC\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.critics = nn.ModuleList()\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        for _ in range(n_nets): # multiple critic mlps\n",
    "            net = CriticMLP(state_dim + action_dim, hidden_dims, n_quantiles)\n",
    "            self.critics.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # cat state and action (to pass to critic)\n",
    "        state_act = torch.cat((state, action), dim=1)\n",
    "\n",
    "        # get quantiles from each critic mlp\n",
    "        quantiles = [critic(state_act) for critic in self.critics]\n",
    "        quantiles = torch.stack(quantiles, dim=1) # stack into tensor\n",
    "        return quantiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "id": "H466HICfpDB1"
   },
   "outputs": [],
   "source": [
    "# XinJingHao's (some of) hyperparams\n",
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# params = {\n",
    "#   'eval_interval' : int(2e3),  # Model evaluating interval, in steps\n",
    "#   'delay_freq' : 1,  # Delayed frequency for Actor and Target Net\n",
    "#   'explore_noise' : 0.15,  # Exploring noise when interacting\n",
    "#   'explore_noise_decay' : 0.998,  # Decay rate of explore noise\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My hyperparams\n",
    "\n",
    "# env/general params\n",
    "seed = 42 # DONT CHANGE\n",
    "max_timesteps = MAX_TIMESTEPS # per episode [DONT CHANGE] \n",
    "\n",
    "max_episodes = 1000\n",
    "target_score = 300 # stop training when average score over 100 episodes is > target_score\n",
    "hardcore = False\n",
    "\n",
    "# Agent hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "n_mini_critics = 5 # each mini-critic is a single mlp, which combine to make one mega-critic\n",
    "n_quantiles = 25 # quantiles per mini critic\n",
    "top_quantiles_to_drop_per_net = 2 # per mini critic\n",
    "actor_hidden_dims = [512, 512]\n",
    "mini_critic_hidden_dims = [256, 256] # * n_mini_critics\n",
    "batch_size = 256\n",
    "discount = 0.98 # gamma\n",
    "tau = 0.005\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-4\n",
    "alpha_lr = 3e-4\n",
    "\n",
    "# ERE buffer\n",
    "buffer_size = int(0.5e6) # smaller size improves learning early on but is outperformed later on\n",
    "eta0 = 0.996 # 0.996 - 0.999 is good (according to paper)\n",
    "annealing_steps = int(1e6) # number of steps to anneal eta over (after which sampling is uniform)\n",
    "cmin = 5000 # min number of samples to sample from\n",
    "\n",
    "# dreamer hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "use_dreamer = True\n",
    "batch_size_dreamer = 512\n",
    "hidden_dim = 256\n",
    "num_layers = 16\n",
    "num_heads = 4\n",
    "dreamer_lr = 1e-3\n",
    "dropout_prob = 0.1\n",
    "window_size = 40               # transformer context window size\n",
    "step_size = 1                  # how many timesteps to skip between each context window\n",
    "train_split = 0.8              # train/validation split\n",
    "score_threshold = 0.8          # use dreamer if loss < score_threshold\n",
    "dreamer_train_epochs = 10      # how many epochs to train the dreamer model for\n",
    "dreamer_train_frequency = 10   # how often to train the dreamer model\n",
    "episode_threshold = 50         # how many episodes to run before training the dreamer model\n",
    "max_size = int(5e4)            # maximum size of the training set for the dreamer model\n",
    "\n",
    "# recording/logging\n",
    "plot_interval = 10 # plot every Nth episode\n",
    "\n",
    "is_recording = True\n",
    "video_interval = 25 # record every Nth episode\n",
    "ep_start_rec = 100 # start recording on this episode\n",
    "\n",
    "save_model = False # NOTE - currently only saves actor & no way to load models\n",
    "save_interval = 30 # save every Nth episode\n",
    "\n",
    "## AUTOMATIC PARAMS ##\n",
    "\n",
    "target_entropy = -np.prod(env.action_space.shape).item() # target entropy heuristic (= −dim(A) as in D2RL paper)\n",
    "# max_total_steps = max_episodes * max_timesteps # total number of timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1Xrcek4hxDXl",
    "outputId": "6161f148-a164-4e22-d97a-f9aa53e7b8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SETUP\n",
    "\n",
    "# make env\n",
    "if hardcore == False:\n",
    "  env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "else:\n",
    "  env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"nchw73-agent-hardcore-video\" if hardcore else \"nchw73-agent-video\",  # prefix for videos\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "rld.check_device() # training on CPU recommended\n",
    "\n",
    "env.video = False # switch video recording off (only switch on every x episodes as this is slow)\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, state_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=seed)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWjENKkU_My"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, state, info = rld.seed_everything(seed, env)\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "replay_buffer = EREReplayBuffer(state_dim, act_dim, annealing_steps, buffer_size, eta0, cmin)\n",
    "\n",
    "actor = Actor(state_dim, act_dim, actor_hidden_dims)\n",
    "\n",
    "critic = Critic(state_dim, act_dim, n_quantiles, n_mini_critics, mini_critic_hidden_dims)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "if use_dreamer:\n",
    "    dreamer = DreamerAgent(state_dim, act_dim, hidden_dim, \n",
    "        window_size, num_layers, num_heads, dropout_prob, dreamer_lr)\n",
    "\n",
    "grad_step_class = GradientStep(\n",
    "    actor=actor, critic=critic, critic_target=critic_target,\n",
    "    discount=discount, tau=tau,\n",
    "    actor_lr=actor_lr, critic_lr=critic_lr, alpha_lr=alpha_lr,\n",
    "    n_quantiles=n_quantiles, n_mini_critics=n_mini_critics,\n",
    "    top_quantiles_to_drop_per_net=top_quantiles_to_drop_per_net,\n",
    "    target_entropy=target_entropy,\n",
    "    )\n",
    "\n",
    "actor.train()\n",
    "\n",
    "ep_timesteps = 0\n",
    "total_steps = 0\n",
    "ep_reward = 0\n",
    "memory_ptr = 0 # marks boundary between new and old data in the replay buffer\n",
    "train_set, test_set = None, None\n",
    "reward_list = []\n",
    "reward_avg_list = []\n",
    "# ep_timesteps_dreamer = ep_reward_dreamer = dreamer_eps = 0\n",
    "\n",
    "episode = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 1 | Ep tsteps: 2000 | Reward: -68.324 | Total Steps: 2e+03 | Dreamer: False\n",
      "Ep: 2 | Ep tsteps: 2000 | Reward: -73.710 | Total Steps: 4e+03 | Dreamer: False\n",
      "Ep: 3 | Ep tsteps: 57 | Reward: -104.525 | Total Steps: 4.1e+03 | Dreamer: False\n",
      "Ep: 4 | Ep tsteps: 72 | Reward: -108.779 | Total Steps: 4.1e+03 | Dreamer: False\n",
      "Ep: 5 | Ep tsteps: 54 | Reward: -110.711 | Total Steps: 4.2e+03 | Dreamer: False\n",
      "Ep: 6 | Ep tsteps: 192 | Reward: -96.708 | Total Steps: 4.4e+03 | Dreamer: False\n",
      "Ep: 7 | Ep tsteps: 2000 | Reward: -67.417 | Total Steps: 6.4e+03 | Dreamer: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[349], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# sample real env\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m ep_timesteps, ep_reward, input_buffer, info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_step_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_timesteps\n\u001b[1;32m     17\u001b[0m tracker\u001b[38;5;241m.\u001b[39mtrack(info) \u001b[38;5;66;03m# track statistics for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[335], line 62\u001b[0m, in \u001b[0;36mtrain_on_environment\u001b[0;34m(actor, env, grad_step_class, replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length)\u001b[0m\n\u001b[1;32m     58\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size: \u001b[38;5;66;03m# do 1 training update using 1 batch from buffer\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# train the agent using experiences from the real environment\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mgrad_step_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;66;03m# break if ep finished\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[343], line 38\u001b[0m, in \u001b[0;36mGradientStep.take_gradient_step\u001b[0;34m(self, replay_buffer, total_steps, batch_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m new_next_action, next_log_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(next_state) \u001b[38;5;66;03m# make a new action on old memory\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Compute and cut quantiles at the next state\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m next_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_next_action\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Sort and drop top k quantiles to control overestimation (TQC)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m sorted_z, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(next_z\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[344], line 60\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     57\u001b[0m state_act \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((state, action), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# get quantiles from each critic mlp\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m [critic(state_act) \u001b[38;5;28;01mfor\u001b[39;00m critic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritics]\n\u001b[1;32m     61\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(quantiles, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# stack into tensor\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantiles\n",
      "Cell \u001b[0;32mIn[344], line 60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m state_act \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((state, action), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# get quantiles from each critic mlp\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m [\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_act\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m critic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritics]\n\u001b[1;32m     61\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(quantiles, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# stack into tensor\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantiles\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[341], line 60\u001b[0m, in \u001b[0;36mCriticMLP.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m     58\u001b[0m curr \u001b[38;5;241m=\u001b[39m input_\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_list:\n\u001b[0;32m---> 60\u001b[0m   curr \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     61\u001b[0m   curr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([curr, input_], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer(curr)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "while episode <= max_episodes: # index from 1\n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    if is_recording and episode >= ep_start_rec:\n",
    "        env.info = episode % video_interval == 0   # track every x episodes (usually tracking every episode is fine)\n",
    "        env.video = episode % video_interval == 0  # record videos every x episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # sample real env\n",
    "    ep_timesteps, ep_reward, input_buffer, info = train_on_environment(\n",
    "        actor, env, grad_step_class, replay_buffer, max_timesteps, state,\n",
    "        batch_size, total_steps, window_size)\n",
    "\n",
    "    total_steps += ep_timesteps\n",
    "    tracker.track(info) # track statistics for plotting\n",
    "\n",
    "    # update train/test sets for dreamer (add new eps and trim to window size)\n",
    "    train_set, test_set = gen_test_train_seq(\n",
    "        replay_buffer, train_set, test_set, train_split, window_size, step_size, memory_ptr)\n",
    "\n",
    "    # train dreamer after ep_thresh and if the input buffer has enough data to fill context window\n",
    "    if use_dreamer and episode >= episode_threshold and input_buffer.shape[0] == window_size:\n",
    "        # train and assess the dreamer every train_frequency\n",
    "        if episode % dreamer_train_frequency == 0:\n",
    "            dreamer.train_dreamer(train_set, dreamer_train_epochs, batch_size_dreamer)\n",
    "\n",
    "        # truncate the training set to control train time performance\n",
    "        if test_set.shape[0] > max_size:\n",
    "            train_set = train_set[-max_size:]\n",
    "\n",
    "        print(f'Size of train set: {train_set.shape[0]}, test set: {test_set.shape[0]}')\n",
    "        \n",
    "        # evaluate the dreamer's performance & decide num of training steps for dreamer\n",
    "        dreamer_avg_loss = dreamer.test_dreamer(test_set, batch_size_dreamer)\n",
    "        dreamer_eps = get_dreamer_eps(dreamer_avg_loss, score_threshold)\n",
    "\n",
    "        # train on dreamer if its accurate enough\n",
    "        if dreamer_eps > 0:\n",
    "            print(f'Dreamer active for {dreamer_eps} iterations')\n",
    "            ep_timesteps_dreamer = ep_reward_dreamer = 0\n",
    "            for dep in range(dreamer_eps):\n",
    "                print(f'Dreamer ep: {dep+1}')\n",
    "\n",
    "                # initialise dreamer states with the input sequence\n",
    "                dreamer.states = input_buffer[:, :state_dim]\n",
    "                dreamer.actions = input_buffer[:-1, state_dim:state_dim+act_dim]\n",
    "                dreamer.rewards = input_buffer[:-1, -2-state_dim].unsqueeze(1)\n",
    "                dreamer.dones = input_buffer[:-1, -1-state_dim].unsqueeze(1)\n",
    "\n",
    "            # sample from dreamer environment (ignore input buffer and info)\n",
    "            _td, _rd, _, _ = train_on_environment(\n",
    "                actor, dreamer, grad_step_class, replay_buffer,\n",
    "                math.ceil(total_steps/episode) - 1, # max_timesteps \n",
    "                dreamer.states[-1].cpu().numpy(),\n",
    "                batch_size, total_steps, window_size)\n",
    "\n",
    "            ep_timesteps_dreamer += _td\n",
    "            ep_reward_dreamer += _rd\n",
    "\n",
    "    memory_ptr = replay_buffer.ptr # update the memory pointer to the current position in the buffer\n",
    "\n",
    "    print(f\"Ep: {episode} | Ep tsteps: {ep_timesteps} | Reward: {ep_reward:.3f} | Total Steps: {total_steps:.2g} | Dreamer: {dreamer_eps > 0}\")\n",
    "    if use_dreamer and dreamer_eps > 0:\n",
    "        print(f\"\\t Dreamer Eps: {dreamer_eps} | Dreamer Avg Timesteps: {ep_timesteps_dreamer/dreamer_eps:.3f} | Dreamer Avg Reward: {ep_reward_dreamer/dreamer_eps:.3f}\")\n",
    "\n",
    "    reward_list.append(ep_reward)\n",
    "    reward_avg_list.append(ep_reward)\n",
    "    ep_reward = 0\n",
    "\n",
    "    # plot tracked statistics (on real env)\n",
    "    if episode % plot_interval == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "    # save model\n",
    "    if save_model and episode % save_interval == 0:\n",
    "        torch.save(actor.state_dict(), f\"./actor_{episode}.pth\")\n",
    "        # TODO note - maybe save critic / dreamer too?\n",
    "        # also - maybe delete old ones?\n",
    "\n",
    "    episode += 1\n",
    "\n",
    "    # break condition - stop if we consistently meet target score\n",
    "    if len(reward_avg_list) >= 100:\n",
    "        print(f'Current progress: {np.array(reward_avg_list).mean()}/{target_score}')\n",
    "        if np.array(reward_avg_list).mean() >= target_score: # quit when we've got (basically) optimal performance\n",
    "            print('Completed environment!')\n",
    "            break\n",
    "        reward_avg_list = reward_avg_list[-99:] # discard oldest on list (keep most recent 99)\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file\n",
    "if hardcore:\n",
    "    filename = \"nchw73-agent-hardcore-log.txt\"\n",
    "else:\n",
    "    filename = \"nchw73-agent-log.txt\"\n",
    "env.write_log(folder=\"logs\", file=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
