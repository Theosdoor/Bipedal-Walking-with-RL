{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# FOR .py FILE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "# **Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSUGAJsTLOV",
    "outputId": "875609e2-e3eb-4b29-c03c-1989820b1ea7"
   },
   "outputs": [],
   "source": [
    "# https://github.com/robert-lieck/rldurham/tree/main - rldurham source\n",
    "\n",
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham\n",
    "\n",
    "# !pip install wandb\n",
    "# !pip install torchinfo\n",
    "# !pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AFrqd24JTLOW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtheo-farrell99\u001b[0m (\u001b[33mtheo-farrell99-durham-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.distributions as D\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "\n",
    "import rldurham as rld\n",
    "\n",
    "import wandb\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"x\" # TODO remove\n",
    "wandb.login()\n",
    "\n",
    "# from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint setup\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, \"training_checkpoint.pth\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "# **RL agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TIMESTEPS = 2000 # [DONT CHANGE]\n",
    "SOURCE_ID = '' # mac, ncc, colab -> for personal id in wandb\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint helper fns\n",
    "\n",
    "def save_checkpoint(state, filename=CHECKPOINT_FILE):\n",
    "    \"\"\"Saves the current training state.\"\"\"\n",
    "    print(f\"Saving checkpoint to {filename}...\")\n",
    "    torch.save(state, filename)\n",
    "    # Optionally, save replay buffer separately if it's very large\n",
    "    # np.savez_compressed(filename + \"_buffer.npz\",\n",
    "    #                     state=state['replay_buffer']['state'],\n",
    "    #                     action=state['replay_buffer']['action'],\n",
    "    #                     next_state=state['replay_buffer']['next_state'],\n",
    "    #                     reward=state['replay_buffer']['reward'],\n",
    "    #                     not_done=state['replay_buffer']['not_done'])\n",
    "    print(\"Checkpoint saved.\")\n",
    "\n",
    "def load_checkpoint(filename=CHECKPOINT_FILE):\n",
    "    \"\"\"Loads training state from a checkpoint file.\"\"\"\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"Loading checkpoint from {filename}...\")\n",
    "        checkpoint = torch.load(filename)\n",
    "        # Optionally load large replay buffer separately\n",
    "        # if os.path.isfile(filename + \"_buffer.npz\"):\n",
    "        #     print(\"Loading replay buffer data...\")\n",
    "        #     buffer_data = np.load(filename + \"_buffer.npz\")\n",
    "        #     checkpoint['replay_buffer_data'] = buffer_data\n",
    "        # else:\n",
    "        #     print(\"Warning: Replay buffer data file not found.\")\n",
    "        #     checkpoint['replay_buffer_data'] = None # Indicate buffer data is missing\n",
    "        print(\"Checkpoint loaded.\")\n",
    "        return checkpoint\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {filename}, starting from scratch.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward adjustment\n",
    "def adjust_reward(reward):\n",
    "    \"\"\"\n",
    "    adjust reward for bipedal walker\n",
    "    \"\"\"\n",
    "    if reward == -100.0: \n",
    "        reward = -10.0 #Â bipedal walker does -100 for hitting floor, so -10 might make it more stable\n",
    "    else:\n",
    "        reward *= 2 # encourage forward motion\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random warm-up phase for the agent\n",
    "# helps exploration!\n",
    "def run_warm_up(env, replay_buffer, required_steps, initial_state):\n",
    "    \"\"\"\n",
    "    Runs the initial random exploration phase to populate the replay buffer.\n",
    "\n",
    "    Args:\n",
    "        env: The environment instance.\n",
    "        replay_buffer: The replay buffer instance.\n",
    "        required_steps: The number of random steps to take.\n",
    "        initial_state: The starting state for the exploration.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (final_state, updated_total_steps)\n",
    "               - final_state: The state after the last random step (or reset).\n",
    "               - updated_total_steps: The total number of steps after the warm-up.\n",
    "    \"\"\"\n",
    "    print(f\"Running initial {required_steps} random steps...\")\n",
    "    state = initial_state.astype(np.float32) # Ensure correct dtype\n",
    "    steps_done_in_phase = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    while steps_done_in_phase < required_steps:\n",
    "        action = env.action_space.sample() # Sample random action\n",
    "\n",
    "        # Step environment\n",
    "        step = env.step(action)\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, info = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, info = step\n",
    "\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        reward = float(reward)\n",
    "        done_float = float(done) # Keep done as float for consistency in buffer\n",
    "\n",
    "        # adjust reward and add to replay buffer\n",
    "        reward = adjust_reward(reward)\n",
    "        replay_buffer.add(state, action, next_state, reward, done_float)\n",
    "\n",
    "        state = next_state\n",
    "        total_steps += 1\n",
    "        steps_done_in_phase += 1\n",
    "\n",
    "        if done:\n",
    "            state, info = env.reset() # Use info from reset\n",
    "            state = state.astype(np.float32)\n",
    "\n",
    "    print(f\"Finished initial random steps. Total steps: {total_steps}. Buffer size: {replay_buffer.size}\")\n",
    "    # Reset state again before starting the first proper training episode\n",
    "    final_state, _ = env.reset()\n",
    "    final_state = final_state.astype(np.float32)\n",
    "    return final_state, total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERs\n",
    "# for https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def train_on_environment(actor, env, grad_step_class, replay_buffer, max_timesteps,\n",
    "    state, batch_size, total_steps, sequence_length, is_dreamer=False,\n",
    "    use_intrinsic_reward=False, last_dreamer_test_loss=0.0, intrinsic_r_scale=0.0,\n",
    "    ):\n",
    "    '''\n",
    "    abstracted training loop function for both real & dreamer environments to use\n",
    "    '''\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "\n",
    "    state_dim = actor.state_dim\n",
    "    act_dim = actor.action_dim\n",
    "    input_dim = state_dim + act_dim + state_dim + 1 + 1 # state, action, next_state, reward, done\n",
    "    \n",
    "    # save start sequence for the dreamer model\n",
    "    # input_buffer = torch.empty((0, input_dim), dtype=torch.float32).to(DEVICE)\n",
    "    input_buffer = [] # store as python list to avoid cat-ing constantly\n",
    "\n",
    "    # initial state float32 for mps\n",
    "    state = state.astype(np.float32)\n",
    "\n",
    "    for t in range(max_timesteps): \n",
    "        total_steps += 1\n",
    "        ep_timesteps += 1\n",
    "\n",
    "        # select action and step the env\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "\n",
    "        # handle different return formats\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, info = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, info = step\n",
    "\n",
    "        # float32 before processing/storing\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        reward = float(reward) # extrinsic reward\n",
    "        done = float(done)\n",
    "\n",
    "        ep_reward += reward # sum extrinsic reward\n",
    "\n",
    "        r_final = reward\n",
    "        if not is_dreamer: # only adjust reward & add intrinsic for real env\n",
    "            r_final = adjust_reward(reward) # adjust reward for bipedal walker\n",
    "            if use_intrinsic_reward and last_dreamer_test_loss > 0:\n",
    "                r_intrinsic = intrinsic_r_scale * last_dreamer_test_loss\n",
    "                r_final += r_intrinsic\n",
    "                wandb.log({\"Intrinsic Reward Added\": r_intrinsic}, commit=False) # log to wandb (commit = false lets us log things later in the ep)\n",
    "\n",
    "        replay_buffer.add(state, action, next_state, r_final, done) # add to replay buffer for both real and dreamer\n",
    "        # TODO - can i add if t < seq_length?\n",
    "\n",
    "        if is_dreamer and t < sequence_length: # store in input buffer for dreamers first seq\n",
    "            trans = np.concatenate((\n",
    "                state, \n",
    "                action.astype(np.float32), \n",
    "                next_state, \n",
    "                np.array([reward], dtype=np.float32), # NOTE - store original dreamer reward, not adjusted r_final\n",
    "                np.array([done], dtype=np.float32)\n",
    "            ), axis=0)\n",
    "            # combined = torch.tensor(trans, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "            # input_buffer = torch.cat([input_buffer, combined], axis=0)\n",
    "            input_buffer.append(trans)\n",
    "            \n",
    "        # else:\n",
    "        #     if t == sequence_length:\n",
    "        #         for row in input_buffer.cpu().numpy():\n",
    "        #             replay_buffer.add(\n",
    "        #                 row[:state_dim], # state row[:24]\n",
    "        #                 row[state_dim:state_dim+act_dim], # action row[24:28]\n",
    "        #                 row[state_dim+act_dim:state_dim+act_dim+state_dim], # next_state row[28:52]\n",
    "        #                 row[-2], # reward row[52]\n",
    "        #                 row[-1] # done row[53]\n",
    "        #             )\n",
    "        #         # add the last step to the buffer\n",
    "        #         replay_buffer.add(state, action, next_state, reward, done)\n",
    "        #     elif t > sequence_length:\n",
    "        #         # add last step directly\n",
    "        #         replay_buffer.add(state, action, next_state, reward, done)\n",
    "        #     # don't store if simulation ends without reaching the sequence length\n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "        if total_steps >= batch_size: #Â train step if buffer has enough samples for a batch\n",
    "            # train the agent using experiences from the real environment\n",
    "            grad_step_class.take_gradient_step(replay_buffer, total_steps, batch_size)\n",
    "    \n",
    "        if done: # break if finished\n",
    "            break\n",
    "\n",
    "    # only return extrinsic to eval based on that\n",
    "    if is_dreamer and len(input_buffer) == sequence_length: # if input buffer is enough to fill a sequence\n",
    "        input_buffer = torch.tensor(np.array(input_buffer), dtype=torch.float32).to(DEVICE)\n",
    "        return ep_timesteps, ep_reward, input_buffer, info\n",
    "    return ep_timesteps, ep_reward, None, info\n",
    "\n",
    "\n",
    "# test loop for agent on environment\n",
    "# def simulate_on_environment(actor, env, max_timesteps, state):\n",
    "#     ep_timesteps = 0\n",
    "#     ep_reward = 0\n",
    "#     for t in range(max_timesteps):\n",
    "#         action = actor.select_action(state)\n",
    "#         step = env.step(action)\n",
    "#         if len(step) == 5:\n",
    "#             next_state, reward, term, tr, _ = step\n",
    "#             done = term or tr\n",
    "#         else:\n",
    "#             next_state, reward, done, _ = step\n",
    "#         ep_timesteps += 1\n",
    "    \n",
    "#         state = next_state\n",
    "#         ep_reward += reward\n",
    "    \n",
    "#         if done or t == max_timesteps - 1:\n",
    "#             break\n",
    "#     return ep_timesteps, ep_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# generate value array from replay buffer\n",
    "def retrieve_from_replay_buffer(replay_buffer, ptr):\n",
    "    '''\n",
    "    returns (state, action, reward, done, next_state)\n",
    "    '''\n",
    "    return np.concatenate((\n",
    "            replay_buffer.state[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.action[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.reward[ptr:replay_buffer.ptr],\n",
    "            1. - replay_buffer.not_done[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.next_state[ptr:replay_buffer.ptr],                \n",
    "        ), \n",
    "        axis = 1 # along the columns (so each row is a memory)\n",
    "    )\n",
    "\n",
    "# function to create sequences with a given window size and step size\n",
    "def create_sequences(memories, window_size, step_size):\n",
    "    '''\n",
    "    Function to create sequences of memories from the replay buffer.\n",
    "\n",
    "    Each sequence is of length window_size and each spaced apart by step_size\n",
    "    '''\n",
    "    n_memories = memories.shape[0] #Â just the number of time steps currently in the buffer\n",
    "\n",
    "    # calc number of seq (of len window_size) we can create from mems available  \n",
    "    n_sequences = math.floor((n_memories - window_size) / step_size) + 1 # +1 because indices start at 0\n",
    "\n",
    "    sequences = np.zeros((n_sequences, window_size, memories.shape[1]))\n",
    "    for i in range(n_sequences):\n",
    "        start_idx = i * step_size # idx to start seq from\n",
    "        sequences[i, :] = memories[start_idx:start_idx + window_size, :] # grab seq of memories window_size long from start_idx\n",
    "    return sequences\n",
    "\n",
    "def gen_test_train_seq(replay_buffer, train_set, test_set, train_split, window_size, step_size, ptr):\n",
    "    '''\n",
    "    Function to split train and test data\n",
    "    '''\n",
    "    memories = retrieve_from_replay_buffer(replay_buffer, ptr)\n",
    "    try:\n",
    "        memory_sequences = create_sequences(memories, window_size, step_size)\n",
    "        n_sequences = memory_sequences.shape[0]\n",
    "    except: # TODO How might this go wrong? not enough new data to create a sequence?\n",
    "        return train_set, test_set\n",
    "\n",
    "    # shuffle the sequences & split\n",
    "    indices = np.arange(n_sequences)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(train_split * n_sequences) #Â get split point \n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "\n",
    "    if train_set is None: #Â if this is first train/test set, create the sets\n",
    "        return memory_sequences[train_indices, :], memory_sequences[test_indices, :]\n",
    "    # else just add to existing sets\n",
    "    return np.concatenate((train_set, memory_sequences[train_indices, :]), axis=0), np.concatenate((test_set, memory_sequences[test_indices, :]), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dreamer_eps(dreamer_avg_loss, loss_threshold):\n",
    "    '''\n",
    "    Get number of dreamed episdoes the agent should run on the dreamer model.\n",
    "\n",
    "    Only use dreamer for training if it's sufficiently accurate model of env.\n",
    "\n",
    "    Note - loss_threshold is between 0 and 1.\n",
    "\n",
    "    Disclaimer - calculation from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    '''\n",
    "    max_dreamer_it = 10 # a perfect dreamer has this many eps\n",
    "\n",
    "    if dreamer_avg_loss >= loss_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        norm_score = dreamer_avg_loss/loss_threshold # normalise score relative to threshold\n",
    "        inv_score = 1 - norm_score # invert so that closer to 1 ==> dreamer score much better than threshold\n",
    "        sq_score = inv_score**2 # square so that num iter increases quadratically as accuracy improves\n",
    "        return int(max_dreamer_it * sq_score) # scale so that max iterations is 10 (when dreamer very accurate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9kTeUHYL7gU6"
   },
   "outputs": [],
   "source": [
    "class EREReplayBuffer(object):\n",
    "    '''\n",
    "    implementation - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    An ERE buffer is implemented to improve sample efficiency, \n",
    "        for which the paper can be found here: https://arxiv.org/abs/1906.04009\n",
    "\n",
    "    Rationale:\n",
    "    - actions from early in training are likely v different to optimal, so want to prioritise recent ones so we're not constantly learning from crap ones\n",
    "    - as time goes on, they get more similar so annealing eta allows uniform sampling later on\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, T, max_size, eta, cmin):\n",
    "        self.max_size, self.ptr, self.size, self.rollover = max_size, 0, 0, False\n",
    "        self.eta0 = eta\n",
    "        self.cmin = cmin\n",
    "        self.c_list = []\n",
    "        self.index = []\n",
    "        self.T = T\n",
    "\n",
    "        self.reward = np.empty((max_size, 1), dtype=np.float32) #Â float32 for mps\n",
    "        self.state = np.empty((max_size, state_dim), dtype=np.float32)\n",
    "        self.action = np.empty((max_size, action_dim), dtype=np.float32)\n",
    "        self.not_done = np.empty((max_size, 1), dtype=np.float32)\n",
    "        self.next_state = np.empty((max_size, state_dim), dtype=np.float32)\n",
    "        \n",
    "    def sample(self, batch_size, t):\n",
    "        # update eta value for current timestep\n",
    "        eta = self.eta_anneal(t)\n",
    "\n",
    "        index = np.array([self._get_index(eta, k, batch_size) for k in range(batch_size)])\n",
    "\n",
    "        r = torch.tensor(self.reward[index], dtype=torch.float32).to(DEVICE)\n",
    "        s = torch.tensor(self.state[index], dtype=torch.float32).to(DEVICE)\n",
    "        ns = torch.tensor(self.next_state[index], dtype=torch.float32).to(DEVICE)\n",
    "        a = torch.tensor(self.action[index], dtype=torch.float32).to(DEVICE)\n",
    "        nd = torch.tensor(self.not_done[index], dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        return s, a, ns, r, nd\n",
    "    \n",
    "    def _get_index(self, eta, k, batch_size):\n",
    "        c_calc = self.size * eta ** (k * 1000 / batch_size)\n",
    "        ck = c_calc if c_calc > self.cmin else self.size\n",
    "\n",
    "        if not self.rollover: # if we're not overwriting yet, ...\n",
    "            return np.random.randint(self.size - ck, self.size)\n",
    "        \n",
    "        return np.random.randint(self.ptr + self.size - ck, self.ptr + self.size) % self.size\n",
    "\n",
    "    def eta_anneal(self, t):\n",
    "        # eta anneals over time --> 1, which reduces emphasis on recent experiences over time\n",
    "        return min(1.0, self.eta0 + (1 - self.eta0) * t / self.T)\n",
    " \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        # Add experience to replay buffer \n",
    "        self.state[self.ptr] = state.astype(np.float32)\n",
    "        self.action[self.ptr] = action.astype(np.float32)\n",
    "        self.next_state[self.ptr] = next_state.astype(np.float32)\n",
    "        self.reward[self.ptr] = float(reward)\n",
    "        self.not_done[self.ptr] = 1. - float(done)\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.ptr %= self.max_size\n",
    "\n",
    "        # increase size counter until full, then start overwriting (rollover)\n",
    "        if self.max_size > self.size + 1:\n",
    "          self.size += 1\n",
    "        else:\n",
    "          self.size = self.max_size\n",
    "          self.rollover = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dreamer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamerAgent(nn.Module):\n",
    "    '''\n",
    "    Dreamer agent.\n",
    "    Uses a transformer model to predict the next state, reward and done signal given the current (state, action, reward, done)\n",
    "\n",
    "    Acknowledgements:\n",
    "    - implementation based on https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    - (from implementation designer) key ideas and concepts for the auto-regressive transformer design stem from this paper: https://arxiv.org/abs/2202.09481\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, seq_len, num_layers, num_heads, dropout_prob, lr):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.input_dim = state_dim + action_dim + 2 # (state, action, reward, done)\n",
    "        self.target_dim = self.state_dim + self.action_dim # (state, action)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.input_fc = nn.Linear(self.input_dim, hidden_dim).to(DEVICE)\n",
    "        self.target_fc = nn.Linear(self.target_dim, hidden_dim).to(DEVICE)\n",
    "        self.transformer = nn.Transformer( # uses transformer model!\n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            num_heads, \n",
    "            dropout=dropout_prob,\n",
    "            activation=F.gelu,\n",
    "            batch_first=True\n",
    "        ).to(DEVICE)\n",
    "        self.output_next_state = nn.Linear(hidden_dim + self.target_dim, state_dim).to(DEVICE)\n",
    "        self.output_reward = nn.Linear(hidden_dim + self.target_dim, 1).to(DEVICE)\n",
    "        self.output_done = nn.Linear(hidden_dim + self.target_dim, 1).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    # separate out the ground truth variables and compare against predictions\n",
    "    def loss_fn(self, output_next_state, output_reward, output_done, ground_truth):\n",
    "        reward, done, next_state = torch.split(ground_truth, [self.state_dim, 1, 1], dim=-1)\n",
    "        loss = self.mse_loss(output_next_state[:, -1], next_state)\n",
    "        loss += self.mse_loss(output_reward[:, -1], reward)\n",
    "        loss += self.bce_loss(output_done[:, -1], done.float())\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # separate the input and target tensors\n",
    "        target = input_tensor[:, -1, :self.target_dim].unsqueeze(1)\n",
    "        encoded_target = self.target_fc(target)\n",
    "        encoded_input = self.input_fc(input_tensor[:, :-1, :self.input_dim])\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target], axis=2))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target], axis=2))\n",
    "        output_done = self.output_done(torch.cat([encoded_output, target], axis=2)) #Â don't need sigmoid because nn.CrossEntropy already does it? (or can use BCE loss after sigmoid)\n",
    "        # output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target], axis=2)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "    \n",
    "    def predict(self, input_tensor, target_tensor):\n",
    "        # separate the input and target tensors\n",
    "        encoded_target = self.target_fc(target_tensor)\n",
    "        encoded_input = self.input_fc(input_tensor)\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target_tensor], axis=1)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "\n",
    "    # transformer training loop\n",
    "    # sequences shape: (batch, sequence, features)\n",
    "    def train_dreamer(self, sequences, epochs, batch_size=256):\n",
    "        print(\"Training Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float).to(DEVICE)\n",
    "        \n",
    "        train_dataset = TensorDataset(inputs)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(train_dataloader):\n",
    "                input_batch = input_batch[0].to(DEVICE)\n",
    "                self.optimizer.zero_grad()\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, running_loss / len(train_dataloader)))\n",
    "\n",
    "    # transformer testing loop\n",
    "    def test_dreamer(self, sequences, batch_size=64):\n",
    "        print(\"Testing Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float).to(DEVICE)\n",
    "        \n",
    "        test_dataset = TensorDataset(inputs)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(test_dataloader):\n",
    "                input_batch = input_batch[0].to(DEVICE)\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                running_loss += loss.item()\n",
    "            print('Test Loss: {:.4f}'.format(running_loss / len(test_dataloader)))\n",
    "        return running_loss / len(test_dataloader)\n",
    "    \n",
    "    def step(self, action):\n",
    "        act = torch.tensor(np.array([action]), dtype=torch.float32).to(DEVICE)\n",
    "        self.actions = torch.cat([self.actions, act], axis=0)\n",
    "\n",
    "        input_sequence = torch.cat([self.states[:-1], self.actions[:-1], self.rewards, self.dones], axis=1).to(torch.float32)\n",
    "        target = torch.cat([self.states[-1], self.actions[-1]], axis=0).unsqueeze(0).to(torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state, reward, done = self.predict(input_sequence, target)\n",
    "            next_state = next_state.to(torch.float32)\n",
    "            reward = reward.to(torch.float32)\n",
    "            done = (done >= 0.6).to(torch.float32) # bias towards not done to avoid false positives\n",
    "\n",
    "            self.states = torch.cat([self.states, next_state], axis=0)\n",
    "            self.rewards = torch.cat([self.rewards, reward], axis=0)\n",
    "            self.dones = torch.cat([self.dones, done], axis=0)\n",
    "            \n",
    "            # trim sequences\n",
    "            if self.states.shape[0] > self.seq_len:\n",
    "                self.states = self.states[1:]\n",
    "                self.rewards = self.rewards[1:]\n",
    "                self.dones = self.dones[1:]\n",
    "            if self.actions.shape[0] > self.seq_len - 1: # action seq shorter\n",
    "                self.actions = self.actions[1:]\n",
    "        \n",
    "        return next_state.squeeze(0).cpu().numpy(), reward.cpu().item(), done.cpu().item(), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actor-critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS\n",
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def quantile_huber_loss(quantiles, samples, sum_over_quantiles = False):\n",
    "    '''\n",
    "    From TQC (see paper p3)\n",
    "    Specific implementation: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/utils.py#L8\n",
    "\n",
    "    Huber loss is less sensitive to outliers than MSE\n",
    "\n",
    "    samples: (batch_size, 1, n_target_quantiles) -> (batch_size, 1, 1, n_target_quantiles)\n",
    "    quantiles: (batch_size, n_critics, n_quantiles) -> (batch_size, n_critics, n_quantiles, 1)\n",
    "    pairwise_delta: (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    '''\n",
    "    # uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise\n",
    "    delta = samples[:, np.newaxis, np.newaxis, :] - quantiles[:, :, :, np.newaxis]  \n",
    "    abs_delta = torch.abs(delta)\n",
    "    huber_loss = torch.where(abs_delta > 1, abs_delta - 0.5, delta ** 2 * 0.5)\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "\n",
    "    # cumulative probabilities to calc quantiles\n",
    "    cum_prob = (torch.arange(n_quantiles, device=quantiles.device, dtype=torch.float) + 0.5) / n_quantiles\n",
    "    cum_prob = cum_prob.view(1, 1, -1, 1) # quantiles has shape (batch_size, n_critics, n_quantiles), so make cum_prob broadcastable to (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    \n",
    "    loss = (torch.abs(cum_prob - (delta < 0).float()) * huber_loss)\n",
    "\n",
    "    # Summing over the quantile dimension \n",
    "    if sum_over_quantiles:\n",
    "        loss = loss.sum(dim=-2).mean()\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# MLP for actor that implements D2RL architecture\n",
    "class ActorMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim, output size = action dim\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    current_dim = input_dim # first layer has input dim = state dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim # prev layers output + original input size\n",
    "        \n",
    "    # Final layer input dim is last hidden layer output + original input size\n",
    "    self.last_layer_mean_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "    self.last_layer_log_std_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim=1) # cat with output layer\n",
    "\n",
    "    mean_linear = self.last_layer_mean_linear(curr)\n",
    "    log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "    return mean_linear, log_std_linear\n",
    "\n",
    "# MLP for critic that implements D2RL architecture \n",
    "class CriticMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim + action dim, output size = n_quantiles\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    current_dim = input_dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim\n",
    "\n",
    "    self.last_layer = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim = 1)\n",
    "      \n",
    "    output = self.last_layer(curr)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class GradientStep(object):\n",
    "  '''\n",
    "  see (D2RL) https://github.com/pairlab/d2rl/blob/main/sac/sac.py\n",
    "  and (TQC) https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/tqc/tqc.py\n",
    "  '''\n",
    "  def __init__(self,*,\n",
    "    actor, critic, critic_target, discount, tau,\n",
    "    actor_lr, critic_lr, alpha_lr,\n",
    "    n_quantiles, n_mini_critics, top_quantiles_to_drop_per_net, target_entropy,\n",
    "    ):\n",
    "\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.critic_target = critic_target\n",
    "\n",
    "    self.log_alpha = nn.Parameter(torch.zeros(1).to(DEVICE)) # log alpha is learned\n",
    "    self.quantiles_total = n_quantiles * n_mini_critics\n",
    "    \n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "    self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    self.discount, self.tau = discount, tau\n",
    "    self.top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_mini_critics # total number of quantiles to drop\n",
    "    self.target_entropy = target_entropy\n",
    "\n",
    "  def take_gradient_step(self, replay_buffer, total_steps, batch_size=256):\n",
    "    # Sample batch from replay buffer\n",
    "    state, action, next_state, reward, not_done = replay_buffer.sample(batch_size, total_steps)\n",
    "    alpha = torch.exp(self.log_alpha) # entropy temperature coefficient\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # sample new action from actor on next state\n",
    "      new_next_action, next_log_pi = self.actor(next_state)\n",
    "\n",
    "      # Compute and cut quantiles at the next state\n",
    "      next_z = self.critic_target(next_state, new_next_action)  \n",
    "      \n",
    "      # Sort and drop top k quantiles to control overestimation (TQC)\n",
    "      sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "      sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop] # estimated truncated Q-val dist for next state\n",
    "\n",
    "      # td error + entropy term\n",
    "      target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "    \n",
    "    # Get current quantile estimates using action from the replay buffer\n",
    "    cur_z = self.critic(state, action)\n",
    "    critic_loss = quantile_huber_loss(cur_z, target)\n",
    "\n",
    "    new_action, log_pi = self.actor(state)\n",
    "    # detach the variable from the graph so we don't change it with other losses\n",
    "    alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean() # as in D2RL implementation for auto entropy tuning\n",
    "\n",
    "    # Optimise critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # Soft update target networks\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Compute actor (Ï€) loss\n",
    "    # JÏ€ = ð”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\n",
    "    actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "    # ^ mean(2) is over quantiles, mean(1) is over critic ensemble\n",
    "    \n",
    "    # Optimise the actor\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # Optimise the entropy coefficient\n",
    "    self.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    self.alpha_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[512, 512]):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mlp = ActorMLP(state_dim, hidden_dims, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.mlp(obs)\n",
    "        log_std = log_std.clamp(-20, 2) # clamp for stability\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        base_N_dist = D.Normal(mean, std) # base normal dist\n",
    "        tanh_transform = TanhTransform(cache_size=1) # transform to get the tanh dist\n",
    "\n",
    "        log_prob = None\n",
    "        if self.training: # i.e. agent.train()\n",
    "            transformed_dist = D.TransformedDistribution(base_N_dist, tanh_transform) # transformed distribution\n",
    "            action = transformed_dist.rsample() # samples from base dist & applies transform\n",
    "            log_prob = transformed_dist.log_prob(action) # log prob of action after transform\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True) # sum over action dim\n",
    "        else: # evaluation mode\n",
    "            action = torch.tanh(mean)\n",
    "\n",
    "        # FROM ORIGINAL CODE using custom tanh dist\n",
    "        # if self.training:\n",
    "        #     tanh_dist = TanhNormal(mean, std) #Â use custom tanh dist\n",
    "        #     action, pre_tanh = tanh_dist.random_sample()\n",
    "        #     log_prob = tanh_dist.log_probability(pre_tanh)\n",
    "        #     log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        # else:\n",
    "        #     action = torch.tanh(mean)\n",
    "            \n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).to(DEVICE)\n",
    "        if obs.ndim == 1: # add batch dim if missing\n",
    "             obs = obs.unsqueeze(0)\n",
    "        act, _ = self.forward(obs)\n",
    "        return np.array(act[0].cpu().detach())\n",
    "\n",
    "\n",
    "class Critic(nn.Module): # really a mega-critic from lots of mini-critics\n",
    "    '''\n",
    "    Ensemble of critics for TQC\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.critics = nn.ModuleList()\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        for _ in range(n_nets): # multiple critic mlps\n",
    "            net = CriticMLP(state_dim + action_dim, hidden_dims, n_quantiles)\n",
    "            self.critics.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # cat state and action (to pass to critic)\n",
    "        state_act = torch.cat((state, action), dim=1)\n",
    "\n",
    "        # get quantiles from each critic mlp\n",
    "        quantiles = [critic(state_act) for critic in self.critics]\n",
    "        quantiles = torch.stack(quantiles, dim=1) # stack into tensor\n",
    "        return quantiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "H466HICfpDB1"
   },
   "outputs": [],
   "source": [
    "# XinJingHao's (some of) hyperparams\n",
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# params = {\n",
    "#   'eval_interval' : int(2e3),  # Model evaluating interval, in steps\n",
    "#   'delay_freq' : 1,  # Delayed frequency for Actor and Target Net\n",
    "#   'explore_noise' : 0.15,  # Exploring noise when interacting\n",
    "#   'explore_noise_decay' : 0.998,  # Decay rate of explore noise\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My hyperparams\n",
    "\n",
    "seed = 42 # DONT CHANGE FOR COURSEWORK\n",
    "# use_wandb = False # use wandb for logging\n",
    "\n",
    "hyperparams = {\n",
    "    # env/general params\n",
    "    \"max_timesteps\": MAX_TIMESTEPS, # per episode [DONT CHANGE]\n",
    "    \"max_episodes\": 350,\n",
    "    \"target_score\": 300, # stop training when average score over r_list > target_score\n",
    "    \"len_r_list\": 100, # length of reward list to average over for target score (stop training when avg > target_score)\n",
    "    \"hardcore\": False, #Â fixed in wandb sweep\n",
    "    \"init_rand_steps\": 10000, # number of steps to take with random actions before training (helps exploration)\n",
    "\n",
    "    # Agent hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "    \"n_mini_critics\": 5, # each mini-critic is a single mlp, which combine to make one mega-critic\n",
    "    \"n_quantiles\": 20, # quantiles per mini critic\n",
    "    \"top_quantiles_to_drop_per_net\": 'auto', # per mini critic (auto based on n_quantiles)\n",
    "    \"actor_hidden_dims\": [512, 512],\n",
    "    \"mini_critic_hidden_dims\": [256, 256], # * n_mini_critics\n",
    "    \"batch_size\": 256,\n",
    "    \"discount\": 0.98, # gamma\n",
    "    \"tau\": 0.005,\n",
    "    \"actor_lr\": 3.67e-4, # empirically chosen\n",
    "    \"critic_lr\": 3.89e-4, # empirically chosen\n",
    "    \"alpha_lr\": 3.34e-4, # empirically chosen\n",
    "\n",
    "    # ERE buffer (see paper for their choices)\n",
    "    \"buffer_size\": 500000, # smaller size improves learning early on but is outperformed later on\n",
    "    \"eta0\": 0.994, # 0.994 - 0.999 is good (according to paper)\n",
    "    \"annealing_steps\": 'auto', # number of steps to anneal eta over (after which sampling is uniform) - None = auto-set to max estimated steps in training\n",
    "    \"cmin\": 5000, # min number of samples to sample from\n",
    "\n",
    "    # dreamer hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "    # info on hyperparam choice: https://arxiv.org/abs/1912.01603\n",
    "    \"use_dreamer\": True,\n",
    "    \"intrinsic_reward_scale\": 0.0, # scale factor for dreamer intrinsic reward\n",
    "    \"batch_size_dreamer\": 512,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 16,\n",
    "    \"num_heads\": 4,\n",
    "    \"dreamer_lr\": 3e-4,\n",
    "    \"dropout_prob\": 0.1,\n",
    "    \"window_size\": 40,               # transformer context window size\n",
    "    \"step_size\": 1,                  # how many timesteps to skip between each context window\n",
    "    \"train_split\": 0.8,              # train/validation split\n",
    "    \"loss_threshold\": 0.8,           # use dreamer if loss < loss_threshold\n",
    "    \"imagination_horizon\": 15,       # how many timesteps to run the dreamer model for (H in Dreamer paper)\n",
    "    \"dreamer_train_epochs\": 15,      # how many epochs to train the dreamer model for\n",
    "    \"dreamer_train_frequency\": 10,   # how often to train the dreamer model\n",
    "    \"episode_threshold\": 50,         # how many episodes to run before training the dreamer model\n",
    "    \"max_size\": 50000,            # maximum size of the training set for the dreamer model\n",
    "}\n",
    "\n",
    "# recording/logging\n",
    "plot_interval = 100 # plot every Nth episode (wandb still plots every ep)\n",
    "save_fig = True # save figures too\n",
    "\n",
    "is_recording = True\n",
    "if hyperparams['hardcore']: # NOTE this doesnt change in wandb sweep\n",
    "    video_interval = 30 # record every Nth episode\n",
    "    ep_start_rec = 500 # start recording on this episode\n",
    "else:\n",
    "    video_interval = 20\n",
    "    ep_start_rec = 50\n",
    "\n",
    "# use_checkpointing = False # enable/disable checkpointing (NOTE useful for hardcore. Delete in terminal w/ rm -rf ./checkpoints)\n",
    "# checkpoint_interval = 30 # save checkpoint every N episodes\n",
    "\n",
    "if hyperparams['annealing_steps'] == 'auto':\n",
    "    hyperparams['annealing_steps'] = hyperparams['max_episodes']*hyperparams['max_timesteps'] # max est number of steps in training\n",
    "if hyperparams['top_quantiles_to_drop_per_net'] == 'auto':\n",
    "    hyperparams['top_quantiles_to_drop_per_net'] = int(hyperparams['n_quantiles'] // 12.5) # keep ratio same as M=25 d=2 (from TQC paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/wandb/run-20250502_103914-ske1ajrk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/ske1ajrk' target=\"_blank\">_2025-05-02_10-39-14_hardcore</a></strong> to <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d' target=\"_blank\">https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/ske1ajrk' target=\"_blank\">https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/ske1ajrk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SETUP\n",
    "\n",
    "# wandb & checkpoints\n",
    "\n",
    "# load checkpoint\n",
    "loaded_checkpoint = None\n",
    "wandb_run_id = None\n",
    "start_episode = 1 # Default start episode\n",
    "# if use_checkpointing:\n",
    "#     loaded_checkpoint = load_checkpoint()\n",
    "#     if loaded_checkpoint:\n",
    "#         wandb_run_id = loaded_checkpoint.get('wandb_run_id')\n",
    "#         # Load hyperparameters from checkpoint if needed, or ensure consistency\n",
    "#         # config = loaded_checkpoint['config'] # Be careful with overriding wandb.config\n",
    "#         start_episode = loaded_checkpoint['episode'] + 1 # Start from the next episode\n",
    "#         print(f\"Resuming from episode {start_episode}\")\n",
    "\n",
    "# init wandb run\n",
    "import datetime\n",
    "suffix = '_hardcore' if hyperparams['hardcore'] else ''\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "run_name = SOURCE_ID + f\"_{timestamp}\" + suffix\n",
    "\n",
    "wandb_init_kwargs = {\n",
    "    \"project\": \"RL-Coursework-Walker2d\",\n",
    "    \"config\": hyperparams,\n",
    "    \"name\": run_name,\n",
    "    \"save_code\": True, # optionally saves code to wandb\n",
    "}\n",
    "if wandb_run_id:\n",
    "    wandb_init_kwargs[\"id\"] = wandb_run_id\n",
    "    wandb_init_kwargs[\"resume\"] = \"allow\" # Or \"must\" if you require resuming\n",
    "\n",
    "wandb.init(**wandb_init_kwargs)\n",
    "\n",
    "config = wandb.config # use wandb.config to access hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1Xrcek4hxDXl",
    "outputId": "6161f148-a164-4e22-d97a-f9aa53e7b8b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make env\n",
    "# only attempt hardcore when your agent has solved the non-hardcore version\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=config.hardcore)\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "video_prefix = \"nchw73-agent-hardcore-video\" if config.hardcore else \"nchw73-agent-video\"\n",
    "video_prefix = f\"{timestamp}_\" + video_prefix #Â mark video with date/time to be mateched to run (remove later!)\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=video_prefix,          # prefix for videos\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "rld.check_device() # training on CPU recommended\n",
    "\n",
    "env.video = False # switch video recording off (only switch on every x episodes as this is slow)\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, state_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=seed)\n",
    "# rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWjENKkU_My"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, state, info = rld.seed_everything(seed, env)\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "replay_buffer = EREReplayBuffer(state_dim, act_dim, config.annealing_steps, config.buffer_size, config.eta0, config.cmin)\n",
    "\n",
    "actor = Actor(state_dim, act_dim, config.actor_hidden_dims).to(DEVICE)\n",
    "\n",
    "critic = Critic(state_dim, act_dim, config.n_quantiles, config.n_mini_critics, config.mini_critic_hidden_dims).to(DEVICE)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "dreamer = None #Â init as none\n",
    "if config.use_dreamer:\n",
    "    dreamer = DreamerAgent(state_dim, act_dim, config.hidden_dim, \n",
    "        config.window_size, config.num_layers, config.num_heads, config.dropout_prob, config.dreamer_lr).to(DEVICE)\n",
    "\n",
    "target_entropy = -np.prod(env.action_space.shape).item() # target entropy heuristic (= âˆ’dim(A) as in D2RL paper)\n",
    "grad_step_class = GradientStep(\n",
    "    actor=actor, critic=critic, critic_target=critic_target,\n",
    "    discount=config.discount, tau=config.tau,\n",
    "    actor_lr=config.actor_lr, critic_lr=config.critic_lr, alpha_lr=config.alpha_lr,\n",
    "    n_quantiles=config.n_quantiles, n_mini_critics=config.n_mini_critics,\n",
    "    top_quantiles_to_drop_per_net=config.top_quantiles_to_drop_per_net,\n",
    "    target_entropy=target_entropy,\n",
    "    )\n",
    "\n",
    "actor.train()\n",
    "\n",
    "total_steps = 0\n",
    "memory_ptr = 0 # marks boundary between new and old data in the replay buffer\n",
    "train_set, test_set = None, None\n",
    "recent_rewards= []\n",
    "ep_timesteps_dreamer = ep_reward_dreamer = dreamer_eps = 0\n",
    "last_dreamer_test_loss = 0.0 # track dreamer test loss\n",
    "completed_env = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 1 | Timesteps: 1477 | Reward: -145.080 | Total Steps: 1.5e+03 | Dreamer: False\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m# float32 for mps\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# sample real env\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m ep_timesteps, ep_reward, input_buffer, info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_step_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_intrinsic_reward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_dreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# use intrinsic reward if dreamer active\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_dreamer_test_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dreamer_test_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintrinsic_r_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintrinsic_reward_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_timesteps\n\u001b[1;32m     24\u001b[0m tracker\u001b[38;5;241m.\u001b[39mtrack(info) \u001b[38;5;66;03m# track statistics for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 93\u001b[0m, in \u001b[0;36mtrain_on_environment\u001b[0;34m(actor, env, grad_step_class, replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length, use_intrinsic_reward, last_dreamer_test_loss, intrinsic_r_scale)\u001b[0m\n\u001b[1;32m     89\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size: \u001b[38;5;66;03m#Â do 1 training update using 1 batch from buffer if enough\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# train the agent using experiences from the real environment\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mgrad_step_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;66;03m# break if finished\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 39\u001b[0m, in \u001b[0;36mGradientStep.take_gradient_step\u001b[0;34m(self, replay_buffer, total_steps, batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m new_next_action, next_log_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(next_state)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Compute and cut quantiles at the next state\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m next_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_next_action\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Sort and drop top k quantiles to control overestimation (TQC)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m sorted_z, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(next_z\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     61\u001b[0m state_act \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((state, action), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# get quantiles from each critic mlp\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m [critic(state_act) \u001b[38;5;28;01mfor\u001b[39;00m critic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritics]\n\u001b[1;32m     65\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(quantiles, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# stack into tensor\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantiles\n",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m state_act \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((state, action), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# get quantiles from each critic mlp\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m [\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_act\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m critic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritics]\n\u001b[1;32m     65\u001b[0m quantiles \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(quantiles, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# stack into tensor\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantiles\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 61\u001b[0m, in \u001b[0;36mCriticMLP.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_list:\n\u001b[1;32m     60\u001b[0m   curr \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(layer(curr))\n\u001b[0;32m---> 61\u001b[0m   curr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([curr, input_], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer(curr)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode = start_episode #Â =1 by default or whatever was in the checkpoint\n",
    "\n",
    "if episode == 1: # random warmup phase\n",
    "    state, _ = env.reset() # Get a fresh state to start warmup\n",
    "    state, total_steps = run_warm_up(env, replay_buffer, config.init_rand_steps, state)\n",
    "\n",
    "# training loop\n",
    "while episode <= config.max_episodes: # index from 1\n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    if is_recording and episode >= ep_start_rec:\n",
    "        env.info = episode % video_interval == 0   # track every x episodes (usually tracking every episode is fine)\n",
    "        env.video = episode % video_interval == 0  # record videos every x episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    state, info = env.reset()\n",
    "    state = state.astype(np.float32) # float32 for mps\n",
    "\n",
    "    # sample real env\n",
    "    ep_timesteps, ep_reward, input_buffer, info = train_on_environment(\n",
    "        actor, env, grad_step_class, replay_buffer, config.max_timesteps, state,\n",
    "        config.batch_size, total_steps, config.window_size, is_dreamer=False,\n",
    "        use_intrinsic_reward=(config.use_dreamer and episode > config.episode_threshold), # use intrinsic reward if dreamer active\n",
    "        last_dreamer_test_loss=last_dreamer_test_loss,\n",
    "        intrinsic_r_scale=config.intrinsic_reward_scale,\n",
    "        )\n",
    "\n",
    "    total_steps += ep_timesteps\n",
    "    tracker.track(info) # track statistics for plotting\n",
    "\n",
    "    # update train/test sets for dreamer (add new eps and trim to window size)\n",
    "    train_set, test_set = gen_test_train_seq(\n",
    "        replay_buffer, train_set, test_set, config.train_split, config.window_size, config.step_size, memory_ptr)\n",
    "\n",
    "    # train dreamer after ep_thresh and if the input buffer has enough data to fill context window\n",
    "    if config.use_dreamer and episode >= config.episode_threshold and input_buffer.shape[0] == config.window_size:\n",
    "        # train and assess the dreamer every train_frequency\n",
    "        if episode % config.dreamer_train_frequency == 0:\n",
    "            dreamer.train_dreamer(train_set, config.dreamer_train_epochs, config.batch_size_dreamer)\n",
    "\n",
    "        # truncate the training set to control train time performance\n",
    "        if test_set.shape[0] > config.max_size:\n",
    "            train_set = train_set[-config.max_size:]\n",
    "\n",
    "        print(f'Size of train set: {train_set.shape[0]}, test set: {test_set.shape[0]}')\n",
    "        \n",
    "        # evaluate the dreamer's performance & decide num of training steps for dreamer\n",
    "        dreamer_avg_loss = dreamer.test_dreamer(test_set, config.batch_size_dreamer)\n",
    "        dreamer_eps = get_dreamer_eps(dreamer_avg_loss, config.loss_threshold)\n",
    "\n",
    "        # train on dreamer if its accurate enough\n",
    "        H = config.imagination_horizon # H = imagination horizon\n",
    "        if dreamer_eps > 0:\n",
    "            # scale H based on dreamer loss: 1 if loss = 0 and 0 if loss >= thresh \n",
    "            H_scale_factor = max(0., 1. - (dreamer_avg_loss / config.loss_threshold))\n",
    "            H = int(H * H_scale_factor ** 2) # quadratic scaling to encourage better models\n",
    "\n",
    "            print(f'Dreamer active for {dreamer_eps} iterations')\n",
    "            ep_timesteps_dreamer = ep_reward_dreamer = 0\n",
    "            for dep in range(dreamer_eps):\n",
    "                print(f'Dreamer ep: {dep+1}')\n",
    "\n",
    "                # initialise dreamer states with the input sequence\n",
    "                # input buffer = state, action, next_state, reward, done NOTE - this is different to order learned in training\n",
    "                dreamer.states = input_buffer[:, :state_dim]\n",
    "                dreamer.actions = input_buffer[:-1, state_dim:state_dim+act_dim]\n",
    "                dreamer.rewards = input_buffer[:-1, -2].unsqueeze(1)\n",
    "                dreamer.dones = input_buffer[:-1, -1].unsqueeze(1)\n",
    "\n",
    "                # sample from dreamer environment (ignore input buffer and info)\n",
    "                _td, _rd, _, _ = train_on_environment(\n",
    "                    actor, dreamer, grad_step_class, replay_buffer,\n",
    "                    H, # number of timesteps to run the dreamer for\n",
    "                    dreamer.states[-1].cpu().numpy(), # initial state for dreamer\n",
    "                    config.batch_size, total_steps, config.window_size, is_dreamer=True,\n",
    "                    use_intrinsic_reward=False)\n",
    "\n",
    "                ep_timesteps_dreamer += _td\n",
    "                ep_reward_dreamer += _rd\n",
    "\n",
    "    memory_ptr = replay_buffer.ptr # update the memory pointer to the current position in the buffer\n",
    "\n",
    "    print(f\"Ep: {episode} | Timesteps: {ep_timesteps} | Reward: {ep_reward:.3f} | Total Steps: {total_steps:.2g} | Dreamer: {dreamer_eps > 0}\")\n",
    "    if config.use_dreamer and dreamer_eps > 0:\n",
    "        print(f\"\\t Dreamer Eps: {dreamer_eps} | Dreamer Avg Reward: {ep_reward_dreamer/dreamer_eps:.3f} | Dreamer Avg Timesteps: {ep_timesteps_dreamer/dreamer_eps:.3g}\")\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    log_dict = {\n",
    "        \"Total Steps\": total_steps,\n",
    "        \"Episode Timesteps\": ep_timesteps,\n",
    "    }\n",
    "\n",
    "    # log tracked statistics\n",
    "    if 'recorder' in info: # if we have info available\n",
    "        log_dict[\"TrackedInfo/r_mean_\"] = info['recorder']['r_mean_'] # r is extrinsic only\n",
    "        log_dict[\"TrackedInfo/r_std_\"] = info['recorder']['r_std_']\n",
    "        log_dict[\"TrackedInfo/r_sum\"] = info['recorder']['r_sum']\n",
    "\n",
    "    if config.use_dreamer:\n",
    "        log_dict[\"Dreamer Average Loss\"] = dreamer_avg_loss if 'dreamer_avg_loss' in locals() else None # Only log if calculated\n",
    "        log_dict[\"Dreamer Episodes Run\"] = dreamer_eps\n",
    "        if dreamer_eps > 0:\n",
    "            log_dict[\"Dreamer Average Reward\"] = ep_reward_dreamer / dreamer_eps\n",
    "            log_dict[\"Dreamer Average Timesteps\"] = ep_timesteps_dreamer / dreamer_eps\n",
    "\n",
    "    wandb.log(log_dict, step=episode) # log metrics each ep\n",
    "\n",
    "    recent_rewards.append(ep_reward)\n",
    "\n",
    "    # env completion (break) condition - stop if we consistently meet target score\n",
    "    if len(recent_rewards) >= config.len_r_list:\n",
    "        current_avg = np.array(recent_rewards).mean()\n",
    "        print(f'Current progress: {current_avg:.3f} / {config.target_score}')\n",
    "        if current_avg >= config.target_score: # quit when we've got good enough performance\n",
    "            completed_env = True\n",
    "        recent_rewards = recent_rewards[-config.len_r_list+1:] # discard oldest on list (keep most recent 99)\n",
    "\n",
    "    # plot tracked statistics (on real env)\n",
    "    if episode % plot_interval == 0 or completed_env: # always save last plot\n",
    "        # save as well (show=False returns it but doesnt display)\n",
    "        if save_fig and info: # check info isnt {} (i.e. not None)\n",
    "            fig, _ = tracker.plot(show=False, r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "            wandb.log({\"Coursework Plot\": wandb.Image(fig)}, step=episode) # Log the figure directly to W&B\n",
    "            # fig.savefig(f'./tracker_{run_name}.png', bbox_inches = 'tight')\n",
    "            plt.close(fig) # Close the figure to free memory\n",
    "        # show by default\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "    if completed_env:\n",
    "        print(f\"Completed environment in {episode} episodes!\")\n",
    "        break\n",
    "\n",
    "    episode += 1\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file\n",
    "if config.hardcore:\n",
    "    filename = f\"[{run_name}]_\" + \"nchw73-agent-hardcore-log.txt\"\n",
    "else:\n",
    "    filename = f\"[{run_name}]_\" + \"nchw73-agent-log.txt\"\n",
    "env.write_log(folder=\"logs\", file=filename)\n",
    "\n",
    "wandb.finish() # finish wandb run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
