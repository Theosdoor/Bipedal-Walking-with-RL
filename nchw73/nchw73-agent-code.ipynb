{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# FOR .py FILE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclosures**\n",
    "\n",
    "The use of TQC+D2RL+ERE+Dreamer was inspired by this implementation which gave good results on BipedalWalker, which was used as a starting point - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "However, I made some important changes from this:\n",
    "* The ERE buffer anneals 'eta' which determins the amount of emphasis to put on recent events. Originally, eta annealed over the course of each episode, then reset for the next one. In contrast, in this implementation eta anneals over the whole training process, and doesn't reset at any point. This is more in line with the original paper I believe.\n",
    "* Attempt to introduce intrinsic rewards for exploring where dreamer is uncertain.\n",
    "\n",
    "\n",
    "\n",
    "Paper references:\n",
    "* TQC - https://bayesgroup.github.io/tqc/\t\n",
    "* ERE - https://arxiv.org/abs/1906.04009 \n",
    "* D2RL - https://sites.google.com/view/d2rl/home\n",
    "* Transformer Dreamer - https://arxiv.org/abs/2202.09481\n",
    "* Intrinsic rewards - https://arxiv.org/abs/1908.06976\n",
    "* Adjusting imagination horizon - https://arxiv.org/abs/2009.09593"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "# **Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSUGAJsTLOV",
    "outputId": "875609e2-e3eb-4b29-c03c-1989820b1ea7"
   },
   "outputs": [],
   "source": [
    "# https://github.com/robert-lieck/rldurham/tree/main - rldurham source\n",
    "\n",
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham\n",
    "\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AFrqd24JTLOW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.distributions as D\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "\n",
    "import rldurham as rld\n",
    "\n",
    "import wandb\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"x\" # TODO remove\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "# **RL agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TIMESTEPS = 2000 # [DONT CHANGE]\n",
    "SOURCE_ID = '' # mac, ncc, colab -> for personal id in wandb\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward adjustment\n",
    "def adjust_reward(reward):\n",
    "    \"\"\"\n",
    "    adjust reward for bipedal walker\n",
    "    \"\"\"\n",
    "    if reward == -100.0: \n",
    "        reward = -10.0 # bipedal walker does -100 for hitting floor, so -10 might make it more stable\n",
    "    else:\n",
    "        reward *= 2 # encourage forward motion\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERs\n",
    "# for https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def train_on_environment(actor, env, grad_step_class, replay_buffer, max_timesteps,\n",
    "    state, batch_size, total_steps, sequence_length, init_rand_steps=None, is_dreamer=False,\n",
    "    use_intrinsic_reward=False, last_dreamer_test_loss=0.0, intrinsic_r_scale=0.0,\n",
    "    ):\n",
    "    '''\n",
    "    abstracted training loop function for both real & dreamer environments to use\n",
    "    '''\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "\n",
    "    state_dim = actor.state_dim\n",
    "    act_dim = actor.action_dim\n",
    "    \n",
    "    # save start sequence for the dreamer model\n",
    "    input_buffer = [] # store as python list to avoid cat-ing constantly\n",
    "\n",
    "    # initial state float32 for mps\n",
    "    state = state.astype(np.float32)\n",
    "\n",
    "    for t in range(max_timesteps): \n",
    "        total_steps += 1\n",
    "        ep_timesteps += 1\n",
    "\n",
    "        # select action and step the env\n",
    "        if init_rand_steps is not None and total_steps < init_rand_steps: # do random steps at start for warm-up\n",
    "            action = env.action_space.sample()\n",
    "        else: # otherwise use actor policy\n",
    "            action = actor.select_action(state)\n",
    "\n",
    "        step = env.step(action)\n",
    "\n",
    "        # handle different return formats\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, info = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, info = step\n",
    "\n",
    "        # float32 before processing/storing\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        reward = float(reward) # extrinsic reward\n",
    "        done = float(done)\n",
    "\n",
    "        ep_reward += reward # sum extrinsic reward\n",
    "\n",
    "        r_final = reward\n",
    "        if not is_dreamer: # only adjust reward & add intrinsic for real env\n",
    "            r_final = adjust_reward(reward) # adjust reward for bipedal walker\n",
    "            if use_intrinsic_reward and last_dreamer_test_loss > 0:\n",
    "                r_intrinsic = intrinsic_r_scale * last_dreamer_test_loss\n",
    "                r_final += r_intrinsic\n",
    "                wandb.log({\"Intrinsic Reward Added\": r_intrinsic}, commit=False) # log to wandb (commit = false lets us log things later in the ep)\n",
    "\n",
    "        replay_buffer.add(state, action, next_state, r_final, done) # add to replay buffer for both real and dreamer\n",
    "\n",
    "        if not is_dreamer and t < sequence_length: # store in input buffer for dreamers first seq\n",
    "            trans = np.concatenate((\n",
    "                state, \n",
    "                action.astype(np.float32), \n",
    "                next_state, \n",
    "                np.array([reward], dtype=np.float32), # NOTE - store original dreamer reward, not adjusted r_final\n",
    "                np.array([done], dtype=np.float32)\n",
    "            ), axis=0)\n",
    "            # combined = torch.tensor(trans, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "            # input_buffer = torch.cat([input_buffer, combined], axis=0)\n",
    "            input_buffer.append(trans)\n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "        # train step if buffer has enough samples for a batch AND we're past warmup\n",
    "        if total_steps >= batch_size and init_rand_steps is not None and total_steps >= init_rand_steps:\n",
    "            # train the agent using experiences from the real environment\n",
    "            grad_step_class.take_gradient_step(replay_buffer, total_steps, batch_size)\n",
    "    \n",
    "        if done: # break if finished\n",
    "            break\n",
    "\n",
    "    final_input_buffer = None\n",
    "    if not is_dreamer and len(input_buffer) > 0: # return if any steps were collected\n",
    "        try:\n",
    "            # Ensure all collected transitions have the same dimension before converting\n",
    "            np_buffer = np.array(input_buffer)\n",
    "            final_input_buffer = torch.tensor(np_buffer, dtype=torch.float32).to(DEVICE)\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Error converting input_buffer to tensor: {e}\")\n",
    "            print(f\"Input buffer contents (first few elements): {input_buffer[:3]}\")\n",
    "\n",
    "    # only return extrinsic to eval based on that\n",
    "    return ep_timesteps, ep_reward, final_input_buffer, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# generate value array from replay buffer\n",
    "def retrieve_from_replay_buffer(replay_buffer, ptr):\n",
    "    '''\n",
    "    returns (state, action, reward, done, next_state)\n",
    "    '''\n",
    "    return np.concatenate((\n",
    "            replay_buffer.state[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.action[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.reward[ptr:replay_buffer.ptr],\n",
    "            1. - replay_buffer.not_done[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.next_state[ptr:replay_buffer.ptr],                \n",
    "        ), \n",
    "        axis = 1 # along the columns (so each row is a memory)\n",
    "    )\n",
    "\n",
    "# function to create sequences with a given window size and step size\n",
    "def create_sequences(memories, window_size, step_size):\n",
    "    '''\n",
    "    Function to create sequences of memories from the replay buffer.\n",
    "\n",
    "    Each sequence is of length window_size and each spaced apart by step_size\n",
    "    '''\n",
    "    n_memories = memories.shape[0] # just the number of time steps currently in the buffer\n",
    "\n",
    "    # calc number of seq (of len window_size) we can create from mems available  \n",
    "    n_sequences = math.floor((n_memories - window_size) / step_size) + 1 # +1 because indices start at 0\n",
    "\n",
    "    sequences = np.zeros((n_sequences, window_size, memories.shape[1]))\n",
    "    for i in range(n_sequences):\n",
    "        start_idx = i * step_size # idx to start seq from\n",
    "        sequences[i, :] = memories[start_idx:start_idx + window_size, :] # grab seq of memories window_size long from start_idx\n",
    "    return sequences\n",
    "\n",
    "def gen_test_train_seq(replay_buffer, train_set, test_set, train_split, window_size, step_size, ptr):\n",
    "    '''\n",
    "    Function to split train and test data\n",
    "    '''\n",
    "    memories = retrieve_from_replay_buffer(replay_buffer, ptr)\n",
    "    try:\n",
    "        memory_sequences = create_sequences(memories, window_size, step_size)\n",
    "        n_sequences = memory_sequences.shape[0]\n",
    "    except: # TODO How might this go wrong? not enough new data to create a sequence?\n",
    "        return train_set, test_set\n",
    "\n",
    "    # shuffle the sequences & split\n",
    "    indices = np.arange(n_sequences)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(train_split * n_sequences) # get split point \n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "\n",
    "    if train_set is None: # if this is first train/test set, create the sets\n",
    "        return memory_sequences[train_indices, :], memory_sequences[test_indices, :]\n",
    "    # else just add to existing sets\n",
    "    return np.concatenate((train_set, memory_sequences[train_indices, :]), axis=0), np.concatenate((test_set, memory_sequences[test_indices, :]), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dreamer_eps(dreamer_avg_loss, loss_threshold):\n",
    "    '''\n",
    "    Get number of dreamed episdoes the agent should run on the dreamer model.\n",
    "\n",
    "    Only use dreamer for training if it's sufficiently accurate model of env.\n",
    "\n",
    "    Note - loss_threshold is between 0 and 1.\n",
    "\n",
    "    Disclaimer - calculation from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    '''\n",
    "    max_dreamer_it = 10 # a perfect dreamer has this many eps\n",
    "\n",
    "    if dreamer_avg_loss >= loss_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        norm_score = dreamer_avg_loss/loss_threshold # normalise score relative to threshold\n",
    "        inv_score = 1 - norm_score # invert so that closer to 1 ==> dreamer score much better than threshold\n",
    "        sq_score = inv_score**2 # square so that num iter increases quadratically as accuracy improves\n",
    "        return int(max_dreamer_it * sq_score) # scale so that max iterations is 10 (when dreamer very accurate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kTeUHYL7gU6"
   },
   "outputs": [],
   "source": [
    "class EREReplayBuffer(object):\n",
    "    '''\n",
    "    ERE implementation - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    ERE paper - https://arxiv.org/abs/1906.04009\n",
    "    \n",
    "    Prioritized Experience Replay (PER) buffer, as in https://arxiv.org/abs/1511.05952.\n",
    "    Implementation: https://github.com/BY571/Soft-Actor-Critic-and-Extensions/blob/master/SAC_PER.py\n",
    "        PER parameters are sourced from this implementation.\n",
    "\n",
    "\n",
    "    Rationale:\n",
    "    - actions from early in training are likely v different to optimal, so want to prioritise recent ones so we're not constantly learning from crap ones\n",
    "    - as time goes on, they get more similar so annealing eta allows uniform sampling later on\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, T, max_size, eta, cmin, use_per=True,\n",
    "                beta_1=0.6, beta_2_start=0.4, beta_2_frames=int(1e5), epsilon=1e-6, recency_scale=1):\n",
    "\n",
    "        self.max_size, self.ptr, self.size, self.rollover = max_size, 0, 0, False\n",
    "        self.use_per = use_per # use PER flag\n",
    "        \n",
    "        # ERE params\n",
    "        self.eta0 = eta\n",
    "        self.cmin = cmin\n",
    "        self.T = T\n",
    "        self.recency_scale = recency_scale # when using PER too, this helps calculate ck\n",
    "\n",
    "        # PER params\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2_start = beta_2_start\n",
    "        self.beta_2_frames = beta_2_frames\n",
    "        self.epsilon = epsilon\n",
    "        self.frame = 1 # for beta calculation\n",
    "\n",
    "        # storage\n",
    "        self.reward = np.empty((max_size, 1), dtype=np.float32) # float32 for mps\n",
    "        self.state = np.empty((max_size, state_dim), dtype=np.float32)\n",
    "        self.action = np.empty((max_size, action_dim), dtype=np.float32)\n",
    "        self.not_done = np.empty((max_size, 1), dtype=np.float32)\n",
    "        self.next_state = np.empty((max_size, state_dim), dtype=np.float32)\n",
    "        self.priorities = np.zeros((max_size, 1), dtype=np.float32) # for PER\n",
    "        self.max_prio = 1.0 # max priority seen so far\n",
    "\n",
    "    def eta_anneal(self, t):\n",
    "        # eta anneals over time --> 1, which reduces emphasis on recent experiences over time\n",
    "        return min(1.0, self.eta0 + (1 - self.eta0) * t / self.T)\n",
    "\n",
    "    def beta_2_by_frame(self, frame_idx):\n",
    "        \"\"\"\n",
    "        Linearly increases beta from beta_2_start to 1 over time from 1 to beta_2_frames.\n",
    "        \n",
    "        see 3.4 ANNEALING THE BIAS (Paper: PER)\n",
    "        \"\"\"\n",
    "        return min(1.0, self.beta_2_start + frame_idx * (1.0 - self.beta_2_start) / self.beta_2_frames)\n",
    " \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        # Add experience to replay buffer \n",
    "        self.state[self.ptr] = state.astype(np.float32)\n",
    "        self.action[self.ptr] = action.astype(np.float32)\n",
    "        self.next_state[self.ptr] = next_state.astype(np.float32)\n",
    "        self.reward[self.ptr] = float(reward)\n",
    "        self.not_done[self.ptr] = 1. - float(done)\n",
    "        self.priorities[self.ptr] = self.max_prio if self.size > 0 else 1.0 # gives max priority if buffer is not empty else 1 TODO\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.ptr %= self.max_size\n",
    "\n",
    "        # increase size counter until full, then start overwriting (rollover)\n",
    "        if self.max_size > self.size + 1:\n",
    "          self.size += 1\n",
    "        else:\n",
    "          self.size = self.max_size\n",
    "          self.rollover = True\n",
    "\n",
    "    def sample(self, batch_size, t):\n",
    "        # update eta value for current timestep\n",
    "        eta = self.eta_anneal(t)\n",
    "\n",
    "        # get ERE window -----\n",
    "        # get ck\n",
    "        c_calc = self.size * eta ** (t / self.T * self.recency_scale)\n",
    "        ck = int(max(self.cmin, c_calc)) # at least cmin samples\n",
    "        ck = min(ck, self.size) # limit to buffer size\n",
    "\n",
    "        # Determine indices within the recent window (handle rollover)\n",
    "        if not self.rollover:\n",
    "            # Buffer not full yet, window is from [size - ck, size)\n",
    "            indices = np.arange(max(0, self.size - ck), self.size)\n",
    "        else:\n",
    "            # Buffer has rolled over, window wraps around\n",
    "            start_idx = (self.ptr - ck + self.max_size) % self.max_size\n",
    "            if start_idx < self.ptr:\n",
    "                indices = np.arange(start_idx, self.ptr)\n",
    "            else:\n",
    "                indices = np.concatenate((np.arange(start_idx, self.max_size), np.arange(0, self.ptr)))\n",
    "\n",
    "        # ensure ck matches the actual number of indices determined\n",
    "        ck = len(indices)\n",
    "        if ck <= 0:\n",
    "            print('uh oh, ck is 0! This shouldnt happen....')\n",
    "\n",
    "        # PER -----\n",
    "        if self.use_per:\n",
    "            # calc P = p^a/sum(p^a)\n",
    "            prios = self.priorities[indices].flatten() + self.epsilon # add epsilon to avoid zero probs\n",
    "            probs = prios ** self.beta_1\n",
    "            probs_sum = probs.sum()\n",
    "            if probs_sum <= 0: # avoid division by zero if all prios = zero\n",
    "                P = np.ones(ck, dtype=np.float32) / ck # uniform sample\n",
    "            else:\n",
    "                P = probs / probs_sum\n",
    "            \n",
    "            # gets the indices depending on the probability p and the c_k range of the buffer\n",
    "            rel_indices = np.random.choice(ck, batch_size, p=P, replace=True)\n",
    "            samples = indices[rel_indices]\n",
    "            \n",
    "            beta_2 = self.beta_2_by_frame(self.frame)\n",
    "            self.frame += 1 # incremement for annealing\n",
    "                    \n",
    "            # Compute importance-sampling weights w = (N * P)^(-beta_2)\n",
    "            weights  = (ck * P[rel_indices]) ** (-beta_2)\n",
    "            # normalize weights\n",
    "            weights /= weights.max() \n",
    "            weights = np.array(weights, dtype=np.float32)\n",
    "        else:\n",
    "            print('PER not used, using uniform sampling') if self.use_per else None\n",
    "            # otherwise sample uniformly like in basic ere\n",
    "            rel_indices = np.random.choice(ck, batch_size, replace=True)\n",
    "            samples = indices[rel_indices]\n",
    "            # return weights of 1\n",
    "            weights = np.ones(batch_size, dtype=np.float32)\n",
    "\n",
    "\n",
    "        r = torch.tensor(self.reward[samples], dtype=torch.float32).to(DEVICE)\n",
    "        s = torch.tensor(self.state[samples], dtype=torch.float32).to(DEVICE)\n",
    "        ns = torch.tensor(self.next_state[samples], dtype=torch.float32).to(DEVICE)\n",
    "        a = torch.tensor(self.action[samples], dtype=torch.float32).to(DEVICE)\n",
    "        nd = torch.tensor(self.not_done[samples], dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        return s, a, ns, r, nd, samples, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        priorities = np.abs(batch_priorities) + self.epsilon # ensure > 0\n",
    "\n",
    "        self.priorities[batch_indices] = priorities.reshape(-1, 1) \n",
    "        self.max_prio = max(self.max_prio, np.max(priorities)) # update max prio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dreamer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamerAgent(nn.Module):\n",
    "    '''\n",
    "    Dreamer agent.\n",
    "    Uses a transformer model to predict the next state, reward and done signal given the current (state, action, reward, done)\n",
    "\n",
    "    Acknowledgements:\n",
    "    - implementation based on https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    - (from implementation designer) key ideas and concepts for the auto-regressive transformer design stem from this paper: https://arxiv.org/abs/2202.09481\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, seq_len, num_layers, num_heads, dropout_prob, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = state_dim + action_dim + 2 # (state, action, reward, done)\n",
    "        self.target_dim = self.state_dim + self.action_dim # (state, action)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.input_fc = nn.Linear(self.input_dim, hidden_dim).to(DEVICE)\n",
    "        self.target_fc = nn.Linear(self.target_dim, hidden_dim).to(DEVICE)\n",
    "\n",
    "        self.transformer = nn.Transformer( # uses transformer model!\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dropout=dropout_prob,\n",
    "            activation=F.gelu,\n",
    "            batch_first=True\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        self.output_next_state = nn.Linear(hidden_dim + self.target_dim, state_dim).to(DEVICE)\n",
    "        self.output_reward = nn.Linear(hidden_dim + self.target_dim, 1).to(DEVICE)\n",
    "        self.output_done = nn.Linear(hidden_dim + self.target_dim, 1).to(DEVICE)\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "    # separate out the ground truth variables and compare against predictions\n",
    "    def loss_fn(self, output_next_state, output_reward, output_done, ground_truth):\n",
    "        reward, done, next_state = torch.split(ground_truth, [1, 1, self.state_dim], dim=-1)\n",
    "        loss = self.mse_loss(output_next_state[:, -1], next_state)\n",
    "        loss += self.mse_loss(output_reward[:, -1], reward)\n",
    "        loss += self.bce_loss(output_done[:, -1], done.float())\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # separate the input and target tensors\n",
    "        target = input_tensor[:, -1, :self.target_dim].unsqueeze(1)\n",
    "        encoded_target = self.target_fc(target)\n",
    "        encoded_input = self.input_fc(input_tensor[:, :-1, :self.input_dim])\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target], axis=2))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target], axis=2))\n",
    "        output_done = self.output_done(torch.cat([encoded_output, target], axis=2)) # don't need sigmoid because nn.CrossEntropy already does it? (or can use BCE loss after sigmoid)\n",
    "        # output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target], axis=2)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "    \n",
    "    def predict(self, input_tensor, target_tensor):\n",
    "        # separate the input and target tensors\n",
    "        encoded_target = self.target_fc(target_tensor)\n",
    "        encoded_input = self.input_fc(input_tensor)\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target_tensor], axis=1)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "\n",
    "    # transformer training loop\n",
    "    # sequences shape: (batch, sequence, features)\n",
    "    def train_dreamer(self, sequences, epochs, batch_size=256):\n",
    "        print(\"Training Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float).to(DEVICE)\n",
    "        \n",
    "        train_dataset = TensorDataset(inputs)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(train_dataloader):\n",
    "                input_batch = input_batch[0].to(DEVICE)\n",
    "                self.optimizer.zero_grad()\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, running_loss / len(train_dataloader)))\n",
    "\n",
    "    # transformer testing loop\n",
    "    def test_dreamer(self, sequences, batch_size=64):\n",
    "        print(\"Testing Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float).to(DEVICE)\n",
    "        \n",
    "        test_dataset = TensorDataset(inputs)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(test_dataloader):\n",
    "                input_batch = input_batch[0].to(DEVICE)\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                running_loss += loss.item()\n",
    "            print('Test Loss: {:.4f}'.format(running_loss / len(test_dataloader)))\n",
    "        return running_loss / len(test_dataloader)\n",
    "    \n",
    "    def step(self, action):\n",
    "        act = torch.tensor(np.array([action]), dtype=torch.float32).to(DEVICE)\n",
    "        self.actions = torch.cat([self.actions, act], axis=0)\n",
    "\n",
    "        input_sequence = torch.cat([self.states[:-1], self.actions[:-1], self.rewards, self.dones], axis=1).to(torch.float32)\n",
    "        target = torch.cat([self.states[-1], self.actions[-1]], axis=0).unsqueeze(0).to(torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state, reward, done = self.predict(input_sequence, target)\n",
    "            next_state = next_state.to(torch.float32)\n",
    "            reward = reward.to(torch.float32)\n",
    "            done = (done >= 0.6).to(torch.float32) # bias towards not done to avoid false positives\n",
    "\n",
    "            self.states = torch.cat([self.states, next_state], axis=0)\n",
    "            self.rewards = torch.cat([self.rewards, reward], axis=0)\n",
    "            self.dones = torch.cat([self.dones, done], axis=0)\n",
    "            \n",
    "            # trim sequences\n",
    "            if self.states.shape[0] > self.seq_len:\n",
    "                self.states = self.states[1:]\n",
    "                self.rewards = self.rewards[1:]\n",
    "                self.dones = self.dones[1:]\n",
    "            if self.actions.shape[0] > self.seq_len - 1: # action seq shorter\n",
    "                self.actions = self.actions[1:]\n",
    "        \n",
    "        return next_state.squeeze(0).cpu().numpy(), reward.cpu().item(), done.cpu().item(), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actor-critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS\n",
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def quantile_huber_loss(quantiles, samples, sum_over_quantiles=False):\n",
    "    '''\n",
    "    From TQC (see paper p3)\n",
    "    Specific implementation: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/utils.py#L8\n",
    "\n",
    "    Huber loss is less sensitive to outliers than MSE\n",
    "\n",
    "    samples: (batch_size, 1, n_target_quantiles) -> (batch_size, 1, 1, n_target_quantiles)\n",
    "    quantiles: (batch_size, n_critics, n_quantiles) -> (batch_size, n_critics, n_quantiles, 1)\n",
    "    pairwise_delta: (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    '''\n",
    "    # uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise\n",
    "    delta = samples.unsqueeze(1) - quantiles.unsqueeze(3) # (batch_size, 1, 1, n_target_quantiles) - (batch_size, n_critics, n_quantiles, 1)\n",
    "    abs_delta = torch.abs(delta)\n",
    "    huber_loss = torch.where(abs_delta > 1., abs_delta - 0.5, delta ** 2 * 0.5) # 1.0 as threshold k for Huber loss\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "\n",
    "    # cumulative probabilities to calc quantiles\n",
    "    cum_prob = (torch.arange(n_quantiles, device=quantiles.device, dtype=torch.float) + 0.5) / n_quantiles\n",
    "    cum_prob = cum_prob.view(1, 1, -1, 1) # quantiles has shape (batch_size, n_critics, n_quantiles), so make cum_prob broadcastable to (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    \n",
    "    # Calculate quantile loss: |τ - I(δ < 0)| * L_k(δ)\n",
    "    # τ = cum_prob, I(δ < 0) = (delta < 0).float(), L_k(δ) = huber_loss\n",
    "    loss = (torch.abs(cum_prob - (delta < 0).float()) * huber_loss)\n",
    "\n",
    "    # Summing over the quantile dimension \n",
    "    if sum_over_quantiles:\n",
    "        # sum over quantiles\n",
    "        # then average over target quantiles\n",
    "        loss = loss.sum(dim=2).mean(dim=2) # (batch_size, n_critics)\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - remove this\n",
    "# Standard MLP for actor (without D2RL)\n",
    "class StandardActorMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        layer_sizes = [input_dim] + hidden_dims\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]).to(DEVICE))\n",
    "\n",
    "        self.last_layer_mean_linear = nn.Linear(hidden_dims[-1], output_dim).to(DEVICE)\n",
    "        self.last_layer_log_std_linear = nn.Linear(hidden_dims[-1], output_dim).to(DEVICE)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        curr = input_\n",
    "        for layer in self.layers:\n",
    "            curr = F.gelu(layer(curr)) # Or F.relu\n",
    "\n",
    "        mean_linear = self.last_layer_mean_linear(curr)\n",
    "        log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "        return mean_linear, log_std_linear\n",
    "        \n",
    "# Standard MLP for critic (without D2RL)\n",
    "class StandardCriticMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        layer_sizes = [input_dim] + hidden_dims\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]).to(DEVICE))\n",
    "\n",
    "        self.last_layer = nn.Linear(hidden_dims[-1], output_dim).to(DEVICE)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        curr = input_\n",
    "        for layer in self.layers:\n",
    "            curr = F.gelu(layer(curr)) # Or F.relu\n",
    "        output = self.last_layer(curr)\n",
    "        return output\n",
    "\n",
    "\n",
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# MLP for actor that implements D2RL architecture\n",
    "class ActorMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim, output size = action dim\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    current_dim = input_dim # first layer has input dim = state dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim # prev layers output + original input size\n",
    "        \n",
    "    # Final layer input dim is last hidden layer output + original input size\n",
    "    self.last_layer_mean_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "    self.last_layer_log_std_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim=1) # cat with output layer\n",
    "\n",
    "    mean_linear = self.last_layer_mean_linear(curr)\n",
    "    log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "    return mean_linear, log_std_linear\n",
    "\n",
    "# MLP for critic that implements D2RL architecture \n",
    "class CriticMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim + action dim, output size = n_quantiles\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    current_dim = input_dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim\n",
    "\n",
    "    self.last_layer = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim=1)\n",
    "      \n",
    "    output = self.last_layer(curr)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class GradientStep(object):\n",
    "  '''\n",
    "  see (D2RL) https://github.com/pairlab/d2rl/blob/main/sac/sac.py\n",
    "  and (TQC) https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/tqc/tqc.py\n",
    "  '''\n",
    "  def __init__(self,*,\n",
    "    actor, critic, critic_target, discount, tau,\n",
    "    actor_lr, critic_lr, alpha_lr,\n",
    "    n_quantiles, n_mini_critics, top_quantiles_to_drop_per_net, target_entropy,\n",
    "    use_per=True\n",
    "    ):\n",
    "\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.critic_target = critic_target\n",
    "    self.use_per = use_per\n",
    "\n",
    "    self.log_alpha = nn.Parameter(torch.zeros(1).to(DEVICE)) # log alpha is learned\n",
    "    self.quantiles_total = n_quantiles * n_mini_critics\n",
    "    \n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "    self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    self.discount, self.tau = discount, tau\n",
    "    self.top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_mini_critics # total number of quantiles to drop\n",
    "    self.target_entropy = target_entropy\n",
    "\n",
    "  def take_gradient_step(self, replay_buffer, total_steps, batch_size=256):\n",
    "    # Sample batch from replay buffer\n",
    "    state, action, next_state, reward, not_done, indices, weights = replay_buffer.sample(batch_size, total_steps)\n",
    "    weights = torch.tensor(weights, dtype=torch.float32).to(DEVICE).unsqueeze(1) # PER weights. add dim for broadcast\n",
    "    alpha = torch.exp(self.log_alpha) # entropy temperature coefficient\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # sample new action from actor on next state\n",
    "      new_next_action, next_log_pi = self.actor(next_state)\n",
    "\n",
    "      # Compute and cut quantiles at the next state\n",
    "      next_z = self.critic_target(next_state, new_next_action)\n",
    "      \n",
    "      # Sort and drop top k quantiles to control overestimation (TQC)\n",
    "      sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "      sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop] # estimated truncated Q-val dist for next state\n",
    "\n",
    "      # td error + entropy term\n",
    "      target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "    \n",
    "    # Get current quantile estimates using action from the replay buffer\n",
    "    cur_z = self.critic(state, action)\n",
    "    per_critic_loss = quantile_huber_loss(cur_z, target.unsqueeze(1), sum_over_quantiles=True) # keep quantile dim for now\n",
    "    critic_loss = (per_critic_loss * weights).mean() # PER loss\n",
    "\n",
    "    new_action, log_pi = self.actor(state)\n",
    "    # detach the variable from the graph so we don't change it with other losses\n",
    "    alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean() # as in D2RL implementation for auto entropy tuning\n",
    "\n",
    "    # Optimise critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # update PER prios\n",
    "    if self.use_per and indices is not None:\n",
    "      avg_critic_loss = per_critic_loss.mean(1) # average over critics\n",
    "      new_prios = avg_critic_loss.detach().cpu().numpy()\n",
    "      replay_buffer.update_priorities(indices, new_prios)\n",
    "\n",
    "    # Soft update target networks\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Compute actor (π) loss\n",
    "    # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]\n",
    "    actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "    # ^ mean(2) is over quantiles, mean(1) is over critic ensemble\n",
    "    \n",
    "    # Optimise the actor\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # Optimise the entropy coefficient\n",
    "    self.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    self.alpha_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[512, 512], use_d2rl=True):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        if use_d2rl:\n",
    "            self.mlp = ActorMLP(state_dim, hidden_dims, action_dim)\n",
    "        else:\n",
    "            self.mlp = StandardActorMLP(state_dim, hidden_dims, action_dim) # Use standard MLP\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.mlp(obs)\n",
    "        log_std = log_std.clamp(-20, 2) # clamp for stability\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        base_N_dist = D.Normal(mean, std) # base normal dist\n",
    "        tanh_transform = TanhTransform(cache_size=1) # transform to get the tanh dist\n",
    "\n",
    "        log_prob = None\n",
    "        if self.training: # i.e. agent.train()\n",
    "            transformed_dist = D.TransformedDistribution(base_N_dist, tanh_transform) # transformed distribution\n",
    "            action = transformed_dist.rsample() # samples from base dist & applies transform\n",
    "            log_prob = transformed_dist.log_prob(action) # log prob of action after transform\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True) # sum over action dim\n",
    "        else: # evaluation mode\n",
    "            action = torch.tanh(mean)\n",
    "            \n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).to(DEVICE)\n",
    "        if obs.ndim == 1: # add batch dim if missing\n",
    "             obs = obs.unsqueeze(0)\n",
    "        act, _ = self.forward(obs)\n",
    "        return np.array(act[0].cpu().detach())\n",
    "\n",
    "\n",
    "class Critic(nn.Module): # really a mega-critic from lots of mini-critics\n",
    "    '''\n",
    "    Ensemble of critics for TQC\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets, hidden_dims=[256, 256], use_d2rl=True):\n",
    "        super().__init__()\n",
    "        self.critics = nn.ModuleList()\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        for _ in range(n_nets): # multiple critic mlps\n",
    "            if use_d2rl:\n",
    "                net = CriticMLP(state_dim + action_dim, hidden_dims, n_quantiles)\n",
    "            else:\n",
    "                net = StandardCriticMLP(state_dim + action_dim, hidden_dims, n_quantiles) # Use standard MLP\n",
    "            self.critics.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # cat state and action (to pass to critic)\n",
    "        state_act = torch.cat((state, action), dim=1)\n",
    "\n",
    "        # pool quantiles from each critic mlp\n",
    "        quantiles = [critic(state_act) for critic in self.critics]\n",
    "        quantiles = torch.stack(quantiles, dim=1) # stack into tensor\n",
    "        return quantiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My hyperparams\n",
    "\n",
    "seed = 42 # DONT CHANGE FOR COURSEWORK\n",
    "\n",
    "hyperparams = {\n",
    "    # env/general params\n",
    "    \"max_timesteps\": MAX_TIMESTEPS, # per episode [DONT CHANGE]\n",
    "    \"max_episodes\": 100,\n",
    "    \"target_score\": 300, # stop training when average score over r_list > target_score\n",
    "    \"len_r_list\": 100, # length of reward list to average over for target score (stop training when avg > target_score)\n",
    "    \"hardcore\": False, # fixed in wandb sweep\n",
    "    \"init_rand_steps\": 10000, # number of steps to take with random actions before training (helps exploration)\n",
    "\n",
    "    # Agent hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "    \"use_d2rl\": True, # use D2RL architecture for actor/critic TODO remove after ablation\n",
    "    \"n_mini_critics\": 5, # each mini-critic is a single mlp, which combine to make one mega-critic\n",
    "    \"n_quantiles\": 20, # quantiles per mini critic\n",
    "    \"top_quantiles_to_drop_per_net\": 'auto', # per mini critic (auto based on n_quantiles)\n",
    "    \"actor_hidden_dims\": [512, 512],\n",
    "    \"mini_critic_hidden_dims\": [256, 256], # * n_mini_critics\n",
    "    \"batch_size\": 256,\n",
    "    \"discount\": 0.98, # gamma\n",
    "    \"tau\": 0.005,\n",
    "    \"actor_lr\": 3.29e-4, # empirically chosen\n",
    "    \"critic_lr\": 3.8e-4, # empirically chosen\n",
    "    \"alpha_lr\": 3.24e-4, # empirically chosen\n",
    "\n",
    "    # ERE buffer (see paper for their choices)\n",
    "    \"use_per\": False, # use PER sampling as well (DO NOT USE ON MAC IT BREAKS AHHH)\n",
    "    \"buffer_size\": 100000, # smaller size improves learning early on but is outperformed later on\n",
    "    \"eta0\": 0.996, # 0.994 - 0.999 is good (according to paper)\n",
    "    \"annealing_steps\": 'auto', # number of steps to anneal eta over (after which sampling is uniform) - None = auto-set to max estimated steps in training\n",
    "    \"cmin\": 5000, # min number of samples to sample from\n",
    "    \"recency_scale\": 1, # scale factor for recency\n",
    "\n",
    "    # dreamer hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "    # info on hyperparam choice: https://arxiv.org/abs/1912.01603\n",
    "    \"use_dreamer\": False,\n",
    "    \"intrinsic_reward_scale\": 0., # scale factor for dreamer intrinsic reward\n",
    "    \"batch_size_dreamer\": 512,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 4,\n",
    "    \"num_heads\": 8,\n",
    "    \"dreamer_lr\": 3e-4,\n",
    "    \"dreamer_weight_decay\": 1e-4,\n",
    "    \"dropout_prob\": 0.1,\n",
    "    \"window_size\": 40,               # transformer context window size\n",
    "    \"step_size\": 1,                  # how many timesteps to skip between each context window\n",
    "    \"train_split\": 0.8,              # train/validation split\n",
    "    \"loss_threshold\": 10,           # use dreamer if loss < loss_threshold\n",
    "    \"imagination_horizon\": 15,       # how many timesteps to run the dreamer model for (H in Dreamer paper)\n",
    "    \"dreamer_train_epochs\": 10,      # how many epochs to train the dreamer model for\n",
    "    \"dreamer_train_frequency\": 10,   # how often to train the dreamer model\n",
    "    \"episode_threshold\": 20,         # how many episodes to run before training the dreamer model\n",
    "    \"max_size\": 50000,               # maximum size of the training set for the dreamer model\n",
    "}\n",
    "\n",
    "# recording/logging\n",
    "plot_interval = 100 # plot every Nth episode (wandb still plots every ep)\n",
    "save_fig = True # save figures too\n",
    "\n",
    "is_recording = True # TODO dont forget this\n",
    "if hyperparams['hardcore']: # NOTE this doesnt change in wandb sweep\n",
    "    video_interval = 30 # record every Nth episode\n",
    "    ep_start_rec = 500 # start recording on this episode\n",
    "else:\n",
    "    video_interval = 20\n",
    "    ep_start_rec = 50\n",
    "\n",
    "if hyperparams['annealing_steps'] == 'auto':\n",
    "    hyperparams['annealing_steps'] = hyperparams['max_episodes']*hyperparams['max_timesteps'] # max est number of steps in training\n",
    "if hyperparams['top_quantiles_to_drop_per_net'] == 'auto':\n",
    "    hyperparams['top_quantiles_to_drop_per_net'] = int(hyperparams['n_quantiles'] // 12.5) # keep ratio same as M=25 d=2 (from TQC paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtheo-farrell99\u001b[0m (\u001b[33mtheo-farrell99-durham-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/wandb/run-20250505_190219-zfb70qiq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/zfb70qiq' target=\"_blank\">_2025-05-05_19-02-18</a></strong> to <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d' target=\"_blank\">https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/zfb70qiq' target=\"_blank\">https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/zfb70qiq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SETUP\n",
    "\n",
    "# wandb & (potential) checkpoints\n",
    "\n",
    "# load checkpoint here if using\n",
    "wandb_run_id = None\n",
    "start_episode = 1 # Default start episode\n",
    "\n",
    "# init wandb run\n",
    "import datetime\n",
    "suffix = '_hardcore' if hyperparams['hardcore'] else ''\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "run_name = SOURCE_ID + f\"_{timestamp}\" + suffix\n",
    "\n",
    "wandb_init_kwargs = {\n",
    "    \"project\": \"RL-Coursework-Walker2d\",\n",
    "    \"config\": hyperparams,\n",
    "    \"name\": run_name,\n",
    "    \"save_code\": True, # optionally saves code to wandb\n",
    "}\n",
    "if wandb_run_id:\n",
    "    wandb_init_kwargs[\"id\"] = wandb_run_id\n",
    "    wandb_init_kwargs[\"resume\"] = \"allow\" # Or \"must\" if you require resuming\n",
    "\n",
    "wandb.init(**wandb_init_kwargs)\n",
    "\n",
    "config = wandb.config # use wandb.config to access hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1Xrcek4hxDXl",
    "outputId": "6161f148-a164-4e22-d97a-f9aa53e7b8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.3781445e-03, -2.0011244e-05,  1.4475726e-03, -1.5999975e-02,\n",
       "         9.8153561e-02, -1.2663029e-03,  8.5725147e-01,  2.0965904e-03,\n",
       "         1.0000000e+00,  3.9205227e-02, -1.2662878e-03,  8.5045713e-01,\n",
       "         1.0122511e-03,  1.0000000e+00,  4.3961507e-01,  4.4460756e-01,\n",
       "         4.6016780e-01,  4.8821869e-01,  5.3265011e-01,  6.0082245e-01,\n",
       "         7.0722014e-01,  8.8352221e-01,  1.0000000e+00,  1.0000000e+00],\n",
       "       dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make env\n",
    "# only attempt hardcore when your agent has solved the non-hardcore version\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=config.hardcore)\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "video_prefix = \"nchw73-agent-hardcore-video\" if config.hardcore else \"nchw73-agent-video\"\n",
    "video_prefix = f\"{timestamp}_\" + video_prefix # mark video with date/time to be mateched to run (remove later!)\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=video_prefix,          # prefix for videos\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "rld.check_device() # training on CPU recommended\n",
    "\n",
    "env.video = False # switch video recording off (only switch on every x episodes as this is slow)\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, state_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=seed)\n",
    "# rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWjENKkU_My"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, state, info = rld.seed_everything(seed, env)\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "replay_buffer = EREReplayBuffer(\n",
    "    state_dim, act_dim, config.annealing_steps, config.buffer_size, config.eta0, config.cmin,\n",
    "    use_per=config.use_per, recency_scale=config.recency_scale,\n",
    "    )\n",
    "\n",
    "actor = Actor(\n",
    "    state_dim, act_dim, config.actor_hidden_dims, use_d2rl=config.use_d2rl\n",
    "    ).to(DEVICE)\n",
    "\n",
    "critic = Critic(\n",
    "    state_dim, act_dim, config.n_quantiles, config.n_mini_critics,\n",
    "    config.mini_critic_hidden_dims, use_d2rl=config.use_d2rl\n",
    "    ).to(DEVICE)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "dreamer = None # init as none\n",
    "if config.use_dreamer:\n",
    "    dreamer = DreamerAgent(\n",
    "        state_dim, act_dim, config.hidden_dim, \n",
    "        config.window_size, config.num_layers, config.num_heads, config.dropout_prob,\n",
    "        config.dreamer_lr, config.dreamer_weight_decay,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "target_entropy = -np.prod(env.action_space.shape).item() # target entropy heuristic = −dim(A)\n",
    "grad_step_class = GradientStep(\n",
    "    actor=actor, critic=critic, critic_target=critic_target,\n",
    "    discount=config.discount, tau=config.tau,\n",
    "    actor_lr=config.actor_lr, critic_lr=config.critic_lr, alpha_lr=config.alpha_lr,\n",
    "    n_quantiles=config.n_quantiles, n_mini_critics=config.n_mini_critics,\n",
    "    top_quantiles_to_drop_per_net=config.top_quantiles_to_drop_per_net,\n",
    "    target_entropy=target_entropy, use_per=config.use_per,\n",
    "    )\n",
    "\n",
    "actor.train()\n",
    "\n",
    "total_steps = 0\n",
    "memory_ptr = 0 # marks boundary between new and old data in the replay buffer\n",
    "train_set, test_set = None, None\n",
    "recent_rewards= []\n",
    "ep_timesteps_dreamer = ep_reward_dreamer = dreamer_eps = 0\n",
    "last_dreamer_test_loss = 0.0 # track dreamer test loss\n",
    "completed_env = False\n",
    "\n",
    "episode = start_episode # =1 by default or whatever was in a checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 1 | Timesteps: 69 | Reward: -116.031 | Total Steps: 69 | Dreamer: False\n",
      "Ep: 2 | Timesteps: 130 | Reward: -100.340 | Total Steps: 2e+02 | Dreamer: False\n",
      "Ep: 3 | Timesteps: 2000 | Reward: -61.315 | Total Steps: 2.2e+03 | Dreamer: False\n",
      "Ep: 4 | Timesteps: 95 | Reward: -97.463 | Total Steps: 2.3e+03 | Dreamer: False\n",
      "Ep: 5 | Timesteps: 2000 | Reward: -74.790 | Total Steps: 4.3e+03 | Dreamer: False\n",
      "Ep: 6 | Timesteps: 2000 | Reward: -70.298 | Total Steps: 6.3e+03 | Dreamer: False\n",
      "Ep: 7 | Timesteps: 2000 | Reward: -66.159 | Total Steps: 8.3e+03 | Dreamer: False\n",
      "Ep: 8 | Timesteps: 110 | Reward: -98.922 | Total Steps: 8.4e+03 | Dreamer: False\n",
      "torch.Size([256, 256, 95])\n",
      "weights shape: torch.Size([256, 1])\n",
      "per critic loss shape: torch.Size([256, 256, 95])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m# float32 for mps\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# sample real env\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m ep_timesteps, ep_reward, input_buffer, info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_step_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_rand_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use intrinsic reward if dreamer active\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_intrinsic_reward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_dreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_rand_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_dreamer_test_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dreamer_test_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintrinsic_r_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintrinsic_reward_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_timesteps\n\u001b[1;32m     26\u001b[0m tracker\u001b[38;5;241m.\u001b[39mtrack(info) \u001b[38;5;66;03m# track statistics for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 76\u001b[0m, in \u001b[0;36mtrain_on_environment\u001b[0;34m(actor, env, grad_step_class, replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length, init_rand_steps, is_dreamer, use_intrinsic_reward, last_dreamer_test_loss, intrinsic_r_scale)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# train step if buffer has enough samples for a batch AND we're past warmup\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;129;01mand\u001b[39;00m init_rand_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m init_rand_steps:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# train the agent using experiences from the real environment\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mgrad_step_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;66;03m# break if finished\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 64\u001b[0m, in \u001b[0;36mGradientStep.take_gradient_step\u001b[0;34m(self, replay_buffer, total_steps, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Optimise critic\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 64\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# update PER prios\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training based on https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# training loop\n",
    "while episode <= config.max_episodes: # index from 1\n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    if is_recording and episode >= ep_start_rec:\n",
    "        env.info = episode % video_interval == 0   # track every x episodes (usually tracking every episode is fine)\n",
    "        env.video = episode % video_interval == 0  # record videos every x episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    state, info = env.reset()\n",
    "    state = state.astype(np.float32) # float32 for mps\n",
    "\n",
    "    # sample real env\n",
    "    ep_timesteps, ep_reward, input_buffer, info = train_on_environment(\n",
    "        actor, env, grad_step_class, replay_buffer, config.max_timesteps, state,\n",
    "        config.batch_size, total_steps, config.window_size, config.init_rand_steps,\n",
    "        is_dreamer=False,\n",
    "        # use intrinsic reward if dreamer active\n",
    "        use_intrinsic_reward=(config.use_dreamer and episode > config.episode_threshold and total_steps >= config.init_rand_steps),\n",
    "        last_dreamer_test_loss=last_dreamer_test_loss,\n",
    "        intrinsic_r_scale=config.intrinsic_reward_scale,\n",
    "        )\n",
    "\n",
    "    total_steps += ep_timesteps\n",
    "    tracker.track(info) # track statistics for plotting\n",
    "\n",
    "    # update train/test sets for dreamer (add new eps and trim to window size)\n",
    "    train_set, test_set = gen_test_train_seq(\n",
    "        replay_buffer, train_set, test_set, config.train_split, config.window_size, config.step_size, memory_ptr)\n",
    "\n",
    "    # train dreamer after ep_thresh and if we have train/test sets\n",
    "    if config.use_dreamer and episode >= config.episode_threshold and train_set is not None:\n",
    "        # train and assess the dreamer every train_frequency\n",
    "        if episode % config.dreamer_train_frequency == 0:\n",
    "            dreamer.train_dreamer(train_set, config.dreamer_train_epochs, config.batch_size_dreamer)\n",
    "\n",
    "        # truncate the training set to control train time performance\n",
    "        if train_set.shape[0] > config.max_size:\n",
    "            train_set = train_set[-config.max_size:]\n",
    "\n",
    "        if test_set.shape[0] > config.max_size//4: # cap test set to 25% train set\n",
    "            test_set = test_set[-(config.max_size//4):]\n",
    "\n",
    "        print(f'Size of train set: {train_set.shape[0]}, test set: {test_set.shape[0]}')\n",
    "        \n",
    "        # evaluate the dreamer's performance & decide num of training steps for dreamer\n",
    "        dreamer_avg_loss = dreamer.test_dreamer(test_set, config.batch_size_dreamer)\n",
    "        dreamer_eps = get_dreamer_eps(dreamer_avg_loss, config.loss_threshold)\n",
    "        last_dreamer_test_loss = dreamer_avg_loss\n",
    "\n",
    "        # train on dreamer if its accurate enough\n",
    "        H = config.imagination_horizon # H = imagination horizon\n",
    "        if dreamer_eps > 0:\n",
    "            # if input_buffer is None or input_buffer.shape[0] != config.window_size:\n",
    "                # input buffer not big enough to run dreamer\n",
    "                # print('Dreamer not run: input buffer not big enough')\n",
    "            if state is None or not isinstance(state, np.ndarray): # TODO\n",
    "                print(\"Warning: Invalid initial state for Dreamer imagination. Skipping.\")\n",
    "                dreamer_eps = 0\n",
    "            else:\n",
    "                dreamer_start = state.astype(np.float32)\n",
    "                dreamer.reset(dreamer_start) # reset dreamer with the current state\n",
    "\n",
    "                # scale H based on dreamer loss: 1 if loss = 0 and 0 if loss >= thresh \n",
    "                H_scale_factor = max(0., 1. - (dreamer_avg_loss / config.loss_threshold))\n",
    "                H = int(H * H_scale_factor ** 2) # quadratic scaling to encourage better models\n",
    "\n",
    "                print(f'Dreamer active for {dreamer_eps} iterations, with {H} timesteps per ep.')\n",
    "                ep_timesteps_dreamer = ep_reward_dreamer = 0\n",
    "                for dep in range(dreamer_eps):\n",
    "                    # print(f'Dreamer ep: {dep+1}')\n",
    "\n",
    "                    # initialise dreamer states with the input sequence\n",
    "                    # input buffer = state, action, next_state, reward, done\n",
    "                    # dreamer.states = input_buffer[:, :state_dim]\n",
    "                    # dreamer.actions = input_buffer[:-1, state_dim:state_dim+act_dim]\n",
    "                    # dreamer.rewards = input_buffer[:-1, -2].unsqueeze(1)\n",
    "                    # dreamer.dones = input_buffer[:-1, -1].unsqueeze(1)\n",
    "\n",
    "                    dreamer.reset(dreamer_start) # TODO alternatively could resample from reply buffer\n",
    "\n",
    "                    # sample from dreamer environment (ignore input buffer and info)\n",
    "                    _td, _rd, _, _ = train_on_environment(\n",
    "                        actor=actor, env=dreamer, grad_step_class=grad_step_class,\n",
    "                        replay_buffer=replay_buffer, max_timesteps=H,\n",
    "                        state=dreamer.current_state, # initial state for dreamer TODO\n",
    "                        batch_size=config.batch_size, total_steps=total_steps,\n",
    "                        sequence_length=config.window_size, init_rand_steps=None, # No random steps during imagination\n",
    "                        is_dreamer=True, use_intrinsic_reward=False\n",
    "                        )\n",
    "\n",
    "                    ep_timesteps_dreamer += _td\n",
    "                    ep_reward_dreamer += _rd\n",
    "\n",
    "    memory_ptr = replay_buffer.ptr # update the memory pointer to the current position in the buffer\n",
    "\n",
    "    print(f\"Ep: {episode} | Timesteps: {ep_timesteps} | Reward: {ep_reward:.3f} | Total Steps: {total_steps:.2g} | Dreamer: {dreamer_eps > 0}\")\n",
    "    if config.use_dreamer and dreamer_eps > 0:\n",
    "        print(f\"\\t Dreamer Eps: {dreamer_eps} | Dreamer Avg Reward: {ep_reward_dreamer/dreamer_eps:.3f} | Dreamer Avg Timesteps: {ep_timesteps_dreamer/dreamer_eps:.3g}\")\n",
    "\n",
    "    # plot tracked statistics (on real env)\n",
    "    if episode % plot_interval == 0 or completed_env or episode == config.max_episodes: # always save last plot\n",
    "        # save as well (show=False returns it but doesnt display)\n",
    "        if save_fig and info: # check info isnt {} (i.e. not None)\n",
    "            fig, _ = tracker.plot(show=False, r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "            wandb.log({\"Coursework Plot\": wandb.Image(fig)}, step=episode) # Log the figure directly to W&B\n",
    "            # fig.savefig(f'./tracker_{run_name}.png', bbox_inches = 'tight')\n",
    "            plt.close(fig) # Close the figure to free memory\n",
    "        # show by default\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    log_dict = {\n",
    "        \"Total Steps\": total_steps,\n",
    "        \"Episode Timesteps\": ep_timesteps,\n",
    "    }\n",
    "\n",
    "    # log tracked statistics\n",
    "    if 'recorder' in info: # if we have info available\n",
    "        log_dict[\"TrackedInfo/r_mean_\"] = info['recorder']['r_mean_'] # r is extrinsic only\n",
    "        log_dict[\"TrackedInfo/r_std_\"] = info['recorder']['r_std_']\n",
    "        log_dict[\"TrackedInfo/r_sum\"] = info['recorder']['r_sum']\n",
    "\n",
    "    if config.use_dreamer:\n",
    "        log_dict[\"Dreamer Average Loss\"] = dreamer_avg_loss if 'dreamer_avg_loss' in locals() else None # Only log if calculated\n",
    "        log_dict[\"Dreamer Episodes Run\"] = dreamer_eps\n",
    "        if dreamer_eps > 0:\n",
    "            log_dict[\"Dreamer Average Reward\"] = ep_reward_dreamer / dreamer_eps\n",
    "            log_dict[\"Dreamer Average Timesteps\"] = ep_timesteps_dreamer / dreamer_eps\n",
    "\n",
    "    recent_rewards.append(ep_reward)\n",
    "    current_avg = np.array(recent_rewards).mean()\n",
    "    log_dict[\"R_mean_100\"] = current_avg\n",
    "\n",
    "    wandb.log(log_dict, step=episode) # log metrics each ep\n",
    "\n",
    "    # env completion (break) condition - stop if we consistently meet target score\n",
    "    if len(recent_rewards) >= config.len_r_list:\n",
    "        print(f'Current progress: {current_avg:.3f} / {config.target_score}')\n",
    "        if current_avg >= config.target_score: # quit when we've got good enough performance\n",
    "            print(f\"Completed environment in {episode} episodes!\")\n",
    "            break\n",
    "        recent_rewards = recent_rewards[-config.len_r_list+1:] # discard oldest on list (keep most recent 99)        \n",
    "    \n",
    "    \n",
    "    episode += 1\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file\n",
    "if config.hardcore:\n",
    "    filename = f\"[{run_name}]_\" + \"nchw73-agent-hardcore-log.txt\"\n",
    "else:\n",
    "    filename = f\"[{run_name}]_\" + \"nchw73-agent-log.txt\"\n",
    "env.write_log(folder=\"logs\", file=filename)\n",
    "\n",
    "wandb.finish() # finish wandb run\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
