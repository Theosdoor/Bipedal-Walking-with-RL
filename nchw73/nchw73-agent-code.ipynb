{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "# **Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSUGAJsTLOV",
    "outputId": "875609e2-e3eb-4b29-c03c-1989820b1ea7"
   },
   "outputs": [],
   "source": [
    "# https://github.com/robert-lieck/rldurham/tree/main - rldurham source\n",
    "\n",
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham\n",
    "\n",
    "# !pip install wandb\n",
    "# !pip install torchinfo\n",
    "# !pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AFrqd24JTLOW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtheo-farrell99\u001b[0m (\u001b[33mtheo-farrell99-durham-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import os\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"x\" # TODO remove - BE CAREFUL DONT PUT ON GITHUB!!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.distributions as D\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "\n",
    "import rldurham as rld\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "# **RL agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TIMESTEPS = 2000 # [DONT CHANGE]\n",
    "SOURCE_ID = 'mac' # mac, ncc, colab -> for personal id in wandb\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERs\n",
    "# for https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def train_on_environment(actor, env, grad_step_class, replay_buffer, max_timesteps,\n",
    "    state, batch_size, total_steps, sequence_length):\n",
    "    '''\n",
    "    abstracted training loop function for both real & dreamer environments to use\n",
    "\n",
    "    the real environment will always have MAX_TIMESTEPS as the max_timesteps value,\n",
    "    while the dreamer enviroment might have less timesteps TODO could it also have MAX_TIMESTEPS?\n",
    "    '''\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "    # save start sequence for the dreamer model TODO - where does 54 come from?\n",
    "    input_buffer = torch.empty((0, 54)).to(DEVICE)\n",
    "\n",
    "    for t in range(max_timesteps): \n",
    "        total_steps += 1\n",
    "        # select action and step the env\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "\n",
    "        # handle different return formats\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, info = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, info = step\n",
    "\n",
    "        ep_timesteps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        if max_timesteps == MAX_TIMESTEPS: # could we be in dreamer for this?\n",
    "            # reward shaping\n",
    "            if reward == -100.0:\n",
    "                reward = -10.0 # bipedal walker does -100 for hitting floor, so -10 might make it more stable\n",
    "            else:\n",
    "                reward *= 2 # TODO why might this be justified? (encouraging forward motion?)\n",
    "            replay_buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "        else:\n",
    "            if t == sequence_length:\n",
    "                for row in input_buffer.cpu().numpy():\n",
    "                    replay_buffer.add(row[:24], row[24:28], row[28:52], row[52], row[53])\n",
    "                    # ^ magin numbers are from state_dim, state_dim + action_dim, state_dim + action_dim + reward_dim, done\n",
    "            elif t > sequence_length:\n",
    "                replay_buffer.add(state, action, next_state, reward, done)\n",
    "            # don't store if simulation ends without reaching the sequence length\n",
    "        \n",
    "        if t < sequence_length: # store in input buffer TODO what does this do? why's it needed?\n",
    "            combined = torch.tensor(np.concatenate((state, action, next_state, np.array([reward]), np.array([done])), axis=0)).unsqueeze(0).to(DEVICE)\n",
    "            input_buffer = torch.cat([input_buffer, combined], axis=0)\n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "        if total_steps >= batch_size: # do 1 training update using 1 batch from buffer if enough\n",
    "            # train the agent using experiences from the real environment\n",
    "            grad_step_class.take_gradient_step(replay_buffer, total_steps, batch_size)\n",
    "    \n",
    "        if done: # break if finished\n",
    "            break\n",
    "\n",
    "    if sequence_length > 0: # TODO - this means...? \n",
    "        return ep_timesteps, ep_reward, input_buffer, info\n",
    "    return ep_timesteps, ep_reward, None, info\n",
    "\n",
    "\n",
    "# test loop for agent on environment\n",
    "def simulate_on_environment(actor, env, max_timesteps, state):\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "    for t in range(max_timesteps):\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, _ = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, _ = step\n",
    "        ep_timesteps += 1\n",
    "    \n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "    \n",
    "        if done or t == max_timesteps - 1:\n",
    "            break\n",
    "    return ep_timesteps, ep_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# generate value array from replay buffer\n",
    "def retrieve_from_replay_buffer(replay_buffer, ptr):\n",
    "    '''\n",
    "    returns (state, action, reward, done, next_state)\n",
    "    '''\n",
    "    return np.concatenate((\n",
    "            replay_buffer.state[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.action[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.reward[ptr:replay_buffer.ptr],\n",
    "            1. - replay_buffer.not_done[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.next_state[ptr:replay_buffer.ptr],                \n",
    "        ), \n",
    "        axis = 1 # along the columns (so each row is a memory)\n",
    "    )\n",
    "\n",
    "# function to create sequences with a given window size and step size\n",
    "def create_sequences(memories, window_size, step_size):\n",
    "    '''\n",
    "    Function to create sequences of memories from the replay buffer.\n",
    "\n",
    "    Each sequence is of length window_size and each spaced apart by step_size\n",
    "    '''\n",
    "    n_memories = memories.shape[0] # just the number of time steps currently in the buffer\n",
    "\n",
    "    # calc number of seq (of len window_size) we can create from mems available  \n",
    "    n_sequences = math.floor((n_memories - window_size) / step_size) + 1 # +1 because indices start at 0\n",
    "\n",
    "    sequences = np.zeros((n_sequences, window_size, memories.shape[1]))\n",
    "    for i in range(n_sequences):\n",
    "        start_idx = i * step_size # idx to start seq from\n",
    "        sequences[i, :] = memories[start_idx:start_idx + window_size, :] # grab seq of memories window_size long from start_idx\n",
    "    return sequences\n",
    "\n",
    "def gen_test_train_seq(replay_buffer, train_set, test_set, train_split, window_size, step_size, ptr):\n",
    "    '''\n",
    "    Function to split train and test data\n",
    "    '''\n",
    "    memories = retrieve_from_replay_buffer(replay_buffer, ptr)\n",
    "    try:\n",
    "        memory_sequences = create_sequences(memories, window_size, step_size)\n",
    "        n_sequences = memory_sequences.shape[0]\n",
    "    except: # TODO How might this go wrong? not enough new data to create a sequence?\n",
    "        return train_set, test_set\n",
    "\n",
    "    # shuffle the sequences & split\n",
    "    indices = np.arange(n_sequences)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(train_split * n_sequences) # get split point \n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "\n",
    "    if train_set is None: # if this is first train/test set, create the sets\n",
    "        return memory_sequences[train_indices, :], memory_sequences[test_indices, :]\n",
    "    # else just add to existing sets\n",
    "    return np.concatenate((train_set, memory_sequences[train_indices, :]), axis=0), np.concatenate((test_set, memory_sequences[test_indices, :]), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dreamer_eps(dreamer_avg_loss, score_threshold):\n",
    "    '''\n",
    "    Get number of dreamed episdoes the agent should run on the dreamer model.\n",
    "\n",
    "    Only use dreamer for training if it's sufficiently accurate model of env.\n",
    "\n",
    "    Note - score_threshold is between 0 and 1.\n",
    "\n",
    "    Disclaimer - calculation from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    '''\n",
    "    max_dreamer_it = 10 # a perfect dreamer has this many eps\n",
    "\n",
    "    if dreamer_avg_loss >= score_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        norm_score = dreamer_avg_loss/score_threshold # normalise score relative to threshold\n",
    "        inv_score = 1 - norm_score # invert so that closer to 1 ==> dreamer score much better than threshold\n",
    "        sq_score = inv_score**2 # square so that num iter increases quadratically as accuracy improves\n",
    "        return int(max_dreamer_it * sq_score) # scale so that max iterations is 10 (when dreamer very accurate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9kTeUHYL7gU6"
   },
   "outputs": [],
   "source": [
    "class EREReplayBuffer(object):\n",
    "    '''\n",
    "    implementation - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    An ERE buffer is implemented to improve sample efficiency, \n",
    "        for which the paper can be found here: https://arxiv.org/abs/1906.04009\n",
    "\n",
    "    Rationale:\n",
    "    - actions from early in training are likely v different to optimal, so want to prioritise recent ones so we're not constantly learning from crap ones\n",
    "    - as time goes on, they get more similar so annealing eta allows uniform sampling later on\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, T, max_size, eta, cmin):\n",
    "        self.max_size, self.ptr, self.size, self.rollover = max_size, 0, 0, False\n",
    "        self.eta0 = eta\n",
    "        self.cmin = cmin\n",
    "        self.c_list = []\n",
    "        self.index = []\n",
    "        self.T = T\n",
    "\n",
    "        self.reward = np.empty((max_size, 1))\n",
    "        self.state = np.empty((max_size, state_dim))\n",
    "        self.action = np.empty((max_size, action_dim))\n",
    "        self.not_done = np.empty((max_size, 1))\n",
    "        self.next_state = np.empty((max_size, state_dim))\n",
    "        \n",
    "    def sample(self, batch_size, t):\n",
    "        # update eta value for current timestep\n",
    "        eta = self.eta_anneal(t)\n",
    "\n",
    "        index = np.array([self._get_index(eta, k, batch_size) for k in range(batch_size)])\n",
    "\n",
    "        r = torch.tensor(self.reward[index], dtype = torch.float).to(DEVICE)\n",
    "        s = torch.tensor(self.state[index], dtype = torch.float).to(DEVICE)\n",
    "        ns = torch.tensor(self.next_state[index], dtype = torch.float).to(DEVICE)\n",
    "        a = torch.tensor(self.action[index], dtype = torch.float).to(DEVICE)\n",
    "        nd = torch.tensor(self.not_done[index], dtype = torch.float).to(DEVICE)\n",
    "        \n",
    "        return s, a, ns, r, nd\n",
    "    \n",
    "    def _get_index(self, eta, k, batch_size):\n",
    "        c_calc = self.size * eta ** (k * 1000 / batch_size)\n",
    "        ck = c_calc if c_calc > self.cmin else self.size\n",
    "\n",
    "        if not self.rollover: # if we're not overwriting yet, ...\n",
    "            return np.random.randint(self.size - ck, self.size)\n",
    "        \n",
    "        return np.random.randint(self.ptr + self.size - ck, self.ptr + self.size) % self.size\n",
    "\n",
    "    def eta_anneal(self, t):\n",
    "        # eta anneals over time --> 1, which reduces emphasis on recent experiences over time\n",
    "        return min(1.0, self.eta0 + (1 - self.eta0) * t / self.T)\n",
    " \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        # Add experience to replay buffer \n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.ptr %= self.max_size\n",
    "\n",
    "        # increase size counter until full, then start overwriting (rollover)\n",
    "        if self.max_size > self.size + 1:\n",
    "          self.size += 1\n",
    "        else:\n",
    "          self.size = self.max_size\n",
    "          self.rollover = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dreamer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamerAgent(nn.Module):\n",
    "    '''\n",
    "    Dreamer agent.\n",
    "    Uses a transformer model to predict the next state, reward and done signal given the current (state, action, reward, done)\n",
    "\n",
    "    Acknowledgements:\n",
    "    - implementation based on https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    - (from implementation designer) key ideas and concepts for the auto-regressive transformer design stem from this paper: https://arxiv.org/abs/2202.09481\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, seq_len, num_layers, num_heads, dropout_prob, lr):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.input_dim = state_dim + action_dim + 2 # (state, action, reward, done)\n",
    "        self.target_dim = self.state_dim + self.action_dim # (state, action)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.input_fc = nn.Linear(self.input_dim, hidden_dim).to(DEVICE)\n",
    "        self.target_fc = nn.Linear(self.target_dim, hidden_dim).to(DEVICE)\n",
    "        self.transformer = nn.Transformer( # uses transformer model!\n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            num_heads, \n",
    "            dropout=dropout_prob,\n",
    "            activation=F.gelu,\n",
    "            batch_first=True\n",
    "        ).to(DEVICE)\n",
    "        self.output_next_state = nn.Linear(hidden_dim + self.target_dim, state_dim).to(DEVICE)\n",
    "        self.output_reward = nn.Linear(hidden_dim + self.target_dim, 1).to(DEVICE)\n",
    "        self.output_done = nn.Linear(hidden_dim + self.target_dim, 1).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    # separate out the ground truth variables and compare against predictions\n",
    "    def loss_fn(self, output_next_state, output_reward, output_done, ground_truth):\n",
    "        reward, done, next_state = torch.split(ground_truth, [1, 1, self.state_dim], dim=-1)\n",
    "        loss = self.mse_loss(output_next_state[:, -1], next_state)\n",
    "        loss += self.mse_loss(output_reward[:, -1], reward)\n",
    "        # loss += self.ce_loss(output_done[:, -1], done)\n",
    "        loss += self.bce_loss(output_done[:, -1], done.float())\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # separate the input and target tensors\n",
    "        target = input_tensor[:, -1, :self.target_dim].unsqueeze(1)\n",
    "        encoded_target = self.target_fc(target)\n",
    "        encoded_input = self.input_fc(input_tensor[:, :-1, :self.input_dim])\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target], axis=2))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target], axis=2))\n",
    "        output_done = self.output_done(torch.cat([encoded_output, target], axis=2)) # don't need sigmoid because nn.CrossEntropy already does it? (or can use BCE loss after sigmoid)\n",
    "        # output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target], axis=2)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "    \n",
    "    def predict(self, input_tensor, target_tensor):\n",
    "        # separate the input and target tensors\n",
    "        encoded_target = self.target_fc(target_tensor)\n",
    "        encoded_input = self.input_fc(input_tensor)\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target_tensor], axis=1)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "\n",
    "    # transformer training loop\n",
    "    # sequences shape: (batch, sequence, features)\n",
    "    def train_dreamer(self, sequences, epochs, batch_size=256):\n",
    "        print(\"Training Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float).to(DEVICE)\n",
    "        \n",
    "        train_dataset = TensorDataset(inputs)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(train_dataloader):\n",
    "                input_batch = input_batch[0].to(DEVICE)\n",
    "                self.optimizer.zero_grad()\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, running_loss / len(train_dataloader)))\n",
    "\n",
    "    # transformer testing loop\n",
    "    def test_dreamer(self, sequences, batch_size=64):\n",
    "        print(\"Testing Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float).to(DEVICE)\n",
    "        \n",
    "        test_dataset = TensorDataset(inputs)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(test_dataloader):\n",
    "                input_batch = input_batch[0].to(DEVICE)\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                running_loss += loss.item()\n",
    "            print('Test Loss: {:.4f}'.format(running_loss / len(test_dataloader)))\n",
    "        return running_loss / len(test_dataloader)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.actions = torch.cat([self.actions, torch.tensor(np.array([action])).to(DEVICE)], axis=0)\n",
    "        input_sequence = torch.cat([self.states[:-1], self.actions[:-1], self.rewards, self.dones], axis=1).to(torch.float32)\n",
    "        target = torch.cat([self.states[-1], self.actions[-1]], axis=0).unsqueeze(0).to(torch.float32)\n",
    "        with torch.no_grad():\n",
    "            next_state, reward, done = self.predict(input_sequence, target)\n",
    "            done = (done >= 0.6).float() # bias towards not done to avoid false positives\n",
    "\n",
    "            self.states = torch.cat([self.states, next_state], axis=0)\n",
    "            self.rewards = torch.cat([self.rewards, reward], axis=0)\n",
    "            self.dones = torch.cat([self.dones, done], axis=0)\n",
    "            \n",
    "            if self.states.shape[0] > self.seq_len:\n",
    "                self.states = self.states[1:]\n",
    "                self.rewards = self.rewards[1:]\n",
    "                self.dones = self.dones[1:]\n",
    "            if self.actions.shape[0] > self.seq_len - 1:\n",
    "                self.actions = self.actions[1:]\n",
    "        \n",
    "        return next_state.squeeze(0).cpu().numpy(), reward.cpu().item(), done.cpu().item(), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actor-critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS\n",
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def quantile_huber_loss(quantiles, samples, sum_over_quantiles = False):\n",
    "    '''\n",
    "    From TQC (see paper p3)\n",
    "    Specific implementation: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/utils.py#L8\n",
    "\n",
    "    Huber loss is less sensitive to outliers than MSE\n",
    "\n",
    "    samples: (batch_size, 1, n_target_quantiles) -> (batch_size, 1, 1, n_target_quantiles)\n",
    "    quantiles: (batch_size, n_critics, n_quantiles) -> (batch_size, n_critics, n_quantiles, 1)\n",
    "    pairwise_delta: (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    '''\n",
    "    # uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise\n",
    "    delta = samples[:, np.newaxis, np.newaxis, :] - quantiles[:, :, :, np.newaxis]  \n",
    "    abs_delta = torch.abs(delta)\n",
    "    huber_loss = torch.where(abs_delta > 1, abs_delta - 0.5, delta ** 2 * 0.5)\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "\n",
    "    # cumulative probabilities to calc quantiles\n",
    "    cum_prob = (torch.arange(n_quantiles, device=quantiles.device, dtype=torch.float) + 0.5) / n_quantiles\n",
    "    cum_prob = cum_prob.view(1, 1, -1, 1) # quantiles has shape (batch_size, n_critics, n_quantiles), so make cum_prob broadcastable to (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    \n",
    "    loss = (torch.abs(cum_prob - (delta < 0).float()) * huber_loss)\n",
    "\n",
    "    # Summing over the quantile dimension \n",
    "    if sum_over_quantiles:\n",
    "        loss = loss.sum(dim=-2).mean()\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# MLP for actor that implements D2RL architecture\n",
    "class ActorMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim, output size = action dim\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "    # input_dim_ = input_dim\n",
    "    # num_inputs = 24 # input_dim\n",
    "    # input_dim = hidden_dims[0] + num_inputs\n",
    "\n",
    "    current_dim = input_dim # first layer has input dim = state dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim # prev layers output + original input size\n",
    "      # if i == 0:\n",
    "      #   lay = nn.Linear(input_dim_, next_size)\n",
    "      # else:\n",
    "      #   lay = nn.Linear(input_dim, next_size)\n",
    "      # self.add_module(f'layer{i}', lay)\n",
    "      # self.layer_list.append(lay)\n",
    "      # input_dim_ = next_size\n",
    "        \n",
    "    # Final layer input dim is last hidden layer output + original input size\n",
    "    self.last_layer_mean_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "    self.last_layer_log_std_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim=1) # cat with output layer\n",
    "\n",
    "    mean_linear = self.last_layer_mean_linear(curr)\n",
    "    log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "    return mean_linear, log_std_linear\n",
    "\n",
    "# MLP for critic that implements D2RL architecture \n",
    "class CriticMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim + action dim, output size = n_quantiles\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    current_dim = input_dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim\n",
    "\n",
    "    self.last_layer = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim = 1)\n",
    "      \n",
    "    output = self.last_layer(curr)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh dist for actor to sample actions from \n",
    "\n",
    "# FROM ORIGINAL CODE https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# class TanhNormal(D.Distribution):\n",
    "#   def __init__(self, normal_mean, normal_std):\n",
    "#     super().__init__()\n",
    "#     self.normal_mean = normal_mean\n",
    "#     self.normal_std = normal_std\n",
    "#     self.normal = D.Normal(normal_mean, normal_std)\n",
    "#     self.stand_normal = D.Normal(torch.zeros_like(self.normal_mean), torch.ones_like(self.normal_std))        \n",
    "      \n",
    "#   def logsigmoid(tensor):\n",
    "#     denominator = 1 + torch.exp(-tensor)\n",
    "#     return torch.log(1/ denominator)\n",
    "\n",
    "#   def log_probability(self, pre_tanh):\n",
    "#     final = (self.normal.log_prob(pre_tanh)) - (2 * np.log(2) + F.logsigmoid(2 * pre_tanh) + F.logsigmoid(-2 * pre_tanh))\n",
    "#     return final\n",
    "\n",
    "#   def random_sample(self):\n",
    "#     pretanh = self.normal_mean + self.normal_std * self.stand_normal.sample()\n",
    "#     return torch.tanh(pretanh), pretanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class GradientStep(object):\n",
    "  '''\n",
    "  see (D2RL) https://github.com/pairlab/d2rl/blob/main/sac/sac.py\n",
    "  and (TQC) https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/tqc/tqc.py\n",
    "  '''\n",
    "  def __init__(self,*,\n",
    "    actor, critic, critic_target, discount, tau,\n",
    "    actor_lr, critic_lr, alpha_lr,\n",
    "    n_quantiles, n_mini_critics, top_quantiles_to_drop_per_net, target_entropy,\n",
    "    ):\n",
    "\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.critic_target = critic_target\n",
    "\n",
    "    self.log_alpha = nn.Parameter(torch.zeros(1).to(DEVICE)) # log alpha is learned\n",
    "    self.quantiles_total = n_quantiles * n_mini_critics\n",
    "    \n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "    self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    self.discount, self.tau = discount, tau\n",
    "    self.top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_mini_critics # total number of quantiles to drop\n",
    "    self.target_entropy = target_entropy\n",
    "\n",
    "  def take_gradient_step(self, replay_buffer, total_steps, batch_size=256):\n",
    "    # Sample batch from replay buffer\n",
    "    state, action, next_state, reward, not_done = replay_buffer.sample(batch_size, total_steps)\n",
    "    alpha = torch.exp(self.log_alpha) # entropy temperature coefficient\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # sample new action from actor on next state\n",
    "      new_next_action, next_log_pi = self.actor(next_state)\n",
    "\n",
    "      # Compute and cut quantiles at the next state\n",
    "      next_z = self.critic_target(next_state, new_next_action)  \n",
    "      \n",
    "      # Sort and drop top k quantiles to control overestimation (TQC)\n",
    "      sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "      sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop] # estimated truncated Q-val dist for next state\n",
    "\n",
    "      # td error + entropy term\n",
    "      target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "    \n",
    "    # Get current quantile estimates using action from the replay buffer\n",
    "    cur_z = self.critic(state, action)\n",
    "    critic_loss = quantile_huber_loss(cur_z, target)\n",
    "\n",
    "    new_action, log_pi = self.actor(state)\n",
    "    # detach the variable from the graph so we don't change it with other losses\n",
    "    alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean() # as in D2RL implementation for auto entropy tuning\n",
    "\n",
    "    # Optimise critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # Soft update target networks\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Compute actor (π) loss\n",
    "    # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]\n",
    "    actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "    # ^ mean(2) is over quantiles, mean(1) is over critic ensemble\n",
    "    \n",
    "    # Optimise the actor\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # Optimise the entropy coefficient\n",
    "    self.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    self.alpha_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[512, 512]):\n",
    "        super().__init__()\n",
    "        self.mlp = ActorMLP(state_dim, hidden_dims, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.mlp(obs)\n",
    "        log_std = log_std.clamp(-20, 2) # clamp for stability\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        base_N_dist = D.Normal(mean, std) # base normal dist\n",
    "        tanh_transform = TanhTransform(cache_size=1) # transform to get the tanh dist\n",
    "\n",
    "        log_prob = None\n",
    "        if self.training: # i.e. agent.train()\n",
    "            transformed_dist = D.TransformedDistribution(base_N_dist, tanh_transform) # transformed distribution\n",
    "            action = transformed_dist.rsample() # samples from base dist & applies transform\n",
    "            log_prob = transformed_dist.log_prob(action) # log prob of action after transform\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True) # sum over action dim\n",
    "        else: # evaluation mode\n",
    "            action = torch.tanh(mean)\n",
    "\n",
    "        # FROM ORIGINAL CODE using custom tanh dist\n",
    "        # if self.training:\n",
    "        #     tanh_dist = TanhNormal(mean, std) # use custom tanh dist\n",
    "        #     action, pre_tanh = tanh_dist.random_sample()\n",
    "        #     log_prob = tanh_dist.log_probability(pre_tanh)\n",
    "        #     log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        # else:\n",
    "        #     action = torch.tanh(mean)\n",
    "            \n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs).to(DEVICE)[np.newaxis, :]\n",
    "        action, log_prob = self.forward(obs)\n",
    "        return np.array(action[0].cpu().detach())\n",
    "\n",
    "\n",
    "class Critic(nn.Module): # really a mega-critic from lots of mini-critics\n",
    "    '''\n",
    "    Ensemble of critics for TQC\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.critics = nn.ModuleList()\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        for _ in range(n_nets): # multiple critic mlps\n",
    "            net = CriticMLP(state_dim + action_dim, hidden_dims, n_quantiles)\n",
    "            self.critics.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # cat state and action (to pass to critic)\n",
    "        state_act = torch.cat((state, action), dim=1)\n",
    "\n",
    "        # get quantiles from each critic mlp\n",
    "        quantiles = [critic(state_act) for critic in self.critics]\n",
    "        quantiles = torch.stack(quantiles, dim=1) # stack into tensor\n",
    "        return quantiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "H466HICfpDB1"
   },
   "outputs": [],
   "source": [
    "# XinJingHao's (some of) hyperparams\n",
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# params = {\n",
    "#   'eval_interval' : int(2e3),  # Model evaluating interval, in steps\n",
    "#   'delay_freq' : 1,  # Delayed frequency for Actor and Target Net\n",
    "#   'explore_noise' : 0.15,  # Exploring noise when interacting\n",
    "#   'explore_noise_decay' : 0.998,  # Decay rate of explore noise\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My hyperparams\n",
    "\n",
    "seed = 42 # DONT CHANGE FOR COURSEWORK\n",
    "# use_wandb = False # use wandb for logging\n",
    "\n",
    "hyperparams = {\n",
    "    # env/general params\n",
    "    \"max_timesteps\": MAX_TIMESTEPS, # per episode [DONT CHANGE]\n",
    "    \"max_episodes\": 200,\n",
    "    \"target_score\": 300, # stop training when average score over 100 episodes is > target_score\n",
    "    \"hardcore\": False,\n",
    "\n",
    "    # Agent hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "    \"n_mini_critics\": 5, # each mini-critic is a single mlp, which combine to make one mega-critic\n",
    "    \"n_quantiles\": 25, # quantiles per mini critic\n",
    "    \"top_quantiles_to_drop_per_net\": 'auto', # per mini critic (auto based on n_quantiles)\n",
    "    \"actor_hidden_dims\": [512, 512],\n",
    "    \"mini_critic_hidden_dims\": [256, 256], # * n_mini_critics\n",
    "    \"batch_size\": 256,\n",
    "    \"discount\": 0.98, # gamma\n",
    "    \"tau\": 0.005,\n",
    "    \"actor_lr\": 3e-4,\n",
    "    \"critic_lr\": 3e-4,\n",
    "    \"alpha_lr\": 3e-4,\n",
    "\n",
    "    # ERE buffer (see paper for their choices)\n",
    "    \"buffer_size\": 500000, # smaller size improves learning early on but is outperformed later on\n",
    "    \"eta0\": 0.994, # 0.994 - 0.999 is good (according to paper)\n",
    "    \"annealing_steps\": 'auto', # number of steps to anneal eta over (after which sampling is uniform) - None = auto-set to max estimated steps in training\n",
    "    \"cmin\": 5000, # min number of samples to sample from\n",
    "\n",
    "    # dreamer hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "    # info on hyperparam choice: https://arxiv.org/abs/1912.01603\n",
    "    \"use_dreamer\": True,\n",
    "    \"batch_size_dreamer\": 512,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 16,\n",
    "    \"num_heads\": 4,\n",
    "    \"dreamer_lr\": 3e-4,\n",
    "    \"dropout_prob\": 0.1,\n",
    "    \"window_size\": 40,               # transformer context window size\n",
    "    \"step_size\": 1,                  # how many timesteps to skip between each context window\n",
    "    \"train_split\": 0.8,              # train/validation split\n",
    "    \"score_threshold\": 0.8,          # use dreamer if loss < score_threshold\n",
    "    \"auto_horizon\": False,           # True: sets imagination_horizon to min (imagination_horizon, last episode length)\n",
    "    \"imagination_horizon\": 50,       # how many timesteps to run the dreamer model for (H in Dreamer paper) - need to empirically test for best\n",
    "    \"dreamer_train_epochs\": 15,      # how many epochs to train the dreamer model for\n",
    "    \"dreamer_train_frequency\": 10,   # how often to train the dreamer model\n",
    "    \"episode_threshold\": 50,         # how many episodes to run before training the dreamer model\n",
    "    \"max_size\": int(5e4),            # maximum size of the training set for the dreamer model\n",
    "}\n",
    "\n",
    "# recording/logging\n",
    "plot_interval = 10 # plot every Nth episode\n",
    "save_fig = True # save figures too\n",
    "\n",
    "is_recording = True\n",
    "video_interval = 25 # record every Nth episode\n",
    "ep_start_rec = 100 # start recording on this episode\n",
    "\n",
    "save_model = False # NOTE - currently only saves actor & no way to load models\n",
    "save_interval = 30 # save every Nth episode\n",
    "\n",
    "if hyperparams['annealing_steps'] == 'auto':\n",
    "    hyperparams['annealing_steps'] = hyperparams['max_episodes']*hyperparams['max_timesteps'] # max est number of steps in training\n",
    "if hyperparams['top_quantiles_to_drop_per_net'] == 'auto':\n",
    "    hyperparams['top_quantiles_to_drop_per_net'] = int(hyperparams['n_quantiles'] // 12.5) # keep ratio same as M=25 d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1Xrcek4hxDXl",
    "outputId": "6161f148-a164-4e22-d97a-f9aa53e7b8b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/wandb/run-20250425_230529-mibtd7ut</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/mibtd7ut' target=\"_blank\">mac_2025-04-25_23-05-29</a></strong> to <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d' target=\"_blank\">https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/mibtd7ut' target=\"_blank\">https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/mibtd7ut</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SETUP\n",
    "\n",
    "# init wandb run\n",
    "import datetime\n",
    "suffix = '_hardcore' if hyperparams['hardcore'] else ''\n",
    "run_name = SOURCE_ID + f\"_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\" + suffix\n",
    "wandb.init(\n",
    "    project=\"RL-Coursework-Walker2d\",\n",
    "    config=hyperparams,\n",
    "    name=run_name,\n",
    "    save_code=True, # Optional: saves main script/notebook to W&B\n",
    ")\n",
    "config = wandb.config # use wandb.config to access hyperparameters\n",
    "\n",
    "# make env\n",
    "# only attempt hardcore when your agent has solved the non-hardcore version\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=config.hardcore)\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=\"nchw73-agent-hardcore-video\" if config.hardcore else \"nchw73-agent-video\",  # prefix for videos\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "rld.check_device() # training on CPU recommended\n",
    "\n",
    "env.video = False # switch video recording off (only switch on every x episodes as this is slow)\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, state_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=seed)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWjENKkU_My"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, state, info = rld.seed_everything(seed, env)\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "replay_buffer = EREReplayBuffer(state_dim, act_dim, config.annealing_steps, config.buffer_size, config.eta0, config.cmin)\n",
    "\n",
    "actor = Actor(state_dim, act_dim, config.actor_hidden_dims).to(DEVICE)\n",
    "\n",
    "critic = Critic(state_dim, act_dim, config.n_quantiles, config.n_mini_critics, config.mini_critic_hidden_dims).to(DEVICE)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "if config.use_dreamer:\n",
    "    dreamer = DreamerAgent(state_dim, act_dim, config.hidden_dim, \n",
    "        config.window_size, config.num_layers, config.num_heads, config.dropout_prob, config.dreamer_lr).to(DEVICE)\n",
    "\n",
    "target_entropy = -np.prod(env.action_space.shape).item() # target entropy heuristic (= −dim(A) as in D2RL paper)\n",
    "grad_step_class = GradientStep(\n",
    "    actor=actor, critic=critic, critic_target=critic_target,\n",
    "    discount=config.discount, tau=config.tau,\n",
    "    actor_lr=config.actor_lr, critic_lr=config.critic_lr, alpha_lr=config.alpha_lr,\n",
    "    n_quantiles=config.n_quantiles, n_mini_critics=config.n_mini_critics,\n",
    "    top_quantiles_to_drop_per_net=config.top_quantiles_to_drop_per_net,\n",
    "    target_entropy=target_entropy,\n",
    "    )\n",
    "\n",
    "actor.train()\n",
    "\n",
    "ep_timesteps = 0\n",
    "total_steps = 0\n",
    "ep_reward = 0\n",
    "memory_ptr = 0 # marks boundary between new and old data in the replay buffer\n",
    "train_set, test_set = None, None\n",
    "reward_list = []\n",
    "reward_avg_list = []\n",
    "ep_timesteps_dreamer = ep_reward_dreamer = dreamer_eps = 0\n",
    "\n",
    "episode = 1\n",
    "success_ep = config.max_episodes # minimise i such that r_mean @ episode i >= target_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# sample real env\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m ep_timesteps, ep_reward, input_buffer, info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_step_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_timesteps\n\u001b[1;32m     17\u001b[0m tracker\u001b[38;5;241m.\u001b[39mtrack(info) \u001b[38;5;66;03m# track statistics for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m, in \u001b[0;36mtrain_on_environment\u001b[0;34m(actor, env, grad_step_class, replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length)\u001b[0m\n\u001b[1;32m     54\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size: \u001b[38;5;66;03m# do 1 training update using 1 batch from buffer if enough\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# train the agent using experiences from the real environment\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mgrad_step_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;66;03m# break if finished\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 50\u001b[0m, in \u001b[0;36mGradientStep.take_gradient_step\u001b[0;34m(self, replay_buffer, total_steps, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Get current quantile estimates using action from the replay buffer\u001b[39;00m\n\u001b[1;32m     49\u001b[0m cur_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(state, action)\n\u001b[0;32m---> 50\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m \u001b[43mquantile_huber_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m new_action, log_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# detach the variable from the graph so we don't change it with other losses\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mquantile_huber_loss\u001b[0;34m(quantiles, samples, sum_over_quantiles)\u001b[0m\n\u001b[1;32m     16\u001b[0m delta \u001b[38;5;241m=\u001b[39m samples[:, np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis, :] \u001b[38;5;241m-\u001b[39m quantiles[:, :, :, np\u001b[38;5;241m.\u001b[39mnewaxis]  \n\u001b[1;32m     17\u001b[0m abs_delta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(delta)\n\u001b[0;32m---> 18\u001b[0m huber_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mabs_delta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m, abs_delta \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m, delta \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     19\u001b[0m n_quantiles \u001b[38;5;241m=\u001b[39m quantiles\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# cumulative probabilities to calc quantiles\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "while episode <= config.max_episodes: # index from 1\n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    if is_recording and episode >= ep_start_rec:\n",
    "        env.info = episode % video_interval == 0   # track every x episodes (usually tracking every episode is fine)\n",
    "        env.video = episode % video_interval == 0  # record videos every x episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # sample real env\n",
    "    ep_timesteps, ep_reward, input_buffer, info = train_on_environment(\n",
    "        actor, env, grad_step_class, replay_buffer, config.max_timesteps, state,\n",
    "        config.batch_size, total_steps, config.window_size)\n",
    "\n",
    "    total_steps += ep_timesteps\n",
    "    tracker.track(info) # track statistics for plotting\n",
    "\n",
    "    # update train/test sets for dreamer (add new eps and trim to window size)\n",
    "    train_set, test_set = gen_test_train_seq(\n",
    "        replay_buffer, train_set, test_set, config.train_split, config.window_size, config.step_size, memory_ptr)\n",
    "\n",
    "    # train dreamer after ep_thresh and if the input buffer has enough data to fill context window\n",
    "    if config.use_dreamer and episode >= config.episode_threshold and input_buffer.shape[0] == config.window_size:\n",
    "        # train and assess the dreamer every train_frequency\n",
    "        if episode % config.dreamer_train_frequency == 0:\n",
    "            dreamer.train_dreamer(train_set, config.dreamer_train_epochs, config.batch_size_dreamer)\n",
    "\n",
    "        # truncate the training set to control train time performance\n",
    "        if test_set.shape[0] > config.max_size:\n",
    "            train_set = train_set[-config.max_size:]\n",
    "\n",
    "        print(f'Size of train set: {train_set.shape[0]}, test set: {test_set.shape[0]}')\n",
    "        \n",
    "        # evaluate the dreamer's performance & decide num of training steps for dreamer\n",
    "        dreamer_avg_loss = dreamer.test_dreamer(test_set, config.batch_size_dreamer)\n",
    "        dreamer_eps = get_dreamer_eps(dreamer_avg_loss, config.score_threshold)\n",
    "\n",
    "        # train on dreamer if its accurate enough\n",
    "        if dreamer_eps > 0:\n",
    "            print(f'Dreamer active for {dreamer_eps} iterations')\n",
    "            ep_timesteps_dreamer = ep_reward_dreamer = 0\n",
    "            for dep in range(dreamer_eps):\n",
    "                print(f'Dreamer ep: {dep+1}')\n",
    "\n",
    "                # initialise dreamer states with the input sequence\n",
    "                # input buffer = state, action, next_state, reward, done NOTE - this is different to order learned in training\n",
    "                dreamer.states = input_buffer[:, :state_dim]\n",
    "                dreamer.actions = input_buffer[:-1, state_dim:state_dim+act_dim]\n",
    "                dreamer.rewards = input_buffer[:-1, -2-state_dim].unsqueeze(1)\n",
    "                dreamer.dones = input_buffer[:-1, -1-state_dim].unsqueeze(1)\n",
    "\n",
    "                # sample from dreamer environment (ignore input buffer and info)\n",
    "                if config.auto_horizon:\n",
    "                    imagination_horizon = min(config.imagination_horizon, ep_timesteps)\n",
    "                else:\n",
    "                    imagination_horizon = config.imagination_horizon\n",
    "                _td, _rd, _, _ = train_on_environment(\n",
    "                    actor, dreamer, grad_step_class, replay_buffer,\n",
    "                    imagination_horizon, # number of timesteps to run the dreamer for\n",
    "                    dreamer.states[-1].cpu().numpy(), # use last state from dreamer TODO why?\n",
    "                    config.batch_size, total_steps, config.window_size)\n",
    "\n",
    "                ep_timesteps_dreamer += _td\n",
    "                ep_reward_dreamer += _rd\n",
    "\n",
    "    memory_ptr = replay_buffer.ptr # update the memory pointer to the current position in the buffer\n",
    "\n",
    "    print(f\"Ep: {episode} | Timesteps: {ep_timesteps} | Reward: {ep_reward:.3f} | Total Steps: {total_steps:.2g} | Dreamer: {dreamer_eps > 0}\")\n",
    "    if config.use_dreamer and dreamer_eps > 0:\n",
    "        print(f\"\\t Dreamer Eps: {dreamer_eps} | Dreamer Avg Reward: {ep_reward_dreamer/dreamer_eps:.3f} | Dreamer Avg Timesteps: {ep_timesteps_dreamer/dreamer_eps:.3g}\")\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    log_dict = {\n",
    "        \"Total Steps\": total_steps,\n",
    "        # \"Episode Reward\": ep_reward, # same as r_sum\n",
    "        \"Episode Timesteps\": ep_timesteps,\n",
    "    }\n",
    "\n",
    "    # log tracked statistics\n",
    "    if 'recorder' in info: # if we have info available\n",
    "        log_dict[\"TrackedInfo/r_mean_\"] = info['recorder']['r_mean_']\n",
    "        log_dict[\"TrackedInfo/r_std_\"] = info['recorder']['r_std_']\n",
    "        log_dict[\"TrackedInfo/r_sum\"] = info['recorder']['r_sum']\n",
    "\n",
    "        if info['recorder']['r_mean_'] >= config.target_score: # update success_ep\n",
    "            success_ep = min(success_ep, episode)\n",
    "            print(f'target score reached at episode {success_ep}!')\n",
    "        log_dict[\"Episodes to Reach Target\"] = success_ep\n",
    "\n",
    "    if config.use_dreamer:\n",
    "        log_dict[\"Dreamer Average Loss\"] = dreamer_avg_loss if 'dreamer_avg_loss' in locals() else None # Only log if calculated\n",
    "        log_dict[\"Dreamer Episodes Run\"] = dreamer_eps\n",
    "        if dreamer_eps > 0:\n",
    "            log_dict[\"Dreamer Average Reward\"] = ep_reward_dreamer / dreamer_eps\n",
    "            log_dict[\"Dreamer Average Timesteps\"] = ep_timesteps_dreamer / dreamer_eps\n",
    "\n",
    "    wandb.log(log_dict, step=episode) # log metrics each ep\n",
    "\n",
    "    reward_list.append(ep_reward)\n",
    "    reward_avg_list.append(ep_reward)\n",
    "\n",
    "    # plot tracked statistics (on real env)\n",
    "    if episode % plot_interval == 0:\n",
    "        # save as well (show=False returns it but doesnt display)\n",
    "        if save_fig and info : # check info isnt {} (i.e. not None)\n",
    "            fig, _ = tracker.plot(show=False, r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "            wandb.log({\"Coursework Plot\": wandb.Image(fig)}, step=episode) # Log the figure directly to W&B\n",
    "            fig.savefig(f'./tracker_{run_name}.png', bbox_inches = 'tight')\n",
    "            plt.close(fig) # Close the figure to free memory\n",
    "        # show by default\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "    # save model\n",
    "    if save_model and episode % save_interval == 0:\n",
    "        model_path = f\"./actor_{run_name}_{episode}.pth\" # Include run name in path\n",
    "        torch.save(actor.state_dict(), model_path)\n",
    "        wandb.save(model_path) # Upload the saved model file to W&B run\n",
    "        print(f\"Model saved: {model_path}\")\n",
    "        # TODO note - maybe save critic / dreamer too?\n",
    "        # also - maybe delete old ones?\n",
    "\n",
    "    episode += 1\n",
    "    ep_reward = 0\n",
    "\n",
    "    # break condition - stop if we consistently meet target score\n",
    "    if len(reward_avg_list) >= 100 or success_ep < config.max_episodes:\n",
    "        current_avg = np.array(reward_avg_list).mean()\n",
    "        print(f'Current progress: {current_avg:.3f} / {config.target_score}')\n",
    "        if current_avg >= config.target_score or success_ep < config.max_episodes: # quit when we've got good enough performance\n",
    "            print('Completed environment!')\n",
    "            break\n",
    "        reward_avg_list = reward_avg_list[-99:] # discard oldest on list (keep most recent 99)\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file\n",
    "if config.hardcore:\n",
    "    filename = \"nchw73-agent-hardcore-log.txt\"\n",
    "else:\n",
    "    filename = \"nchw73-agent-log.txt\"\n",
    "env.write_log(folder=\"logs\", file=filename)\n",
    "\n",
    "wandb.finish() # finish wandb run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
