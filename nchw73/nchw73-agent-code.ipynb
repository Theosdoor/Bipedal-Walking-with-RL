{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyOJa8oZPjGq"
   },
   "source": [
    "https://arena-chapter2-rl.streamlit.app/[2.2]_Q-Learning_and_DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSUGAJsTLOV",
    "outputId": "875609e2-e3eb-4b29-c03c-1989820b1ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /home2/nchw73/myjupyterenv/lib/python3.9/site-packages (4.3.0)\n",
      "Requirement already satisfied: torchinfo in /home2/nchw73/myjupyterenv/lib/python3.9/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/robert-lieck/rldurham/tree/main - rldurham repo\n",
    "\n",
    "!pip install swig\n",
    "# !pip install --upgrade rldurham\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AFrqd24JTLOW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# Jinghao\n",
    "import os\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np # consider switching out for torch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import rldurham as rld\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "**Reinforcement learning agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkDif1GF-Swa"
   },
   "source": [
    "NOTE - I used [this implementation](https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py) of a TD3 agent, which I informally refer to as 'Jinghao' in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4jXNHP8_U-rn"
   },
   "outputs": [],
   "source": [
    "# class Agent(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Agent, self).__init__()\n",
    "\n",
    "#     def sample_action(self, s):\n",
    "#         return torch.rand(act_dim) * 2 - 1 # uniform random in [-1, 1]\n",
    "\n",
    "#     def put_data(self, action, observation, reward):\n",
    "#         pass\n",
    "\n",
    "#     def train(self):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8YWZaW-YoT2j"
   },
   "outputs": [],
   "source": [
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# helper fns\n",
    "\n",
    "def evaluate_policy(env, agent, turns=3):\n",
    "    total_scores = 0\n",
    "    for i in range(turns):\n",
    "        s, _ = env.reset()\n",
    "        for t in range(2000): # 2000 = max_timesteps (TODO - diff from Jinghao's imp)\n",
    "            # Take deterministic actions at test time\n",
    "            a = agent.select_action(s, deterministic=True)\n",
    "            s_next, r, dw, tr, _ = env.step(a)\n",
    "            done = (dw or tr)\n",
    "\n",
    "            total_scores += r\n",
    "            s = s_next\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return int(total_scores / turns)\n",
    "\n",
    "def adapt_reward(r):\n",
    "    \"\"\"Reward engineering for better training.\"\"\"\n",
    "    # For BipedalWalker (both versions)\n",
    "    if r <= -100:\n",
    "        r = -1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9kTeUHYL7gU6"
   },
   "outputs": [],
   "source": [
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# agent\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, state_dim, action_dim, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.s = torch.zeros((max_size, state_dim), dtype=torch.float)\n",
    "        self.a = torch.zeros((max_size, action_dim), dtype=torch.float)\n",
    "        self.r = torch.zeros((max_size, 1), dtype=torch.float)\n",
    "        self.s_next = torch.zeros((max_size, state_dim), dtype=torch.float)\n",
    "        self.dw = torch.zeros((max_size, 1), dtype=torch.bool)\n",
    "\n",
    "    def add(self, s, a, r, s_next, dw):\n",
    "        self.s[self.ptr] = torch.from_numpy(s)\n",
    "        self.a[self.ptr] = torch.from_numpy(a) # Note that a is numpy.array\n",
    "        self.r[self.ptr] = r\n",
    "        self.s_next[self.ptr] = torch.from_numpy(s_next)\n",
    "        self.dw[self.ptr] = dw\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size  # Overwrite when full\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = torch.randint(0, self.size, size=(batch_size,))\n",
    "        return self.s[ind], self.a[ind], self.r[ind], self.s_next[ind], self.dw[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i35_XJdr9LoC"
   },
   "outputs": [],
   "source": [
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# actor and critic\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, net_width, maxaction):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, net_width)\n",
    "        self.l2 = nn.Linear(net_width, net_width)\n",
    "        self.l3 = nn.Linear(net_width, action_dim)\n",
    "        self.maxaction = maxaction\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = torch.tanh(self.l1(state))\n",
    "        a = torch.tanh(self.l2(a))\n",
    "        a = torch.tanh(self.l3(a)) * self.maxaction\n",
    "        return a\n",
    "\n",
    "class Double_Q_Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, net_width):\n",
    "        super(Double_Q_Critic, self).__init__()\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, net_width)\n",
    "        self.l2 = nn.Linear(net_width, net_width)\n",
    "        self.l3 = nn.Linear(net_width, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, net_width)\n",
    "        self.l5 = nn.Linear(net_width, net_width)\n",
    "        self.l6 = nn.Linear(net_width, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "s04vKGU3dNuF"
   },
   "outputs": [],
   "source": [
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "class TD3_agent():\n",
    "    def __init__(self, **kwargs):\n",
    "        # Init hyperparameters for agent, just like \"self.gamma = opt.gamma, self.lambd = opt.lambd, ...\"\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.policy_noise = 0.2 * self.max_action\n",
    "        self.noise_clip = 0.5 * self.max_action\n",
    "        self.tau = 0.005\n",
    "        self.delay_counter = 0\n",
    "\n",
    "        # Actor network and its target\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.net_width, self.max_action)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.a_lr)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "\n",
    "        # Double Q-Critic network and its target\n",
    "        self.q_critic = Double_Q_Critic(self.state_dim, self.action_dim, self.net_width)\n",
    "        self.q_critic_optimizer = torch.optim.Adam(self.q_critic.parameters(), lr=self.c_lr)\n",
    "        self.q_critic_target = copy.deepcopy(self.q_critic)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.state_dim, self.action_dim, max_size=int(1e6))\n",
    "\n",
    "    def select_action(self, state, deterministic):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state[np.newaxis, :]) # from [x,x,...,x] to [[x,x,...,x]]\n",
    "            a = self.actor(state).cpu().numpy()[0] # from [[x,x,...,x]] to [x,x,...,x]\n",
    "            if deterministic:\n",
    "                return a\n",
    "            else:\n",
    "                noise = np.random.normal(0, self.max_action * self.explore_noise, size=self.action_dim)\n",
    "                return (a + noise).clip(-self.max_action, self.max_action)\n",
    "\n",
    "    def train(self):\n",
    "        self.delay_counter += 1\n",
    "        with torch.no_grad():\n",
    "            s, a, r, s_next, dw = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "            # Compute target Q using target actor and target critic networks\n",
    "            target_a_noise = (torch.randn_like(a) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            '''↓↓↓ Target Policy Smoothing Regularization ↓↓↓'''\n",
    "            smoothed_target_a = (self.actor_target(s_next) + target_a_noise).clamp(-self.max_action, self.max_action)\n",
    "            target_Q1, target_Q2 = self.q_critic_target(s_next, smoothed_target_a)\n",
    "            '''↓↓↓ Clipped Double Q-learning ↓↓↓'''\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = r + (~dw) * self.gamma * target_Q # dw: die or win\n",
    "\n",
    "        current_Q1, current_Q2 = self.q_critic(s, a) # Get current Q estimates\n",
    "\n",
    "        # Compute critic loss, and optimize the q_critic\n",
    "        q_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "        self.q_critic_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_critic_optimizer.step()\n",
    "\n",
    "        # Delayed policy updates\n",
    "        '''↓↓↓ Clipped Double Q-learning ↓↓↓'''\n",
    "        if self.delay_counter > self.delay_freq:\n",
    "            # Update the Actor\n",
    "            a_loss = -self.q_critic.Q1(s, self.actor(s)).mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            a_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            with torch.no_grad():\n",
    "                for param, target_param in zip(self.q_critic.parameters(), self.q_critic_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            self.delay_counter = 0\n",
    "\n",
    "    def save(self, EnvName, timestep):\n",
    "        torch.save(self.actor.state_dict(), f\"./model/{EnvName}_actor{timestep}.pth\")\n",
    "        torch.save(self.q_critic.state_dict(), f\"./model/{EnvName}_q_critic{timestep}.pth\")\n",
    "\n",
    "    def load(self, EnvName, timestep):\n",
    "        self.actor.load_state_dict(torch.load(f\"./model/{EnvName}_actor{timestep}.pth\"))\n",
    "        self.q_critic.load_state_dict(torch.load(f\"./model/{EnvName}_q_critic{timestep}.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H466HICfpDB1"
   },
   "outputs": [],
   "source": [
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "params = {\n",
    "  'Loadmodel': False,  # Load pretrained model or Not\n",
    "  'ModelIdex': 30,  # Which model to load\n",
    "  'update_every' : 50,  # Training frequency\n",
    "  'save_interval' : int(1e5),  # Model saving interval, in steps\n",
    "  'eval_interval' : int(2e3),  # Model evaluating interval, in steps\n",
    "  'delay_freq' : 1,  # Delayed frequency for Actor and Target Net\n",
    "  'gamma' : 0.99,  # Discounted Factor\n",
    "  'net_width' : 256,  # Hidden net width\n",
    "  'a_lr' : 1e-4,  # Learning rate of actor\n",
    "  'c_lr' : 1e-4,  # Learning rate of critic\n",
    "  'batch_size' : 256,  # Batch size for training\n",
    "  'explore_noise' : 0.15,  # Exploring noise when interacting\n",
    "  'explore_noise_decay' : 0.998,  # Decay rate of explore noise\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1Xrcek4hxDXl",
    "outputId": "6161f148-a164-4e22-d97a-f9aa53e7b8b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/nchw73/myjupyterenv/lib/python3.9/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home2/nchw73/Year 3/RL/coursework/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hardcore = True\n",
    "\n",
    "if hardcore == False:\n",
    "  env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n",
    "  eval_env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\") # TODO Jinghao - maybe necessary to eval policy w/o noise?\n",
    "else:\n",
    "  env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n",
    "  eval_env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True)\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "if hardcore:\n",
    "    vid_prefix = \"nchw73-agent-hardcore-video\"\n",
    "else:\n",
    "    vid_prefix = \"nchw73-agent-video\"\n",
    "    \n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=vid_prefix,  # prefix for videos\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "# training on CPU recommended\n",
    "rld.check_device()\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n",
    "params['max_action'] = float(env.action_space.high[0]) # Jinghao: this is used to scale the agent's actions to ensure they stay in action space\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=42)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWjENKkU_My"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxRrlDYLU6HG",
    "outputId": "972a134b-ac9b-49dd-dae0-11a9fd436e2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, observation, info = rld.seed_everything(42, env)\n",
    "\n",
    "max_episodes = 100 # at least 100 for setup\n",
    "max_timesteps = 2000 # DO NOT CHANGE\n",
    "max_total_steps = max_episodes * max_timesteps\n",
    "warm_up_steps = 10 * max_timesteps # TODO adjust this to need\n",
    "\n",
    "# for Jinghao format TODO change\n",
    "params['state_dim'] = obs_dim\n",
    "params['action_dim'] = act_dim\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "is_recording = True\n",
    "env.video = False # switch video recording off (only switch on every x episodes as this is slow)\n",
    "\n",
    "# initialise agent\n",
    "agent = TD3_agent(**params) # ** unpacks the dictionary into keyword arguments\n",
    "# summary(agent) # torchinfo for parameter breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "rDl6ViIDlVOk",
    "outputId": "4068a79b-d730-41f9-9de6-0dcee018c535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0, Steps: 0k, Episode Reward: -13\n",
      "Ep: 1, Steps: 0k, Episode Reward: -98\n",
      "Ep: 2, Steps: 0k, Episode Reward: -97\n",
      "Ep: 3, Steps: 0k, Episode Reward: -78\n"
     ]
    }
   ],
   "source": [
    "# training procedure\n",
    "total_steps = 0\n",
    "test = True\n",
    "for episode in range(max_episodes):\n",
    "\n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    if is_recording: # generally get good results after ep 500 on easy\n",
    "        env.info = episode % 10 == 0   # track every x episodes (usually tracking every episode is fine)\n",
    "        env.video = episode % 10 == 0  # record videos every x episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run episode\n",
    "    for t in range(max_timesteps):\n",
    "\n",
    "        # select the agent action\n",
    "        if total_steps < warm_up_steps: # warm up period for first x% of steps (NOTE - different to Jinghao imp) TODO check this helps\n",
    "            if not test:\n",
    "                print('warm up... again???')\n",
    "            action = env.action_space.sample() # random actions\n",
    "        else:\n",
    "            if test:\n",
    "              print('warm up over!')\n",
    "              test = False\n",
    "            # action = agent.sample_action(observation)\n",
    "            action = agent.select_action(observation, deterministic=False) # Jinghao TODO\n",
    "\n",
    "        # take action in the environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        reward = adapt_reward(reward) # Jinghao\n",
    "\n",
    "        # remember\n",
    "        # agent.put_data(action, observation, reward)\n",
    "        agent.replay_buffer.add(observation, action, reward, next_observation, terminated) # Jinghao todo\n",
    "\n",
    "        observation = next_observation # TODO necessary if its just reset next ep?\n",
    "\n",
    "        total_steps += 1\n",
    "\n",
    "        # check whether done\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # TRAIN THE AGENT HERE!\n",
    "        # TODO Jinghao does this as follows in the loop, but template trains after each ep\n",
    "        # Train every 'update_every' steps after an initial period\n",
    "        if (episode >= 1) and (total_steps % params['update_every'] == 0):\n",
    "            for _ in range(params['update_every']):\n",
    "                agent.train()\n",
    "\n",
    "        # stop episode\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Evaluate policy after each episode (TODO Jinghao - JH imp does this every 2000 steps (hyperparam), not necc after each ep)\n",
    "    agent.explore_noise *= params['explore_noise_decay']\n",
    "    ep_reward = evaluate_policy(eval_env, agent, turns=3)\n",
    "    print(f'Ep: {episode}, Steps: {total_steps//1000}k, Episode Reward: {ep_reward}')\n",
    "\n",
    "    # track and plot statistics\n",
    "    tracker.track(info)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "eval_env.close()\n",
    "\n",
    "# write log file (for coursework)\n",
    "if hardcore:\n",
    "    filename = \"nchw73-agent-hardcore-log.txt\"\n",
    "else:\n",
    "    filename = \"nchw73-agent-log.txt\"\n",
    "env.write_log(folder=\"logs\", file=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7fuO3i5_IgR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUgcPaSl_GPd"
   },
   "source": [
    "##  Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF3VpYkaTLOY"
   },
   "source": [
    "A small demo with a predefined heuristic that is suboptimal and has no notion of balance (and is designed for the orignal BipedalWalker environment)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLr7xO8aTLOY"
   },
   "outputs": [],
   "source": [
    "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n",
    "\n",
    "env = rld.make(\n",
    "    \"rldurham/Walker\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    render_mode=\"human\",\n",
    "    # render_mode=\"rgb_array\",\n",
    "    hardcore=False,\n",
    "    # hardcore=True,\n",
    ")\n",
    "_, obs, info = rld.seed_everything(42, env)\n",
    "# track statistics for plotting\n",
    "tracker2 = rld.InfoTracker()\n",
    "\n",
    "heuristics = BipedalWalkerHeuristics()\n",
    "\n",
    "act = heuristics.step_heuristic(obs)\n",
    "for i in range(500):\n",
    "    obs, rew, terminated, truncated, info = env.step(act)\n",
    "    act = heuristics.step_heuristic(obs)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    if env.render_mode == \"rgb_array\":\n",
    "        rld.render(env, clear=True)\n",
    "\n",
    "    # track and plot statistics\n",
    "    tracker2.track(info)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        tracker2.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
