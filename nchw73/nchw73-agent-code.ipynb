{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# FOR .py FILE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "# **Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSUGAJsTLOV",
    "outputId": "875609e2-e3eb-4b29-c03c-1989820b1ea7"
   },
   "outputs": [],
   "source": [
    "# https://github.com/robert-lieck/rldurham/tree/main - rldurham source\n",
    "\n",
    "# !pip install swig\n",
    "# !pip install --upgrade rldurham\n",
    "\n",
    "# !pip install wandb\n",
    "# !pip install torchinfo\n",
    "# !pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AFrqd24JTLOW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtheo-farrell99\u001b[0m (\u001b[33mtheo-farrell99-durham-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.distributions as D\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "\n",
    "import rldurham as rld\n",
    "\n",
    "import wandb\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"x\" # TODO remove\n",
    "wandb.login()\n",
    "\n",
    "# from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint setup\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, \"training_checkpoint.pth\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "# **RL agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TIMESTEPS = 2000 # [DONT CHANGE]\n",
    "SOURCE_ID = '' # mac, ncc, colab -> for personal id in wandb\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint helper fns\n",
    "\n",
    "def save_checkpoint(state, filename=CHECKPOINT_FILE):\n",
    "    \"\"\"Saves the current training state.\"\"\"\n",
    "    print(f\"Saving checkpoint to {filename}...\")\n",
    "    torch.save(state, filename)\n",
    "    # Optionally, save replay buffer separately if it's very large\n",
    "    # np.savez_compressed(filename + \"_buffer.npz\",\n",
    "    #                     state=state['replay_buffer']['state'],\n",
    "    #                     action=state['replay_buffer']['action'],\n",
    "    #                     next_state=state['replay_buffer']['next_state'],\n",
    "    #                     reward=state['replay_buffer']['reward'],\n",
    "    #                     not_done=state['replay_buffer']['not_done'])\n",
    "    print(\"Checkpoint saved.\")\n",
    "\n",
    "def load_checkpoint(filename=CHECKPOINT_FILE):\n",
    "    \"\"\"Loads training state from a checkpoint file.\"\"\"\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"Loading checkpoint from {filename}...\")\n",
    "        checkpoint = torch.load(filename)\n",
    "        # Optionally load large replay buffer separately\n",
    "        # if os.path.isfile(filename + \"_buffer.npz\"):\n",
    "        #     print(\"Loading replay buffer data...\")\n",
    "        #     buffer_data = np.load(filename + \"_buffer.npz\")\n",
    "        #     checkpoint['replay_buffer_data'] = buffer_data\n",
    "        # else:\n",
    "        #     print(\"Warning: Replay buffer data file not found.\")\n",
    "        #     checkpoint['replay_buffer_data'] = None # Indicate buffer data is missing\n",
    "        print(\"Checkpoint loaded.\")\n",
    "        return checkpoint\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {filename}, starting from scratch.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERs\n",
    "# for https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def train_on_environment(actor, env, grad_step_class, replay_buffer, max_timesteps,\n",
    "    state, batch_size, total_steps, sequence_length,\n",
    "    use_intrinsic_reward=False, last_dreamer_test_loss=0.0, intrinsic_r_scale=0.0,\n",
    "    ):\n",
    "    '''\n",
    "    abstracted training loop function for both real & dreamer environments to use\n",
    "\n",
    "    the real environment will always have MAX_TIMESTEPS as the max_timesteps value,\n",
    "    while the dreamer enviroment might have less timesteps TODO could it also have MAX_TIMESTEPS?\n",
    "    '''\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "\n",
    "    state_dim = actor.state_dim\n",
    "    act_dim = actor.action_dim\n",
    "    input_dim = state_dim + act_dim + state_dim + 1 + 1 # state, action, next_state, reward, done\n",
    "    \n",
    "    # save start sequence for the dreamer model\n",
    "    input_buffer = torch.empty((0, input_dim), dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    # initial state float32 for mps\n",
    "    state = state.astype(np.float32)\n",
    "\n",
    "    for t in range(max_timesteps): \n",
    "        total_steps += 1\n",
    "        # select action and step the env\n",
    "        action = actor.select_action(state)\n",
    "        step = env.step(action)\n",
    "\n",
    "        # handle different return formats\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, info = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, info = step\n",
    "\n",
    "        # float32 before processing/storing\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        reward = float(reward)\n",
    "        done = float(done) #Â keep done as float for consistency\n",
    "\n",
    "        ep_timesteps += 1\n",
    "        ep_reward += reward # extrinsic reward in ep_reward\n",
    "\n",
    "        if max_timesteps == MAX_TIMESTEPS: # TODO could we be in dreamer for this?\n",
    "            if reward == -100.0:\n",
    "                reward = -10.0 #Â bipedal walker does -100 for hitting floor, so -10 might make it more stable\n",
    "            else:\n",
    "                reward *= 2 # TODO why might this be justified? (encouraging forward motion?)\n",
    "\n",
    "            combined_r = reward\n",
    "            if use_intrinsic_reward and last_dreamer_test_loss > 0:\n",
    "                r_intr = intrinsic_r_scale * last_dreamer_test_loss\n",
    "                combined_r += r_intr\n",
    "                wandb.log({\"Intrinsic Reward Added\": r_intrinsic}, commit=False) # log to wandb (commit = false lets us log things later in the ep)\n",
    "\n",
    "            replay_buffer.add(state, action, next_state, combined_r, done)\n",
    "        else:\n",
    "            if t == sequence_length:\n",
    "                for row in input_buffer.cpu().numpy():\n",
    "                    replay_buffer.add(\n",
    "                        row[:state_dim], # state row[:24]\n",
    "                        row[state_dim:state_dim+act_dim], # action row[24:28]\n",
    "                        row[state_dim+act_dim:state_dim+act_dim+state_dim], # next_state row[28:52]\n",
    "                        row[-2], # reward row[52]\n",
    "                        row[-1] # done row[53]\n",
    "                    )\n",
    "                # add the last step to the buffer\n",
    "                replay_buffer.add(state, action, next_state, reward, done)\n",
    "            elif t > sequence_length:\n",
    "                # add last step directly\n",
    "                replay_buffer.add(state, action, next_state, reward, done)\n",
    "            # don't store if simulation ends without reaching the sequence length\n",
    "        \n",
    "        if t < sequence_length: # store in input buffer for dreamer start seq\n",
    "            combined_np = np.concatenate((\n",
    "                state, \n",
    "                action.astype(np.float32), \n",
    "                next_state, \n",
    "                np.array([reward], dtype=np.float32), \n",
    "                np.array([done], dtype=np.float32)\n",
    "            ), axis=0)\n",
    "            combined = torch.tensor(combined_np, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "            input_buffer = torch.cat([input_buffer, combined], axis=0)\n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "        if total_steps >= batch_size: #Â do 1 training update using 1 batch from buffer if enough\n",
    "            # train the agent using experiences from the real environment\n",
    "            grad_step_class.take_gradient_step(replay_buffer, total_steps, batch_size)\n",
    "    \n",
    "        if done: # break if finished\n",
    "            break\n",
    "\n",
    "    # only return extrinsic to eval based on that\n",
    "    if sequence_length > 0: # TODO - this means...? \n",
    "        return ep_timesteps, ep_reward, input_buffer.to(torch.float32), info\n",
    "    return ep_timesteps, ep_reward, None, info\n",
    "\n",
    "\n",
    "# test loop for agent on environment\n",
    "# def simulate_on_environment(actor, env, max_timesteps, state):\n",
    "#     ep_timesteps = 0\n",
    "#     ep_reward = 0\n",
    "#     for t in range(max_timesteps):\n",
    "#         action = actor.select_action(state)\n",
    "#         step = env.step(action)\n",
    "#         if len(step) == 5:\n",
    "#             next_state, reward, term, tr, _ = step\n",
    "#             done = term or tr\n",
    "#         else:\n",
    "#             next_state, reward, done, _ = step\n",
    "#         ep_timesteps += 1\n",
    "    \n",
    "#         state = next_state\n",
    "#         ep_reward += reward\n",
    "    \n",
    "#         if done or t == max_timesteps - 1:\n",
    "#             break\n",
    "#     return ep_timesteps, ep_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# generate value array from replay buffer\n",
    "def retrieve_from_replay_buffer(replay_buffer, ptr):\n",
    "    '''\n",
    "    returns (state, action, reward, done, next_state)\n",
    "    '''\n",
    "    return np.concatenate((\n",
    "            replay_buffer.state[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.action[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.reward[ptr:replay_buffer.ptr],\n",
    "            1. - replay_buffer.not_done[ptr:replay_buffer.ptr],\n",
    "            replay_buffer.next_state[ptr:replay_buffer.ptr],                \n",
    "        ), \n",
    "        axis = 1 # along the columns (so each row is a memory)\n",
    "    )\n",
    "\n",
    "# function to create sequences with a given window size and step size\n",
    "def create_sequences(memories, window_size, step_size):\n",
    "    '''\n",
    "    Function to create sequences of memories from the replay buffer.\n",
    "\n",
    "    Each sequence is of length window_size and each spaced apart by step_size\n",
    "    '''\n",
    "    n_memories = memories.shape[0] #Â just the number of time steps currently in the buffer\n",
    "\n",
    "    # calc number of seq (of len window_size) we can create from mems available  \n",
    "    n_sequences = math.floor((n_memories - window_size) / step_size) + 1 # +1 because indices start at 0\n",
    "\n",
    "    sequences = np.zeros((n_sequences, window_size, memories.shape[1]))\n",
    "    for i in range(n_sequences):\n",
    "        start_idx = i * step_size # idx to start seq from\n",
    "        sequences[i, :] = memories[start_idx:start_idx + window_size, :] # grab seq of memories window_size long from start_idx\n",
    "    return sequences\n",
    "\n",
    "def gen_test_train_seq(replay_buffer, train_set, test_set, train_split, window_size, step_size, ptr):\n",
    "    '''\n",
    "    Function to split train and test data\n",
    "    '''\n",
    "    memories = retrieve_from_replay_buffer(replay_buffer, ptr)\n",
    "    try:\n",
    "        memory_sequences = create_sequences(memories, window_size, step_size)\n",
    "        n_sequences = memory_sequences.shape[0]\n",
    "    except: # TODO How might this go wrong? not enough new data to create a sequence?\n",
    "        return train_set, test_set\n",
    "\n",
    "    # shuffle the sequences & split\n",
    "    indices = np.arange(n_sequences)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(train_split * n_sequences) #Â get split point \n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "\n",
    "    if train_set is None: #Â if this is first train/test set, create the sets\n",
    "        return memory_sequences[train_indices, :], memory_sequences[test_indices, :]\n",
    "    # else just add to existing sets\n",
    "    return np.concatenate((train_set, memory_sequences[train_indices, :]), axis=0), np.concatenate((test_set, memory_sequences[test_indices, :]), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dreamer_eps(dreamer_avg_loss, score_threshold):\n",
    "    '''\n",
    "    Get number of dreamed episdoes the agent should run on the dreamer model.\n",
    "\n",
    "    Only use dreamer for training if it's sufficiently accurate model of env.\n",
    "\n",
    "    Note - score_threshold is between 0 and 1.\n",
    "\n",
    "    Disclaimer - calculation from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    '''\n",
    "    max_dreamer_it = 10 # a perfect dreamer has this many eps\n",
    "\n",
    "    if dreamer_avg_loss >= score_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        norm_score = dreamer_avg_loss/score_threshold # normalise score relative to threshold\n",
    "        inv_score = 1 - norm_score # invert so that closer to 1 ==> dreamer score much better than threshold\n",
    "        sq_score = inv_score**2 # square so that num iter increases quadratically as accuracy improves\n",
    "        return int(max_dreamer_it * sq_score) # scale so that max iterations is 10 (when dreamer very accurate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9kTeUHYL7gU6"
   },
   "outputs": [],
   "source": [
    "class EREReplayBuffer(object):\n",
    "    '''\n",
    "    implementation - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    An ERE buffer is implemented to improve sample efficiency, \n",
    "        for which the paper can be found here: https://arxiv.org/abs/1906.04009\n",
    "\n",
    "    Rationale:\n",
    "    - actions from early in training are likely v different to optimal, so want to prioritise recent ones so we're not constantly learning from crap ones\n",
    "    - as time goes on, they get more similar so annealing eta allows uniform sampling later on\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, T, max_size, eta, cmin):\n",
    "        self.max_size, self.ptr, self.size, self.rollover = max_size, 0, 0, False\n",
    "        self.eta0 = eta\n",
    "        self.cmin = cmin\n",
    "        self.c_list = []\n",
    "        self.index = []\n",
    "        self.T = T\n",
    "\n",
    "        self.reward = np.empty((max_size, 1), dtype=np.float32) #Â float32 for mps\n",
    "        self.state = np.empty((max_size, state_dim), dtype=np.float32)\n",
    "        self.action = np.empty((max_size, action_dim), dtype=np.float32)\n",
    "        self.not_done = np.empty((max_size, 1), dtype=np.float32)\n",
    "        self.next_state = np.empty((max_size, state_dim), dtype=np.float32)\n",
    "        \n",
    "    def sample(self, batch_size, t):\n",
    "        # update eta value for current timestep\n",
    "        eta = self.eta_anneal(t)\n",
    "\n",
    "        index = np.array([self._get_index(eta, k, batch_size) for k in range(batch_size)])\n",
    "\n",
    "        r = torch.tensor(self.reward[index], dtype=torch.float32).to(DEVICE)\n",
    "        s = torch.tensor(self.state[index], dtype=torch.float32).to(DEVICE)\n",
    "        ns = torch.tensor(self.next_state[index], dtype=torch.float32).to(DEVICE)\n",
    "        a = torch.tensor(self.action[index], dtype=torch.float32).to(DEVICE)\n",
    "        nd = torch.tensor(self.not_done[index], dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        return s, a, ns, r, nd\n",
    "    \n",
    "    def _get_index(self, eta, k, batch_size):\n",
    "        c_calc = self.size * eta ** (k * 1000 / batch_size)\n",
    "        ck = c_calc if c_calc > self.cmin else self.size\n",
    "\n",
    "        if not self.rollover: # if we're not overwriting yet, ...\n",
    "            return np.random.randint(self.size - ck, self.size)\n",
    "        \n",
    "        return np.random.randint(self.ptr + self.size - ck, self.ptr + self.size) % self.size\n",
    "\n",
    "    def eta_anneal(self, t):\n",
    "        # eta anneals over time --> 1, which reduces emphasis on recent experiences over time\n",
    "        return min(1.0, self.eta0 + (1 - self.eta0) * t / self.T)\n",
    " \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        # Add experience to replay buffer \n",
    "        self.state[self.ptr] = state.astype(np.float32)\n",
    "        self.action[self.ptr] = action.astype(np.float32)\n",
    "        self.next_state[self.ptr] = next_state.astype(np.float32)\n",
    "        self.reward[self.ptr] = float(reward)\n",
    "        self.not_done[self.ptr] = 1. - float(done)\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.ptr %= self.max_size\n",
    "\n",
    "        # increase size counter until full, then start overwriting (rollover)\n",
    "        if self.max_size > self.size + 1:\n",
    "          self.size += 1\n",
    "        else:\n",
    "          self.size = self.max_size\n",
    "          self.rollover = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dreamer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamerAgent(nn.Module):\n",
    "    '''\n",
    "    Dreamer agent.\n",
    "    Uses a transformer model to predict the next state, reward and done signal given the current (state, action, reward, done)\n",
    "\n",
    "    Acknowledgements:\n",
    "    - implementation based on https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    - (from implementation designer) key ideas and concepts for the auto-regressive transformer design stem from this paper: https://arxiv.org/abs/2202.09481\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, seq_len, num_layers, num_heads, dropout_prob, lr):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.input_dim = state_dim + action_dim + 2 # (state, action, reward, done)\n",
    "        self.target_dim = self.state_dim + self.action_dim # (state, action)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.input_fc = nn.Linear(self.input_dim, hidden_dim).to(DEVICE)\n",
    "        self.target_fc = nn.Linear(self.target_dim, hidden_dim).to(DEVICE)\n",
    "        self.transformer = nn.Transformer( # uses transformer model!\n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            num_heads, \n",
    "            dropout=dropout_prob,\n",
    "            activation=F.gelu,\n",
    "            batch_first=True\n",
    "        ).to(DEVICE)\n",
    "        self.output_next_state = nn.Linear(hidden_dim + self.target_dim, state_dim).to(DEVICE)\n",
    "        self.output_reward = nn.Linear(hidden_dim + self.target_dim, 1).to(DEVICE)\n",
    "        self.output_done = nn.Linear(hidden_dim + self.target_dim, 1).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    # separate out the ground truth variables and compare against predictions\n",
    "    def loss_fn(self, output_next_state, output_reward, output_done, ground_truth):\n",
    "        reward, done, next_state = torch.split(ground_truth, [1, 1, self.state_dim], dim=-1)\n",
    "        loss = self.mse_loss(output_next_state[:, -1], next_state)\n",
    "        loss += self.mse_loss(output_reward[:, -1], reward)\n",
    "        # loss += self.ce_loss(output_done[:, -1], done)\n",
    "        loss += self.bce_loss(output_done[:, -1], done.float())\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # separate the input and target tensors\n",
    "        target = input_tensor[:, -1, :self.target_dim].unsqueeze(1)\n",
    "        encoded_target = self.target_fc(target)\n",
    "        encoded_input = self.input_fc(input_tensor[:, :-1, :self.input_dim])\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target], axis=2))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target], axis=2))\n",
    "        output_done = self.output_done(torch.cat([encoded_output, target], axis=2)) #Â don't need sigmoid because nn.CrossEntropy already does it? (or can use BCE loss after sigmoid)\n",
    "        # output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target], axis=2)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "    \n",
    "    def predict(self, input_tensor, target_tensor):\n",
    "        # separate the input and target tensors\n",
    "        encoded_target = self.target_fc(target_tensor)\n",
    "        encoded_input = self.input_fc(input_tensor)\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_done = torch.sigmoid(self.output_done(torch.cat([encoded_output, target_tensor], axis=1)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "\n",
    "    # transformer training loop\n",
    "    # sequences shape: (batch, sequence, features)\n",
    "    def train_dreamer(self, sequences, epochs, batch_size=256):\n",
    "        print(\"Training Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float).to(DEVICE)\n",
    "        \n",
    "        train_dataset = TensorDataset(inputs)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(train_dataloader):\n",
    "                input_batch = input_batch[0].to(DEVICE)\n",
    "                self.optimizer.zero_grad()\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, running_loss / len(train_dataloader)))\n",
    "\n",
    "    # transformer testing loop\n",
    "    def test_dreamer(self, sequences, batch_size=64):\n",
    "        print(\"Testing Dreamer...\")\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float).to(DEVICE)\n",
    "        \n",
    "        test_dataset = TensorDataset(inputs)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(test_dataloader):\n",
    "                input_batch = input_batch[0].to(DEVICE)\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                running_loss += loss.item()\n",
    "            print('Test Loss: {:.4f}'.format(running_loss / len(test_dataloader)))\n",
    "        return running_loss / len(test_dataloader)\n",
    "    \n",
    "    def step(self, action):\n",
    "        act = torch.tensor(np.array([action]), dtype=torch.float32).to(DEVICE)\n",
    "        self.actions = torch.cat([self.actions, act], axis=0)\n",
    "\n",
    "        input_sequence = torch.cat([self.states[:-1], self.actions[:-1], self.rewards, self.dones], axis=1).to(torch.float32)\n",
    "        target = torch.cat([self.states[-1], self.actions[-1]], axis=0).unsqueeze(0).to(torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state, reward, done = self.predict(input_sequence, target)\n",
    "            next_state = next_state.to(torch.float32)\n",
    "            reward = reward.to(torch.float32)\n",
    "            done = (done >= 0.6).to(torch.float32) # bias towards not done to avoid false positives\n",
    "\n",
    "            self.states = torch.cat([self.states, next_state], axis=0)\n",
    "            self.rewards = torch.cat([self.rewards, reward], axis=0)\n",
    "            self.dones = torch.cat([self.dones, done], axis=0)\n",
    "            \n",
    "            # trim sequences\n",
    "            if self.states.shape[0] > self.seq_len:\n",
    "                self.states = self.states[1:]\n",
    "                self.rewards = self.rewards[1:]\n",
    "                self.dones = self.dones[1:]\n",
    "            if self.actions.shape[0] > self.seq_len - 1: # action seq shorter\n",
    "                self.actions = self.actions[1:]\n",
    "        \n",
    "        return next_state.squeeze(0).cpu().numpy(), reward.cpu().item(), done.cpu().item(), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actor-critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS\n",
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def quantile_huber_loss(quantiles, samples, sum_over_quantiles = False):\n",
    "    '''\n",
    "    From TQC (see paper p3)\n",
    "    Specific implementation: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/utils.py#L8\n",
    "\n",
    "    Huber loss is less sensitive to outliers than MSE\n",
    "\n",
    "    samples: (batch_size, 1, n_target_quantiles) -> (batch_size, 1, 1, n_target_quantiles)\n",
    "    quantiles: (batch_size, n_critics, n_quantiles) -> (batch_size, n_critics, n_quantiles, 1)\n",
    "    pairwise_delta: (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    '''\n",
    "    # uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise\n",
    "    delta = samples[:, np.newaxis, np.newaxis, :] - quantiles[:, :, :, np.newaxis]  \n",
    "    abs_delta = torch.abs(delta)\n",
    "    huber_loss = torch.where(abs_delta > 1, abs_delta - 0.5, delta ** 2 * 0.5)\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "\n",
    "    # cumulative probabilities to calc quantiles\n",
    "    cum_prob = (torch.arange(n_quantiles, device=quantiles.device, dtype=torch.float) + 0.5) / n_quantiles\n",
    "    cum_prob = cum_prob.view(1, 1, -1, 1) # quantiles has shape (batch_size, n_critics, n_quantiles), so make cum_prob broadcastable to (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    \n",
    "    loss = (torch.abs(cum_prob - (delta < 0).float()) * huber_loss)\n",
    "\n",
    "    # Summing over the quantile dimension \n",
    "    if sum_over_quantiles:\n",
    "        loss = loss.sum(dim=-2).mean()\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# MLP for actor that implements D2RL architecture\n",
    "class ActorMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim, output size = action dim\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "    # input_dim_ = input_dim\n",
    "    # num_inputs = 24 # input_dim\n",
    "    # input_dim = hidden_dims[0] + num_inputs\n",
    "\n",
    "    current_dim = input_dim # first layer has input dim = state dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim # prev layers output + original input size\n",
    "      # if i == 0:\n",
    "      #   lay = nn.Linear(input_dim_, next_size)\n",
    "      # else:\n",
    "      #   lay = nn.Linear(input_dim, next_size)\n",
    "      # self.add_module(f'layer{i}', lay)\n",
    "      # self.layer_list.append(lay)\n",
    "      # input_dim_ = next_size\n",
    "        \n",
    "    # Final layer input dim is last hidden layer output + original input size\n",
    "    self.last_layer_mean_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "    self.last_layer_log_std_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim=1) # cat with output layer\n",
    "\n",
    "    mean_linear = self.last_layer_mean_linear(curr)\n",
    "    log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "    return mean_linear, log_std_linear\n",
    "\n",
    "# MLP for critic that implements D2RL architecture \n",
    "class CriticMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim + action dim, output size = n_quantiles\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    current_dim = input_dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim\n",
    "\n",
    "    self.last_layer = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim = 1)\n",
    "      \n",
    "    output = self.last_layer(curr)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh dist for actor to sample actions from \n",
    "\n",
    "# FROM ORIGINAL CODE https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# class TanhNormal(D.Distribution):\n",
    "#   def __init__(self, normal_mean, normal_std):\n",
    "#     super().__init__()\n",
    "#     self.normal_mean = normal_mean\n",
    "#     self.normal_std = normal_std\n",
    "#     self.normal = D.Normal(normal_mean, normal_std)\n",
    "#     self.stand_normal = D.Normal(torch.zeros_like(self.normal_mean), torch.ones_like(self.normal_std))        \n",
    "      \n",
    "#   def logsigmoid(tensor):\n",
    "#     denominator = 1 + torch.exp(-tensor)\n",
    "#     return torch.log(1/ denominator)\n",
    "\n",
    "#   def log_probability(self, pre_tanh):\n",
    "#     final = (self.normal.log_prob(pre_tanh)) - (2 * np.log(2) + F.logsigmoid(2 * pre_tanh) + F.logsigmoid(-2 * pre_tanh))\n",
    "#     return final\n",
    "\n",
    "#   def random_sample(self):\n",
    "#     pretanh = self.normal_mean + self.normal_std * self.stand_normal.sample()\n",
    "#     return torch.tanh(pretanh), pretanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class GradientStep(object):\n",
    "  '''\n",
    "  see (D2RL) https://github.com/pairlab/d2rl/blob/main/sac/sac.py\n",
    "  and (TQC) https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/tqc/tqc.py\n",
    "  '''\n",
    "  def __init__(self,*,\n",
    "    actor, critic, critic_target, discount, tau,\n",
    "    actor_lr, critic_lr, alpha_lr,\n",
    "    n_quantiles, n_mini_critics, top_quantiles_to_drop_per_net, target_entropy,\n",
    "    ):\n",
    "\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.critic_target = critic_target\n",
    "\n",
    "    self.log_alpha = nn.Parameter(torch.zeros(1).to(DEVICE)) # log alpha is learned\n",
    "    self.quantiles_total = n_quantiles * n_mini_critics\n",
    "    \n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "    self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    self.discount, self.tau = discount, tau\n",
    "    self.top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_mini_critics # total number of quantiles to drop\n",
    "    self.target_entropy = target_entropy\n",
    "\n",
    "  def take_gradient_step(self, replay_buffer, total_steps, batch_size=256):\n",
    "    # Sample batch from replay buffer\n",
    "    state, action, next_state, reward, not_done = replay_buffer.sample(batch_size, total_steps)\n",
    "    alpha = torch.exp(self.log_alpha) # entropy temperature coefficient\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # sample new action from actor on next state\n",
    "      new_next_action, next_log_pi = self.actor(next_state)\n",
    "\n",
    "      # Compute and cut quantiles at the next state\n",
    "      next_z = self.critic_target(next_state, new_next_action)  \n",
    "      \n",
    "      # Sort and drop top k quantiles to control overestimation (TQC)\n",
    "      sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "      sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop] # estimated truncated Q-val dist for next state\n",
    "\n",
    "      # td error + entropy term\n",
    "      target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "    \n",
    "    # Get current quantile estimates using action from the replay buffer\n",
    "    cur_z = self.critic(state, action)\n",
    "    critic_loss = quantile_huber_loss(cur_z, target)\n",
    "\n",
    "    new_action, log_pi = self.actor(state)\n",
    "    # detach the variable from the graph so we don't change it with other losses\n",
    "    alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean() # as in D2RL implementation for auto entropy tuning\n",
    "\n",
    "    # Optimise critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # Soft update target networks\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Compute actor (Ï€) loss\n",
    "    # JÏ€ = ð”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\n",
    "    actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "    # ^ mean(2) is over quantiles, mean(1) is over critic ensemble\n",
    "    \n",
    "    # Optimise the actor\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # Optimise the entropy coefficient\n",
    "    self.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    self.alpha_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[512, 512]):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mlp = ActorMLP(state_dim, hidden_dims, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.mlp(obs)\n",
    "        log_std = log_std.clamp(-20, 2) # clamp for stability\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        base_N_dist = D.Normal(mean, std) # base normal dist\n",
    "        tanh_transform = TanhTransform(cache_size=1) # transform to get the tanh dist\n",
    "\n",
    "        log_prob = None\n",
    "        if self.training: # i.e. agent.train()\n",
    "            transformed_dist = D.TransformedDistribution(base_N_dist, tanh_transform) # transformed distribution\n",
    "            action = transformed_dist.rsample() # samples from base dist & applies transform\n",
    "            log_prob = transformed_dist.log_prob(action) # log prob of action after transform\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True) # sum over action dim\n",
    "        else: # evaluation mode\n",
    "            action = torch.tanh(mean)\n",
    "\n",
    "        # FROM ORIGINAL CODE using custom tanh dist\n",
    "        # if self.training:\n",
    "        #     tanh_dist = TanhNormal(mean, std) #Â use custom tanh dist\n",
    "        #     action, pre_tanh = tanh_dist.random_sample()\n",
    "        #     log_prob = tanh_dist.log_probability(pre_tanh)\n",
    "        #     log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        # else:\n",
    "        #     action = torch.tanh(mean)\n",
    "            \n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).to(DEVICE)\n",
    "        if obs.ndim == 1: # add batch dim if missing\n",
    "             obs = obs.unsqueeze(0)\n",
    "        act, _ = self.forward(obs)\n",
    "        return np.array(act[0].cpu().detach())\n",
    "\n",
    "\n",
    "class Critic(nn.Module): # really a mega-critic from lots of mini-critics\n",
    "    '''\n",
    "    Ensemble of critics for TQC\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.critics = nn.ModuleList()\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        for _ in range(n_nets): # multiple critic mlps\n",
    "            net = CriticMLP(state_dim + action_dim, hidden_dims, n_quantiles)\n",
    "            self.critics.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # cat state and action (to pass to critic)\n",
    "        state_act = torch.cat((state, action), dim=1)\n",
    "\n",
    "        # get quantiles from each critic mlp\n",
    "        quantiles = [critic(state_act) for critic in self.critics]\n",
    "        quantiles = torch.stack(quantiles, dim=1) # stack into tensor\n",
    "        return quantiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "H466HICfpDB1"
   },
   "outputs": [],
   "source": [
    "# XinJingHao's (some of) hyperparams\n",
    "# https://github.com/XinJingHao/TD3-Pytorch/blob/main/TD3.py\n",
    "\n",
    "# params = {\n",
    "#   'eval_interval' : int(2e3),  # Model evaluating interval, in steps\n",
    "#   'delay_freq' : 1,  # Delayed frequency for Actor and Target Net\n",
    "#   'explore_noise' : 0.15,  # Exploring noise when interacting\n",
    "#   'explore_noise_decay' : 0.998,  # Decay rate of explore noise\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My hyperparams\n",
    "\n",
    "seed = 42 # DONT CHANGE FOR COURSEWORK\n",
    "# use_wandb = False # use wandb for logging\n",
    "\n",
    "hyperparams = {\n",
    "    # env/general params\n",
    "    \"max_timesteps\": MAX_TIMESTEPS, # per episode [DONT CHANGE]\n",
    "    \"max_episodes\": 1000,\n",
    "    \"target_score\": 300, # stop training when average score over r_list > target_score\n",
    "    \"len_r_list\": 100, # length of reward list to average over for target score\n",
    "    \"hardcore\": False, #Â fixed in wandb sweep\n",
    "\n",
    "    # Agent hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "    \"n_mini_critics\": 5, # each mini-critic is a single mlp, which combine to make one mega-critic\n",
    "    \"n_quantiles\": 25, # quantiles per mini critic\n",
    "    \"top_quantiles_to_drop_per_net\": 'auto', # per mini critic (auto based on n_quantiles)\n",
    "    \"actor_hidden_dims\": [512, 512],\n",
    "    \"mini_critic_hidden_dims\": [256, 256], # * n_mini_critics\n",
    "    \"batch_size\": 256,\n",
    "    \"discount\": 0.98, # gamma\n",
    "    \"tau\": 0.005,\n",
    "    \"actor_lr\": 3e-4,\n",
    "    \"critic_lr\": 3e-4,\n",
    "    \"alpha_lr\": 3e-4,\n",
    "\n",
    "    # ERE buffer (see paper for their choices)\n",
    "    \"buffer_size\": 500000, # smaller size improves learning early on but is outperformed later on\n",
    "    \"eta0\": 0.994, # 0.994 - 0.999 is good (according to paper)\n",
    "    \"annealing_steps\": 'auto', # number of steps to anneal eta over (after which sampling is uniform) - None = auto-set to max estimated steps in training\n",
    "    \"cmin\": 5000, # min number of samples to sample from\n",
    "\n",
    "    # dreamer hyperparams (from https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb)\n",
    "    # info on hyperparam choice: https://arxiv.org/abs/1912.01603\n",
    "    \"use_dreamer\": True,\n",
    "    \"intrinsic_reward_scale\": 0.0, # scale factor for dreamer intrinsic reward\n",
    "    \"batch_size_dreamer\": 512,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 16,\n",
    "    \"num_heads\": 4,\n",
    "    \"dreamer_lr\": 3e-4,\n",
    "    \"dropout_prob\": 0.1,\n",
    "    \"window_size\": 40,               # transformer context window size\n",
    "    \"step_size\": 1,                  # how many timesteps to skip between each context window\n",
    "    \"train_split\": 0.8,              # train/validation split\n",
    "    \"score_threshold\": 0.8,          # use dreamer if loss < score_threshold\n",
    "    \"auto_horizon\": False,           # True: sets imagination_horizon to min (imagination_horizon, last episode length)\n",
    "    \"imagination_horizon\": 50,       # how many timesteps to run the dreamer model for (H in Dreamer paper) - need to empirically test for best\n",
    "    \"dreamer_train_epochs\": 15,      # how many epochs to train the dreamer model for\n",
    "    \"dreamer_train_frequency\": 10,   # how often to train the dreamer model\n",
    "    \"episode_threshold\": 50,         # how many episodes to run before training the dreamer model\n",
    "    \"max_size\": int(5e4),            # maximum size of the training set for the dreamer model\n",
    "}\n",
    "\n",
    "# recording/logging\n",
    "plot_interval = 25 # plot every Nth episode (wandb still plots every ep)\n",
    "save_fig = True # save figures too\n",
    "\n",
    "is_recording = True\n",
    "if hyperparams['hardcore']: # NOTE this doesnt change in wandb sweep\n",
    "    video_interval = 30 # record every Nth episode\n",
    "    ep_start_rec = 300 # start recording on this episode\n",
    "else:\n",
    "    video_interval = 20\n",
    "    ep_start_rec = 50\n",
    "\n",
    "use_checkpointing = False # enable/disable checkpointing (NOTE useful for hardcore. Delete in terminal w/ rm -rf ./checkpoints)\n",
    "checkpoint_interval = 30 # save checkpoint every N episodes\n",
    "\n",
    "if hyperparams['annealing_steps'] == 'auto':\n",
    "    hyperparams['annealing_steps'] = hyperparams['max_episodes']*hyperparams['max_timesteps'] # max est number of steps in training\n",
    "if hyperparams['top_quantiles_to_drop_per_net'] == 'auto':\n",
    "    hyperparams['top_quantiles_to_drop_per_net'] = int(hyperparams['n_quantiles'] // 12.5) # keep ratio same as M=25 d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP\n",
    "\n",
    "# wandb & checkpoints\n",
    "\n",
    "# load checkpoint\n",
    "loaded_checkpoint = None\n",
    "wandb_run_id = None\n",
    "start_episode = 1 # Default start episode\n",
    "if use_checkpointing:\n",
    "    loaded_checkpoint = load_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        wandb_run_id = loaded_checkpoint.get('wandb_run_id')\n",
    "        # Load hyperparameters from checkpoint if needed, or ensure consistency\n",
    "        # config = loaded_checkpoint['config'] # Be careful with overriding wandb.config\n",
    "        start_episode = loaded_checkpoint['episode'] + 1 # Start from the next episode\n",
    "        print(f\"Resuming from episode {start_episode}\")\n",
    "\n",
    "# init wandb run\n",
    "import datetime\n",
    "suffix = '_hardcore' if hyperparams['hardcore'] else ''\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "run_name = SOURCE_ID + f\"_{timestamp}\" + suffix\n",
    "\n",
    "wandb_init_kwargs = {\n",
    "    \"project\": \"RL-Coursework-Walker2d\",\n",
    "    \"config\": hyperparams,\n",
    "    \"name\": run_name,\n",
    "    \"save_code\": True, # optionally saves code to wandb\n",
    "}\n",
    "if wandb_run_id:\n",
    "    wandb_init_kwargs[\"id\"] = wandb_run_id\n",
    "    wandb_init_kwargs[\"resume\"] = \"allow\" # Or \"must\" if you require resuming\n",
    "\n",
    "wandb.init(**wandb_init_kwargs)\n",
    "\n",
    "config = wandb.config # use wandb.config to access hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1Xrcek4hxDXl",
    "outputId": "6161f148-a164-4e22-d97a-f9aa53e7b8b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/wandb/run-20250429_140406-uxqbjj58</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/uxqbjj58' target=\"_blank\">_2025-04-29_14-04-06_hardcore</a></strong> to <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d' target=\"_blank\">https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/uxqbjj58' target=\"_blank\">https://wandb.ai/theo-farrell99-durham-university/RL-Coursework-Walker2d/runs/uxqbjj58</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/Subspace_Explorer/Documents/Programming/RL-coursework/nchw73/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.3780703e-03, -3.3779870e-07,  2.4434172e-05, -1.6000001e-02,\n",
       "         9.8403029e-02, -2.1372112e-05,  8.5710919e-01,  1.0572739e-03,\n",
       "         1.0000000e+00,  3.9457139e-02, -2.1373975e-05,  8.5030973e-01,\n",
       "         1.7118502e-05,  1.0000000e+00,  4.3961498e-01,  4.4460747e-01,\n",
       "         4.6016768e-01,  4.8821858e-01,  5.3264999e-01,  6.0082233e-01,\n",
       "         7.0721996e-01,  8.8352203e-01,  1.0000000e+00,  1.0000000e+00],\n",
       "       dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make env\n",
    "# only attempt hardcore when your agent has solved the non-hardcore version\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=config.hardcore)\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "video_prefix = \"nchw73-agent-hardcore-video\" if config.hardcore else \"nchw73-agent-video\"\n",
    "video_prefix = f\"{timestamp}_\" + video_prefix #Â mark video with date/time to be mateched to run (remove later!)\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=video_prefix,          # prefix for videos\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "rld.check_device() # training on CPU recommended\n",
    "\n",
    "env.video = False # switch video recording off (only switch on every x episodes as this is slow)\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, state_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=seed)\n",
    "# rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWjENKkU_My"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, state, info = rld.seed_everything(seed, env)\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "replay_buffer = EREReplayBuffer(state_dim, act_dim, config.annealing_steps, config.buffer_size, config.eta0, config.cmin)\n",
    "\n",
    "actor = Actor(state_dim, act_dim, config.actor_hidden_dims).to(DEVICE)\n",
    "\n",
    "critic = Critic(state_dim, act_dim, config.n_quantiles, config.n_mini_critics, config.mini_critic_hidden_dims).to(DEVICE)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "dreamer = None #Â init as none\n",
    "if config.use_dreamer:\n",
    "    dreamer = DreamerAgent(state_dim, act_dim, config.hidden_dim, \n",
    "        config.window_size, config.num_layers, config.num_heads, config.dropout_prob, config.dreamer_lr).to(DEVICE)\n",
    "\n",
    "target_entropy = -np.prod(env.action_space.shape).item() # target entropy heuristic (= âˆ’dim(A) as in D2RL paper)\n",
    "grad_step_class = GradientStep(\n",
    "    actor=actor, critic=critic, critic_target=critic_target,\n",
    "    discount=config.discount, tau=config.tau,\n",
    "    actor_lr=config.actor_lr, critic_lr=config.critic_lr, alpha_lr=config.alpha_lr,\n",
    "    n_quantiles=config.n_quantiles, n_mini_critics=config.n_mini_critics,\n",
    "    top_quantiles_to_drop_per_net=config.top_quantiles_to_drop_per_net,\n",
    "    target_entropy=target_entropy,\n",
    "    )\n",
    "\n",
    "actor.train()\n",
    "\n",
    "total_steps = 0\n",
    "memory_ptr = 0 # marks boundary between new and old data in the replay buffer\n",
    "train_set, test_set = None, None\n",
    "recent_rewards= []\n",
    "ep_timesteps_dreamer = ep_reward_dreamer = dreamer_eps = 0\n",
    "last_dreamer_test_loss = 0.0 # track dreamer test loss\n",
    "\n",
    "success_ep = config.max_episodes # minimise i such that r_mean @ episode i >= target_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from checkpoint\n",
    "if loaded_checkpoint:\n",
    "    actor.load_state_dict(loaded_checkpoint['actor_state_dict'])\n",
    "    critic.load_state_dict(loaded_checkpoint['critic_state_dict'])\n",
    "    critic_target.load_state_dict(loaded_checkpoint['critic_target_state_dict']) # Load target critic state too\n",
    "    grad_step_class.actor_optimizer.load_state_dict(loaded_checkpoint['actor_optimizer_state_dict'])\n",
    "    grad_step_class.critic_optimizer.load_state_dict(loaded_checkpoint['critic_optimizer_state_dict'])\n",
    "    grad_step_class.alpha_optimizer.load_state_dict(loaded_checkpoint['alpha_optimizer_state_dict'])\n",
    "    grad_step_class.log_alpha.data = loaded_checkpoint['log_alpha'] # Load log_alpha value\n",
    "\n",
    "    if config.use_dreamer and dreamer is not None and 'dreamer_state_dict' in loaded_checkpoint:\n",
    "        dreamer.load_state_dict(loaded_checkpoint['dreamer_state_dict'])\n",
    "        dreamer.optimizer.load_state_dict(loaded_checkpoint['dreamer_optimizer_state_dict'])\n",
    "        train_set = loaded_checkpoint.get('train_set') # Use .get for optional keys\n",
    "        test_set = loaded_checkpoint.get('test_set')\n",
    "\n",
    "    # Load replay buffer state\n",
    "    replay_buffer.ptr = loaded_checkpoint['replay_buffer_ptr']\n",
    "    replay_buffer.size = loaded_checkpoint['replay_buffer_size']\n",
    "    replay_buffer.rollover = loaded_checkpoint['replay_buffer_rollover']\n",
    "    # If loading buffer data separately:\n",
    "    # buffer_data = loaded_checkpoint.get('replay_buffer_data')\n",
    "    # if buffer_data is not None:\n",
    "    #     replay_buffer.state[:replay_buffer.size] = buffer_data['state'][:replay_buffer.size]\n",
    "    #     replay_buffer.action[:replay_buffer.size] = buffer_data['action'][:replay_buffer.size]\n",
    "    #     replay_buffer.next_state[:replay_buffer.size] = buffer_data['next_state'][:replay_buffer.size]\n",
    "    #     replay_buffer.reward[:replay_buffer.size] = buffer_data['reward'][:replay_buffer.size]\n",
    "    #     replay_buffer.not_done[:replay_buffer.size] = buffer_data['not_done'][:replay_buffer.size]\n",
    "    # else: # If buffer data wasn't loaded, reset buffer state variables that depend on it\n",
    "    #     print(\"Warning: Replay buffer data not loaded, buffer content might be inconsistent.\")\n",
    "    #     # Decide how to handle this - maybe clear the buffer?\n",
    "    #     replay_buffer.ptr = 0\n",
    "    #     replay_buffer.size = 0\n",
    "    #     replay_buffer.rollover = False\n",
    "    # If saving buffer state directly in checkpoint (simpler, but larger file):\n",
    "    replay_buffer.state = loaded_checkpoint['replay_buffer_state']\n",
    "    replay_buffer.action = loaded_checkpoint['replay_buffer_action']\n",
    "    replay_buffer.next_state = loaded_checkpoint['replay_buffer_next_state']\n",
    "    replay_buffer.reward = loaded_checkpoint['replay_buffer_reward']\n",
    "    replay_buffer.not_done = loaded_checkpoint['replay_buffer_not_done']\n",
    "\n",
    "\n",
    "    total_steps = loaded_checkpoint['total_steps']\n",
    "    memory_ptr = loaded_checkpoint['memory_ptr']\n",
    "    recent_rewards = loaded_checkpoint['recent_rewards']\n",
    "    success_ep = loaded_checkpoint['success_ep']\n",
    "    if 'tracker_info' in loaded_checkpoint:\n",
    "        tracker.info = loaded_checkpoint['tracker_info']\n",
    "\n",
    "    # Clean up memory\n",
    "    del loaded_checkpoint\n",
    "    # if 'buffer_data' in locals(): del buffer_data\n",
    "    if DEVICE == torch.device(\"cuda\"):\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 1 | Timesteps: 82 | Reward: -117.139 | Total Steps: 82 | Dreamer: False\n",
      "Ep: 2 | Timesteps: 71 | Reward: -116.970 | Total Steps: 1.5e+02 | Dreamer: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m state\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m# float32 for mps\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# sample real env\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m ep_timesteps, ep_reward, input_buffer, info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_step_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_intrinsic_reward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_dreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# use intrinsic reward if dreamer active\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_dreamer_test_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dreamer_test_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintrinsic_r_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintrinsic_reward_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_timesteps\n\u001b[1;32m     23\u001b[0m tracker\u001b[38;5;241m.\u001b[39mtrack(info) \u001b[38;5;66;03m# track statistics for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 93\u001b[0m, in \u001b[0;36mtrain_on_environment\u001b[0;34m(actor, env, grad_step_class, replay_buffer, max_timesteps, state, batch_size, total_steps, sequence_length, use_intrinsic_reward, last_dreamer_test_loss, intrinsic_r_scale)\u001b[0m\n\u001b[1;32m     89\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size: \u001b[38;5;66;03m#Â do 1 training update using 1 batch from buffer if enough\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# train the agent using experiences from the real environment\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mgrad_step_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;66;03m# break if finished\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 63\u001b[0m, in \u001b[0;36mGradientStep.take_gradient_step\u001b[0;34m(self, replay_buffer, total_steps, batch_size)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Soft update target networks\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, target_param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[0;32m---> 63\u001b[0m   target_param\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Compute actor (Ï€) loss\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# JÏ€ = ð”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m (alpha \u001b[38;5;241m*\u001b[39m log_pi \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(state, new_action)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "episode = start_episode #Â =1 by default or whatever was in the checkpoint\n",
    "while episode <= config.max_episodes: # index from 1\n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    if is_recording and episode >= ep_start_rec:\n",
    "        env.info = episode % video_interval == 0   # track every x episodes (usually tracking every episode is fine)\n",
    "        env.video = episode % video_interval == 0  # record videos every x episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    state, info = env.reset()\n",
    "    state.astype(np.float32) # float32 for mps\n",
    "\n",
    "    # sample real env\n",
    "    ep_timesteps, ep_reward, input_buffer, info = train_on_environment(\n",
    "        actor, env, grad_step_class, replay_buffer, config.max_timesteps, state,\n",
    "        config.batch_size, total_steps, config.window_size,\n",
    "        use_intrinsic_reward=(config.use_dreamer and episode > config.episode_threshold), # use intrinsic reward if dreamer active\n",
    "        last_dreamer_test_loss=last_dreamer_test_loss,\n",
    "        intrinsic_r_scale=config.intrinsic_reward_scale,\n",
    "        )\n",
    "\n",
    "    total_steps += ep_timesteps\n",
    "    tracker.track(info) # track statistics for plotting\n",
    "\n",
    "    # update train/test sets for dreamer (add new eps and trim to window size)\n",
    "    train_set, test_set = gen_test_train_seq(\n",
    "        replay_buffer, train_set, test_set, config.train_split, config.window_size, config.step_size, memory_ptr)\n",
    "\n",
    "    # train dreamer after ep_thresh and if the input buffer has enough data to fill context window\n",
    "    if config.use_dreamer and episode >= config.episode_threshold and input_buffer.shape[0] == config.window_size:\n",
    "        # train and assess the dreamer every train_frequency\n",
    "        if episode % config.dreamer_train_frequency == 0:\n",
    "            dreamer.train_dreamer(train_set, config.dreamer_train_epochs, config.batch_size_dreamer)\n",
    "\n",
    "        # truncate the training set to control train time performance\n",
    "        if test_set.shape[0] > config.max_size:\n",
    "            train_set = train_set[-config.max_size:]\n",
    "\n",
    "        print(f'Size of train set: {train_set.shape[0]}, test set: {test_set.shape[0]}')\n",
    "        \n",
    "        # evaluate the dreamer's performance & decide num of training steps for dreamer\n",
    "        dreamer_avg_loss = dreamer.test_dreamer(test_set, config.batch_size_dreamer)\n",
    "        dreamer_eps = get_dreamer_eps(dreamer_avg_loss, config.score_threshold)\n",
    "\n",
    "        # train on dreamer if its accurate enough\n",
    "        if dreamer_eps > 0:\n",
    "            print(f'Dreamer active for {dreamer_eps} iterations')\n",
    "            ep_timesteps_dreamer = ep_reward_dreamer = 0\n",
    "            for dep in range(dreamer_eps):\n",
    "                print(f'Dreamer ep: {dep+1}')\n",
    "\n",
    "                # initialise dreamer states with the input sequence\n",
    "                # input buffer = state, action, next_state, reward, done NOTE - this is different to order learned in training\n",
    "                dreamer.states = input_buffer[:, :state_dim]\n",
    "                dreamer.actions = input_buffer[:-1, state_dim:state_dim+act_dim]\n",
    "                dreamer.rewards = input_buffer[:-1, -2-state_dim].unsqueeze(1)\n",
    "                dreamer.dones = input_buffer[:-1, -1-state_dim].unsqueeze(1)\n",
    "\n",
    "                # sample from dreamer environment (ignore input buffer and info)\n",
    "                if config.auto_horizon:\n",
    "                    imagination_horizon = min(config.imagination_horizon, ep_timesteps)\n",
    "                else:\n",
    "                    imagination_horizon = config.imagination_horizon\n",
    "                _td, _rd, _, _ = train_on_environment(\n",
    "                    actor, dreamer, grad_step_class, replay_buffer,\n",
    "                    imagination_horizon, # number of timesteps to run the dreamer for\n",
    "                    dreamer.states[-1].cpu().numpy(), # use last state from dreamer TODO why?\n",
    "                    config.batch_size, total_steps, config.window_size,\n",
    "                    use_intrinsic_reward=False)\n",
    "\n",
    "                ep_timesteps_dreamer += _td\n",
    "                ep_reward_dreamer += _rd\n",
    "\n",
    "    memory_ptr = replay_buffer.ptr # update the memory pointer to the current position in the buffer\n",
    "\n",
    "    print(f\"Ep: {episode} | Timesteps: {ep_timesteps} | Reward: {ep_reward:.3f} | Total Steps: {total_steps:.2g} | Dreamer: {dreamer_eps > 0}\")\n",
    "    if config.use_dreamer and dreamer_eps > 0:\n",
    "        print(f\"\\t Dreamer Eps: {dreamer_eps} | Dreamer Avg Reward: {ep_reward_dreamer/dreamer_eps:.3f} | Dreamer Avg Timesteps: {ep_timesteps_dreamer/dreamer_eps:.3g}\")\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    log_dict = {\n",
    "        \"Total Steps\": total_steps,\n",
    "        \"Episode Timesteps\": ep_timesteps,\n",
    "    }\n",
    "\n",
    "    # log tracked statistics\n",
    "    if 'recorder' in info: # if we have info available\n",
    "        log_dict[\"TrackedInfo/r_mean_\"] = info['recorder']['r_mean_'] # r is extrinsic only\n",
    "        log_dict[\"TrackedInfo/r_std_\"] = info['recorder']['r_std_']\n",
    "        log_dict[\"TrackedInfo/r_sum\"] = info['recorder']['r_sum']\n",
    "\n",
    "        if info['recorder']['r_mean_'] >= config.target_score: # update success_ep\n",
    "            success_ep = min(success_ep, episode)\n",
    "            print(f'target score reached at episode {success_ep}!')\n",
    "        log_dict[\"Episodes to Reach Target\"] = success_ep\n",
    "\n",
    "    if config.use_dreamer:\n",
    "        log_dict[\"Dreamer Average Loss\"] = dreamer_avg_loss if 'dreamer_avg_loss' in locals() else None # Only log if calculated\n",
    "        log_dict[\"Dreamer Episodes Run\"] = dreamer_eps\n",
    "        if dreamer_eps > 0:\n",
    "            log_dict[\"Dreamer Average Reward\"] = ep_reward_dreamer / dreamer_eps\n",
    "            log_dict[\"Dreamer Average Timesteps\"] = ep_timesteps_dreamer / dreamer_eps\n",
    "\n",
    "    wandb.log(log_dict, step=episode) # log metrics each ep\n",
    "\n",
    "    recent_rewards.append(ep_reward)\n",
    "\n",
    "    # plot tracked statistics (on real env)\n",
    "    if episode % plot_interval == 0:\n",
    "        # save as well (show=False returns it but doesnt display)\n",
    "        if save_fig and info : # check info isnt {} (i.e. not None)\n",
    "            fig, _ = tracker.plot(show=False, r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "            wandb.log({\"Coursework Plot\": wandb.Image(fig)}, step=episode) # Log the figure directly to W&B\n",
    "            # fig.savefig(f'./tracker_{run_name}.png', bbox_inches = 'tight')\n",
    "            plt.close(fig) # Close the figure to free memory\n",
    "        # show by default\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "    # checkpoint\n",
    "    if use_checkpointing and episode % checkpoint_interval == 0:\n",
    "        checkpoint_state = {\n",
    "            'episode': episode,\n",
    "            'total_steps': total_steps,\n",
    "            'actor_state_dict': actor.state_dict(),\n",
    "            'critic_state_dict': critic.state_dict(),\n",
    "            'critic_target_state_dict': critic_target.state_dict(), # Save target critic state\n",
    "            'actor_optimizer_state_dict': grad_step_class.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': grad_step_class.critic_optimizer.state_dict(),\n",
    "            'alpha_optimizer_state_dict': grad_step_class.alpha_optimizer.state_dict(),\n",
    "            'log_alpha': grad_step_class.log_alpha.data, # Save the tensor data\n",
    "            'replay_buffer_ptr': replay_buffer.ptr,\n",
    "            'replay_buffer_size': replay_buffer.size,\n",
    "            'replay_buffer_rollover': replay_buffer.rollover,\n",
    "            # Save buffer content directly (simpler, larger file)\n",
    "            'replay_buffer_state': replay_buffer.state,\n",
    "            'replay_buffer_action': replay_buffer.action,\n",
    "            'replay_buffer_next_state': replay_buffer.next_state,\n",
    "            'replay_buffer_reward': replay_buffer.reward,\n",
    "            'replay_buffer_not_done': replay_buffer.not_done,\n",
    "            'memory_ptr': memory_ptr,\n",
    "            'recent_rewards': recent_rewards,\n",
    "            'success_ep': success_ep,\n",
    "            'wandb_run_id': wandb.run.id,\n",
    "            # 'config': config, # Optional: save config if needed\n",
    "            'tracker_info': tracker.info # save tracker history\n",
    "        }\n",
    "        if config.use_dreamer and dreamer is not None:\n",
    "            checkpoint_state['dreamer_state_dict'] = dreamer.state_dict()\n",
    "            checkpoint_state['dreamer_optimizer_state_dict'] = dreamer.optimizer.state_dict()\n",
    "            checkpoint_state['train_set'] = train_set # Save dreamer datasets\n",
    "            checkpoint_state['test_set'] = test_set\n",
    "\n",
    "        save_checkpoint(checkpoint_state)\n",
    "\n",
    "    episode += 1\n",
    "\n",
    "    # break condition - stop if we consistently meet target score\n",
    "    if len(recent_rewards) >= config.len_r_list or success_ep < config.max_episodes:\n",
    "        current_avg = np.array(recent_rewards).mean()\n",
    "        print(f'Current progress: {current_avg:.3f} / {config.target_score}')\n",
    "        if current_avg >= config.target_score or success_ep < config.max_episodes: # quit when we've got good enough performance\n",
    "            print('Completed environment!')\n",
    "            break\n",
    "        recent_rewards = recent_rewards[-config.len_r_list+1:] # discard oldest on list (keep most recent 99)\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file\n",
    "if config.hardcore:\n",
    "    filename = f\"[{run_name}]_\" + \"nchw73-agent-hardcore-log.txt\"\n",
    "else:\n",
    "    filename = f\"[{run_name}]_\" + \"nchw73-agent-log.txt\"\n",
    "env.write_log(folder=\"logs\", file=filename)\n",
    "\n",
    "wandb.finish() # finish wandb run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
