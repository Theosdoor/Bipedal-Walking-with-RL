{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclosures**\n",
    "\n",
    "The use of TQC+D2RL+ERE was inspired by this various implementations, however this one gave good results on BipedalWalker and was used as a starting point - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "I made some important changes from this:\n",
    "* The ERE buffer anneals 'eta' which determins the amount of emphasis to put on recent events. Originally, eta annealed over the course of each episode, then reset for the next one. In contrast, in this implementation eta anneals over the whole training process, and doesn't reset at any point. This is more in line with the original paper I believe.\n",
    "* I added a Prioritised Experience Replay element to the ERE buffer and removed the Dreamer (which added too much complexity on top of PER)\n",
    "\n",
    "Key components:\n",
    "* TQC - Truncated Quantile Critics for Continuous Control (https://bayesgroup.github.io/tqc/)\n",
    "* ERE - Emphasizing Recent Experience Replay (https://arxiv.org/abs/1906.04009)\n",
    "* PER - Prioritised Experience Replay (https://arxiv.org/abs/1511.05952)\n",
    "* D2RL - Dense-to-Sparse Reward Learning (https://sites.google.com/view/d2rl/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "# **Dependencies and imports**\n",
    "\n",
    "This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSSUGAJsTLOV",
    "outputId": "875609e2-e3eb-4b29-c03c-1989820b1ea7"
   },
   "outputs": [],
   "source": [
    "# https://github.com/robert-lieck/rldurham/tree/main - rldurham source\n",
    "\n",
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Colab, installing dependencies...\")\n",
    "    url = \"\"\n",
    "    !wget -O pyproject.toml {url}\n",
    "    !uv pip install --system -r pyproject.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AFrqd24JTLOW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Subspace_Explorer/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.distributions as D\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "\n",
    "import rldurham as rld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "# **RL agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TIMESTEPS = 2000\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward adjustment\n",
    "def adjust_reward(reward):\n",
    "    \"\"\"\n",
    "    adjust reward for bipedal walker\n",
    "    \"\"\"\n",
    "    if reward == -100.0: \n",
    "        reward = -10.0 #Â bipedal walker does -100 for hitting floor, so -10 might make it more stable\n",
    "    else:\n",
    "        reward *= 2 # encourage forward motion\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop function for the environment\n",
    "def train_on_environment(actor, env, grad_step_class, replay_buffer, max_timesteps,\n",
    "    state, batch_size, total_steps, init_rand_steps=None,\n",
    "    ):\n",
    "    '''\n",
    "    Abstracted training loop function for the environment.\n",
    "\n",
    "    Based on https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "    '''\n",
    "    ep_timesteps = 0\n",
    "    ep_reward = 0\n",
    "\n",
    "    state_dim = actor.state_dim\n",
    "    act_dim = actor.action_dim\n",
    "\n",
    "    # initial state float32 for mps\n",
    "    state = state.astype(np.float32)\n",
    "\n",
    "    for t in range(max_timesteps): \n",
    "        total_steps += 1\n",
    "        ep_timesteps += 1\n",
    "\n",
    "        # select action and step the env\n",
    "        if init_rand_steps is not None and total_steps < init_rand_steps: # do random steps at start for warm-up\n",
    "            action = env.action_space.sample()\n",
    "        else: # otherwise use actor policy\n",
    "            action = actor.select_action(state)\n",
    "\n",
    "        step = env.step(action)\n",
    "\n",
    "        # handle different return formats\n",
    "        if len(step) == 5:\n",
    "            next_state, reward, term, tr, info = step\n",
    "            done = term or tr\n",
    "        else:\n",
    "            next_state, reward, done, info = step\n",
    "\n",
    "        # float32 for mps\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        reward = float(reward) # extrinsic reward\n",
    "        done = float(done)\n",
    "\n",
    "        ep_reward += reward # sum extrinsic reward\n",
    "\n",
    "        reward = adjust_reward(reward) # adjust reward for bipedal walker\n",
    "        replay_buffer.add(state, action, next_state, reward, done) # add to replay buffer\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        # train step if buffer has enough samples for a batch AND we're past warmup\n",
    "        if total_steps >= batch_size and init_rand_steps is not None and total_steps >= init_rand_steps:\n",
    "            # train the agent using experiences from the real environment\n",
    "            grad_step_class.take_gradient_step(replay_buffer, total_steps, batch_size)\n",
    "    \n",
    "        if done: # break if finished\n",
    "            break\n",
    "\n",
    "    # only return extrinsic to eval based on that\n",
    "    return ep_timesteps, ep_reward, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kTeUHYL7gU6"
   },
   "outputs": [],
   "source": [
    "class EREPER_ReplayBuffer(object):\n",
    "    '''\n",
    "    ERE implementation - https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "    ERE paper - https://arxiv.org/abs/1906.04009\n",
    "    \n",
    "    Prioritized Experience Replay (PER) buffer, as in https://arxiv.org/abs/1511.05952.\n",
    "    Implementation: https://github.com/BY571/Soft-Actor-Critic-and-Extensions/blob/master/SAC_PER.py\n",
    "        PER parameters are sourced from this implementation.\n",
    "\n",
    "\n",
    "    Rationale:\n",
    "    - actions from early in training are likely v different to optimal, so want to prioritise recent ones so we're not constantly learning from crap ones\n",
    "    - as time goes on, they get more similar so annealing eta allows uniform sampling later on\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, T, max_size, eta, cmin, use_per=True,\n",
    "                beta_1=0.6, beta_2_start=0.4, beta_2_frames=int(1e5), epsilon=1e-6, recency_scale=1):\n",
    "\n",
    "        self.max_size, self.ptr, self.size, self.rollover = max_size, 0, 0, False\n",
    "        self.use_per = use_per #Â use PER flag\n",
    "        \n",
    "        # ERE params\n",
    "        self.eta0 = eta\n",
    "        self.cmin = cmin\n",
    "        self.T = T\n",
    "        self.recency_scale = recency_scale # when using PER too, this helps calculate ck\n",
    "\n",
    "        # PER params\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2_start = beta_2_start\n",
    "        self.beta_2_frames = beta_2_frames\n",
    "        self.epsilon = epsilon\n",
    "        self.frame = 1 # for beta calculation\n",
    "\n",
    "        # storage\n",
    "        self.reward = np.empty((max_size, 1), dtype=np.float32) #Â float32 for mps\n",
    "        self.state = np.empty((max_size, state_dim), dtype=np.float32)\n",
    "        self.action = np.empty((max_size, action_dim), dtype=np.float32)\n",
    "        self.not_done = np.empty((max_size, 1), dtype=np.float32)\n",
    "        self.next_state = np.empty((max_size, state_dim), dtype=np.float32)\n",
    "        self.priorities = np.zeros((max_size, 1), dtype=np.float32) #Â for PER\n",
    "        self.max_prio = 1.0 #Â max priority seen so far\n",
    "\n",
    "    def eta_anneal(self, t):\n",
    "        # eta anneals over time --> 1, which reduces emphasis on recent experiences over time\n",
    "        return min(1.0, self.eta0 + (1 - self.eta0) * t / self.T)\n",
    "\n",
    "    def beta_2_by_frame(self, frame_idx):\n",
    "        \"\"\"\n",
    "        Linearly increases beta from beta_2_start to 1 over time from 1 to beta_2_frames.\n",
    "        \n",
    "        see 3.4 ANNEALING THE BIAS (Paper: PER)\n",
    "        \"\"\"\n",
    "        return min(1.0, self.beta_2_start + frame_idx * (1.0 - self.beta_2_start) / self.beta_2_frames)\n",
    " \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        # Add experience to replay buffer \n",
    "        self.state[self.ptr] = state.astype(np.float32)\n",
    "        self.action[self.ptr] = action.astype(np.float32)\n",
    "        self.next_state[self.ptr] = next_state.astype(np.float32)\n",
    "        self.reward[self.ptr] = float(reward)\n",
    "        self.not_done[self.ptr] = 1. - float(done)\n",
    "        self.priorities[self.ptr] = self.max_prio if self.size > 0 else 1.0 # gives max priority if buffer is not empty else 1\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.ptr %= self.max_size\n",
    "\n",
    "        # increase size counter until full, then start overwriting (rollover)\n",
    "        if self.max_size > self.size + 1:\n",
    "          self.size += 1\n",
    "        else:\n",
    "          self.size = self.max_size\n",
    "          self.rollover = True\n",
    "\n",
    "    def sample(self, batch_size, t):\n",
    "        # update eta value for current timestep\n",
    "        eta = self.eta_anneal(t)\n",
    "\n",
    "        # ERE -----\n",
    "        # get ck\n",
    "        c_calc = self.size * eta ** (t / self.T * self.recency_scale)\n",
    "        ck = int(max(self.cmin, c_calc)) # at least cmin samples\n",
    "        ck = min(ck, self.size) # limit to buffer size\n",
    "\n",
    "        # get indices within the recent window and handle rollover\n",
    "        if not self.rollover:\n",
    "            # buffer not yet full, window is [size - ck, size)\n",
    "            indices = np.arange(max(0, self.size - ck), self.size)\n",
    "        else:\n",
    "            # Buffer has rolled over, window wraps around\n",
    "            start_idx = (self.ptr - ck + self.max_size) % self.max_size\n",
    "            if start_idx < self.ptr:\n",
    "                indices = np.arange(start_idx, self.ptr)\n",
    "            else:\n",
    "                indices = np.concatenate((np.arange(start_idx, self.max_size), np.arange(0, self.ptr)))\n",
    "\n",
    "        # ensure ck matches the actual number of indices determined\n",
    "        ck = len(indices)\n",
    "\n",
    "        # PER -----\n",
    "        if self.use_per:\n",
    "            # calc P = p^a/sum(p^a)\n",
    "            prios = self.priorities[indices].flatten() + self.epsilon #Â add epsilon to avoid zero probs\n",
    "            probs = prios ** self.beta_1\n",
    "            probs_sum = probs.sum()\n",
    "            if probs_sum <= 0: # avoid division by zero if all prios = zero\n",
    "                P = np.ones(ck, dtype=np.float32) / ck # uniform sample\n",
    "            else:\n",
    "                P = probs / probs_sum\n",
    "            \n",
    "            # gets the indices depending on the probability p and the c_k range of the buffer\n",
    "            rel_indices = np.random.choice(ck, batch_size, p=P, replace=True)\n",
    "            samples = indices[rel_indices]\n",
    "            \n",
    "            beta_2 = self.beta_2_by_frame(self.frame)\n",
    "            self.frame += 1 #Â incremement for annealing\n",
    "                    \n",
    "            # Compute importance-sampling weights w = (N * P)^(-beta_2)\n",
    "            weights  = (ck * P[rel_indices]) ** (-beta_2)\n",
    "            # normalize weights\n",
    "            weights /= weights.max() \n",
    "            weights = np.array(weights, dtype=np.float32)\n",
    "        else:\n",
    "            # otherwise sample uniformly like in basic ere\n",
    "            rel_indices = np.random.choice(ck, batch_size, replace=True)\n",
    "            samples = indices[rel_indices]\n",
    "            # return weights of 1\n",
    "            weights = np.ones(batch_size, dtype=np.float32)\n",
    "\n",
    "\n",
    "        r = torch.tensor(self.reward[samples], dtype=torch.float32).to(DEVICE)\n",
    "        s = torch.tensor(self.state[samples], dtype=torch.float32).to(DEVICE)\n",
    "        ns = torch.tensor(self.next_state[samples], dtype=torch.float32).to(DEVICE)\n",
    "        a = torch.tensor(self.action[samples], dtype=torch.float32).to(DEVICE)\n",
    "        nd = torch.tensor(self.not_done[samples], dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        return s, a, ns, r, nd, samples, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        priorities = np.abs(batch_priorities) + self.epsilon #Â ensure > 0\n",
    "\n",
    "        self.priorities[batch_indices] = priorities.reshape(-1, 1) \n",
    "        self.max_prio = max(self.max_prio, np.max(priorities)) # update max prio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actor-critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS\n",
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "def quantile_huber_loss(quantiles, samples, sum_over_quantiles=False):\n",
    "    '''\n",
    "    From TQC (see paper p3)\n",
    "    Specific implementation: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/utils.py#L8\n",
    "\n",
    "    Huber loss is less sensitive to outliers than MSE\n",
    "\n",
    "    samples: (batch_size, 1, n_target_quantiles) -> (batch_size, 1, 1, n_target_quantiles)\n",
    "    quantiles: (batch_size, n_critics, n_quantiles) -> (batch_size, n_critics, n_quantiles, 1)\n",
    "    pairwise_delta: (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    '''\n",
    "    # uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise\n",
    "    delta = samples.unsqueeze(1) - quantiles.unsqueeze(3) # (batch_size, 1, 1, n_target_quantiles) - (batch_size, n_critics, n_quantiles, 1)\n",
    "    abs_delta = torch.abs(delta)\n",
    "    huber_loss = torch.where(abs_delta > 1., abs_delta - 0.5, delta ** 2 * 0.5) # 1.0 as threshold k for Huber loss\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "\n",
    "    # cumulative probabilities to calc quantiles\n",
    "    cum_prob = (torch.arange(n_quantiles, device=quantiles.device, dtype=torch.float) + 0.5) / n_quantiles\n",
    "    cum_prob = cum_prob.view(1, 1, -1, 1) # quantiles has shape (batch_size, n_critics, n_quantiles), so make cum_prob broadcastable to (batch_size, n_critics, n_quantiles, n_target_quantiles)\n",
    "    \n",
    "    # Calculate quantile loss: |Ï„ - I(Î´ < 0)| * L_k(Î´)\n",
    "    # Ï„ = cum_prob, I(Î´ < 0) = (delta < 0).float(), L_k(Î´) = huber_loss\n",
    "    loss = (torch.abs(cum_prob - (delta < 0).float()) * huber_loss)\n",
    "\n",
    "    # Summing over the quantile dimension \n",
    "    if sum_over_quantiles:\n",
    "        # sum over quantiles\n",
    "        # then average over target quantiles\n",
    "        loss = loss.sum(dim=2).mean(dim=2) # (batch_size, n_critics)\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "# MLP for actor that implements D2RL architecture\n",
    "class ActorMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim, output size = action dim\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    current_dim = input_dim # first layer has input dim = state dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim # prev layers output + original input size\n",
    "        \n",
    "    # Final layer input dim is last hidden layer output + original input size\n",
    "    self.last_layer_mean_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "    self.last_layer_log_std_linear = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim=1) # cat with output layer\n",
    "\n",
    "    mean_linear = self.last_layer_mean_linear(curr)\n",
    "    log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "    return mean_linear, log_std_linear\n",
    "\n",
    "# MLP for critic that implements D2RL architecture \n",
    "class CriticMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "    # input size = state dim + action dim, output size = n_quantiles\n",
    "    super().__init__()\n",
    "    self.layer_list = nn.ModuleList()\n",
    "    self.input_dim = input_dim\n",
    "\n",
    "    current_dim = input_dim\n",
    "    for i, next_size in enumerate(hidden_dims):\n",
    "      layer = nn.Linear(current_dim, next_size).to(DEVICE)\n",
    "      self.layer_list.append(layer)\n",
    "      current_dim = next_size + self.input_dim\n",
    "\n",
    "    self.last_layer = nn.Linear(current_dim, output_dim).to(DEVICE)\n",
    "\n",
    "  def forward(self, input_):\n",
    "    curr = input_\n",
    "    for layer in self.layer_list:\n",
    "      curr = F.gelu(layer(curr))\n",
    "      curr = torch.cat([curr, input_], dim=1)\n",
    "      \n",
    "    output = self.last_layer(curr)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class GradientStep(object):\n",
    "  '''\n",
    "  see (D2RL) https://github.com/pairlab/d2rl/blob/main/sac/sac.py\n",
    "  and (TQC) https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/tqc/tqc.py\n",
    "  '''\n",
    "  def __init__(self,*,\n",
    "    actor, critic, critic_target, discount, tau,\n",
    "    actor_lr, critic_lr, alpha_lr,\n",
    "    n_quantiles, n_mini_critics, top_quantiles_to_drop_per_net, target_entropy,\n",
    "    use_per=True\n",
    "    ):\n",
    "\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.critic_target = critic_target\n",
    "    self.use_per = use_per\n",
    "\n",
    "    self.log_alpha = nn.Parameter(torch.zeros(1).to(DEVICE)) # log alpha is learned\n",
    "    self.quantiles_total = n_quantiles * n_mini_critics\n",
    "    \n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "    self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    self.discount, self.tau = discount, tau\n",
    "    self.top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_mini_critics # total number of quantiles to drop\n",
    "    self.target_entropy = target_entropy\n",
    "\n",
    "  def take_gradient_step(self, replay_buffer, total_steps, batch_size=256):\n",
    "    # Sample batch from replay buffer\n",
    "    state, action, next_state, reward, not_done, indices, weights = replay_buffer.sample(batch_size, total_steps)\n",
    "    weights = torch.tensor(weights, dtype=torch.float32).to(DEVICE).unsqueeze(1) #Â PER weights. add dim for broadcast\n",
    "    alpha = torch.exp(self.log_alpha) # entropy temperature coefficient\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # sample new action from actor on next state\n",
    "      new_next_action, next_log_pi = self.actor(next_state)\n",
    "\n",
    "      # Compute and cut quantiles at the next state\n",
    "      next_z = self.critic_target(next_state, new_next_action)\n",
    "      \n",
    "      # Sort and drop top k quantiles to control overestimation (TQC)\n",
    "      sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "      sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop] # estimated truncated Q-val dist for next state\n",
    "\n",
    "      # td error + entropy term\n",
    "      target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "    \n",
    "    # Get current quantile estimates using action from the replay buffer\n",
    "    cur_z = self.critic(state, action)\n",
    "    per_critic_loss = quantile_huber_loss(cur_z, target.unsqueeze(1), sum_over_quantiles=True) # keep quantile dim for now\n",
    "    critic_loss = (per_critic_loss * weights).mean() #Â PER loss\n",
    "\n",
    "    new_action, log_pi = self.actor(state)\n",
    "    # detach the variable from the graph so we don't change it with other losses\n",
    "    alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean() # as in D2RL implementation for auto entropy tuning\n",
    "\n",
    "    # Optimise critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # update PER prios\n",
    "    if self.use_per and indices is not None:\n",
    "      avg_critic_loss = per_critic_loss.mean(1) #Â average over critics\n",
    "      new_prios = avg_critic_loss.detach().cpu().numpy()\n",
    "      replay_buffer.update_priorities(indices, new_prios)\n",
    "\n",
    "    # Soft update target networks\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Compute actor (Ï€) loss\n",
    "    # JÏ€ = ð”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\n",
    "    actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "    # ^ mean(2) is over quantiles, mean(1) is over critic ensemble\n",
    "    \n",
    "    # Optimise the actor\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # Optimise the entropy coefficient\n",
    "    self.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    self.alpha_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ArijusLengvenis/bipedal-walker-dreamer/blob/main/dreamer_bipedal_walker_code.ipynb\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[512, 512]):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mlp = ActorMLP(state_dim, hidden_dims, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.mlp(obs)\n",
    "        log_std = log_std.clamp(-20, 2) # clamp for stability\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        base_N_dist = D.Normal(mean, std) # base normal dist\n",
    "        tanh_transform = TanhTransform(cache_size=1) # transform to get the tanh dist\n",
    "\n",
    "        log_prob = None\n",
    "        if self.training: # i.e. agent.train()\n",
    "            transformed_dist = D.TransformedDistribution(base_N_dist, tanh_transform) # transformed distribution\n",
    "            action = transformed_dist.rsample() # samples from base dist & applies transform\n",
    "            log_prob = transformed_dist.log_prob(action) # log prob of action after transform\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True) # sum over action dim\n",
    "        else: # evaluation mode\n",
    "            action = torch.tanh(mean)\n",
    "            \n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).to(DEVICE)\n",
    "        if obs.ndim == 1: # add batch dim if missing\n",
    "             obs = obs.unsqueeze(0)\n",
    "        act, _ = self.forward(obs)\n",
    "        return np.array(act[0].cpu().detach())\n",
    "\n",
    "\n",
    "class Critic(nn.Module): # really a mega-critic from lots of mini-critics\n",
    "    '''\n",
    "    Ensemble of critics for TQC\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.critics = nn.ModuleList()\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        for _ in range(n_nets): # multiple critic mlps\n",
    "            net = CriticMLP(state_dim + action_dim, hidden_dims, n_quantiles)\n",
    "            self.critics.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # cat state and action (to pass to critic)\n",
    "        state_act = torch.cat((state, action), dim=1)\n",
    "\n",
    "        # pool quantiles from each critic mlp\n",
    "        quantiles = [critic(state_act) for critic in self.critics]\n",
    "        quantiles = torch.stack(quantiles, dim=1) # stack into tensor\n",
    "        return quantiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture statistics, logs, and videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My hyperparams\n",
    "seed = 42\n",
    "\n",
    "hyperparams = {\n",
    "    # env/general params\n",
    "    \"max_timesteps\": MAX_TIMESTEPS, # per episode\n",
    "    \"max_episodes\": 1000,\n",
    "    \"target_score\": 300, # stop training when average score over r_list > target_score\n",
    "    \"len_r_list\": 100, # length of reward list to average over for target score (stop training when avg > target_score)\n",
    "    \"hardcore\": True, # fixed in wandb sweep\n",
    "    \"init_rand_steps\": 10000, # number of steps to take with random actions before training (helps exploration)\n",
    "\n",
    "    # Agent hyperparams \n",
    "    \"n_mini_critics\": 5, # each mini-critic is a single mlp, which combine to make one mega-critic\n",
    "    \"n_quantiles\": 20, # quantiles per mini critic\n",
    "    \"top_quantiles_to_drop_per_net\": 'auto', # per mini critic (auto based on n_quantiles)\n",
    "    \"actor_hidden_dims\": [512, 512],\n",
    "    \"mini_critic_hidden_dims\": [256, 256], # * n_mini_critics\n",
    "    \"batch_size\": 256,\n",
    "    \"discount\": 0.98, # gamma\n",
    "    \"tau\": 0.005,\n",
    "    \"actor_lr\": 3.29e-4, # empirically chosen\n",
    "    \"critic_lr\": 3.5e-4, # empirically chosen\n",
    "    \"alpha_lr\": 3.24e-4, # empirically chosen\n",
    "\n",
    "    # ERE buffer (see paper for their choices)\n",
    "    \"use_per\": True, # use PER sampling as well\n",
    "    \"buffer_size\": 1000000, # smaller size improves learning early on but is outperformed later on\n",
    "    \"eta0\": 0.996, # 0.994 - 0.999 is good (according to paper)\n",
    "    \"annealing_steps\": 'auto', # number of steps to anneal eta over (after which sampling is uniform) - None = auto-set to max estimated steps in training\n",
    "    \"cmin\": 5000, # min number of samples to sample from\n",
    "    \"recency_scale\": 1, # scale factor for recency\n",
    "}\n",
    "\n",
    "# recording/logging\n",
    "plot_interval = 10 # plot every Nth episode\n",
    "save_fig = False # save figures too\n",
    "\n",
    "is_recording = True # record videos of agent\n",
    "if hyperparams['hardcore']:\n",
    "    video_interval = 30 # record every Nth episode\n",
    "    ep_start_rec = 500 # start recording on this episode\n",
    "else:\n",
    "    video_interval = 20\n",
    "    ep_start_rec = 50\n",
    "\n",
    "if hyperparams['annealing_steps'] == 'auto':\n",
    "    hyperparams['annealing_steps'] = hyperparams['max_episodes']*hyperparams['max_timesteps'] # max est number of steps in training\n",
    "if hyperparams['top_quantiles_to_drop_per_net'] == 'auto':\n",
    "    hyperparams['top_quantiles_to_drop_per_net'] = int(hyperparams['n_quantiles'] // 12.5) # keep ratio same as M=25 d=2 (from TQC paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1Xrcek4hxDXl",
    "outputId": "6161f148-a164-4e22-d97a-f9aa53e7b8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cpu (as recommended)\n",
      "actions are continuous with 4 dimensions/#actions\n",
      "observations are continuous with 24 dimensions/#observations\n",
      "maximum timesteps is: None\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SETUP\n",
    "\n",
    "# make env\n",
    "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=hyperparams['hardcore'])\n",
    "\n",
    "# get statistics, logs, and videos\n",
    "video_prefix = \"nchw73-agent-hardcore-video\" if hyperparams['hardcore'] else \"nchw73-agent-video\"\n",
    "env = rld.Recorder(\n",
    "    env,\n",
    "    smoothing=10,                       # track rolling averages (useful for plotting)\n",
    "    video=True,                         # enable recording videos\n",
    "    video_folder=\"videos\",              # folder for videos\n",
    "    video_prefix=video_prefix,          # prefix for videos\n",
    "    logs=True,                          # keep logs\n",
    ")\n",
    "\n",
    "rld.check_device() # training on CPU recommended\n",
    "\n",
    "env.video = False # switch video recording off (only switch on every x episodes as this is slow)\n",
    "\n",
    "# environment info\n",
    "discrete_act, discrete_obs, act_dim, state_dim = rld.env_info(env, print_out=True)\n",
    "\n",
    "# render start image\n",
    "env.reset(seed=seed)\n",
    "rld.render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWjENKkU_My"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# in the submission please use seed_everything with seed 42 for verification\n",
    "seed, state, info = rld.seed_everything(seed, env)\n",
    "\n",
    "# track statistics for plotting\n",
    "tracker = rld.InfoTracker()\n",
    "\n",
    "replay_buffer = EREPER_ReplayBuffer(\n",
    "    state_dim, act_dim, hyperparams['annealing_steps'], hyperparams['buffer_size'], hyperparams['eta0'], hyperparams['cmin'],\n",
    "    use_per=hyperparams['use_per'], recency_scale=hyperparams['recency_scale'],\n",
    "    )\n",
    "\n",
    "actor = Actor(state_dim, act_dim, hyperparams['actor_hidden_dims']).to(DEVICE)\n",
    "\n",
    "critic = Critic(\n",
    "    state_dim, act_dim, hyperparams['n_quantiles'], hyperparams['n_mini_critics'], hyperparams['mini_critic_hidden_dims']\n",
    "    ).to(DEVICE)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "target_entropy = -np.prod(env.action_space.shape).item() # target entropy heuristic = âˆ’dim(A) (as in SAC paper)\n",
    "\n",
    "grad_step_class = GradientStep(\n",
    "    actor=actor, critic=critic, critic_target=critic_target,\n",
    "    discount=hyperparams['discount'], tau=hyperparams['tau'],\n",
    "    actor_lr=hyperparams['actor_lr'], critic_lr=hyperparams['critic_lr'], alpha_lr=hyperparams['alpha_lr'],\n",
    "    n_quantiles=hyperparams['n_quantiles'], n_mini_critics=hyperparams['n_mini_critics'],\n",
    "    top_quantiles_to_drop_per_net=hyperparams['top_quantiles_to_drop_per_net'],\n",
    "    target_entropy=target_entropy, use_per=hyperparams['use_per'],\n",
    "    )\n",
    "\n",
    "actor.train()\n",
    "\n",
    "total_steps = 0\n",
    "memory_ptr = 0 # marks boundary between new and old data in the replay buffer\n",
    "recent_rewards= []\n",
    "completed_env = False\n",
    "\n",
    "episode = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 14 | Timesteps: 55 | Reward: -110.106 | Total Steps: 1.1e+04\n",
      "Ep: 15 | Timesteps: 2000 | Reward: -72.884 | Total Steps: 1.3e+04\n",
      "Ep: 16 | Timesteps: 2000 | Reward: -68.794 | Total Steps: 1.5e+04\n",
      "Ep: 17 | Timesteps: 59 | Reward: -101.857 | Total Steps: 1.5e+04\n",
      "Ep: 18 | Timesteps: 72 | Reward: -98.471 | Total Steps: 1.5e+04\n",
      "Ep: 19 | Timesteps: 63 | Reward: -98.943 | Total Steps: 1.5e+04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m# float32 for mps\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# sample real env\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ep_timesteps, ep_reward, info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_step_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_timesteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minit_rand_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_timesteps\n\u001b[1;32m     19\u001b[0m tracker\u001b[38;5;241m.\u001b[39mtrack(info) \u001b[38;5;66;03m# track statistics for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m, in \u001b[0;36mtrain_on_environment\u001b[0;34m(actor, env, grad_step_class, replay_buffer, max_timesteps, state, batch_size, total_steps, init_rand_steps)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# train step if buffer has enough samples for a batch AND we're past warmup\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;129;01mand\u001b[39;00m init_rand_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m init_rand_steps:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# train the agent using experiences from the real environment\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mgrad_step_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;66;03m# break if finished\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 82\u001b[0m, in \u001b[0;36mGradientStep.take_gradient_step\u001b[0;34m(self, replay_buffer, total_steps, batch_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# ^ mean(2) is over quantiles, mean(1) is over critic ensemble\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Optimise the actor\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Optimise the entropy coefficient\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "while episode <= hyperparams['max_episodes']: # index from 1\n",
    "    # recording statistics and video can be switched on and off (video recording is slow!)\n",
    "    if is_recording and episode >= ep_start_rec:\n",
    "        env.info = episode % video_interval == 0   # track every x episodes (usually tracking every episode is fine)\n",
    "        env.video = episode % video_interval == 0  # record videos every x episodes (set BEFORE calling reset!)\n",
    "\n",
    "    # reset for new episode\n",
    "    state, info = env.reset()\n",
    "    state = state.astype(np.float32) # float32 for mps\n",
    "\n",
    "    # sample real env\n",
    "    ep_timesteps, ep_reward, info = train_on_environment(\n",
    "        actor, env, grad_step_class, replay_buffer, hyperparams['max_timesteps'], state,\n",
    "        hyperparams['batch_size'], total_steps, hyperparams['init_rand_steps'],\n",
    "        )\n",
    "\n",
    "    total_steps += ep_timesteps\n",
    "    tracker.track(info) # track statistics for plotting\n",
    "\n",
    "    memory_ptr = replay_buffer.ptr # update the memory pointer to the current position in the buffer\n",
    "\n",
    "    # print(f\"Ep: {episode} | Timesteps: {ep_timesteps} | Reward: {ep_reward:.3f} | Total Steps: {total_steps:.2g}\")\n",
    "   \n",
    "    # plot tracked statistics (on real env)\n",
    "    if episode % plot_interval == 0 or completed_env or episode == hyperparams['max_episodes']: # always save last plot\n",
    "        # save as well (show=False returns it but doesnt display)\n",
    "        if save_fig and info: # check info isnt {} (i.e. not None)\n",
    "            fig, _ = tracker.plot(show=False, r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "            fig_suffix = '_hardcore' if hyperparams['hardcore'] else ''\n",
    "            fig.savefig(f'./tracker_nchw73{fig_suffix}.png', bbox_inches = 'tight')\n",
    "            plt.close(fig) # Close the figure to free memory\n",
    "        # show by default\n",
    "        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n",
    "\n",
    "    recent_rewards.append(ep_reward)\n",
    "    current_avg = np.array(recent_rewards).mean()\n",
    "\n",
    "    # env completion (break) condition - stop if we consistently meet target score\n",
    "    if len(recent_rewards) >= hyperparams['len_r_list']:\n",
    "        print(f'Current progress: {current_avg:.3f} / {hyperparams[\"target_score\"]}')\n",
    "        if current_avg >= hyperparams['target_score']: # quit when we've got good enough performance\n",
    "            print(f\"Completed environment in {episode} episodes!\")\n",
    "            completed_env = True # ensure final plot is saved\n",
    "            break\n",
    "        recent_rewards = recent_rewards[-hyperparams['len_r_list']+1:] # discard oldest on list (keep most recent 99)        \n",
    "    \n",
    "    \n",
    "    episode += 1\n",
    "\n",
    "# don't forget to close environment (e.g. triggers last video save)\n",
    "env.close()\n",
    "\n",
    "# write log file\n",
    "if hyperparams['hardcore']:\n",
    "    filename = \"nchw73-agent-hardcore-log.txt\"\n",
    "else:\n",
    "    filename = \"nchw73-agent-log.txt\"\n",
    "env.write_log(folder=\"logs\", file=filename)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
